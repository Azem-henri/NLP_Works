published as a conference paper at iclr 2019 of wikipedia knowledge-powered conversational agents emily dinan∗ stephen roller∗ kurt shuster∗ angela fan michael auli jason weston facebook ai research edinan roller kshuster angelafan michaelauli jase @ fb.com abstract in open-domain dialogue intelligent agents should exhibit the use of knowledge however there are few convincing demonstrations of this to date the most popular sequence to sequence models typically generate and hope generic utterances that can be memorized in the weights of the model when mapping from input utterance s to output rather than employing recalled knowledge as context use of knowledge has so far proved difﬁcult in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding to that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from wikipedia we then design architectures capable of retrieving knowledge reading and conditioning on it and ﬁnally generating natural responses our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations while our new benchmark allows for measuring further improvements in this important research direction 1 introduction arguably one of the key goals of ai and the ultimate the goal of natural language research is for humans to be able to talk to machines in order to get close to this goal machines must master a number of skills to be able to comprehend language employ memory to retain and recall knowledge to reason about these concepts together and ﬁnally output a response that both fulﬁlls functional goals in the conversation while simultaneously being captivating to their human speaking partner the current state-of-the-art approaches sequence to sequence models of various kinds sutskever et al. 2014 vinyals le 2015 serban et al. 2016 vaswani et al. 2017 attempt to address some of these skills but generally suffer from an inability to bring memory and knowledge to bear as indicated by their name they involve encoding an input sequence providing limited reasoning by transforming their hidden state given the input and then decoding to an output to converse intelligently on a given topic a speaker clearly needs knowledge of that subject and it is our contention here that more direct knowledge memory mechanisms need to be employed in this work we consider setups where this can be naturally measured and built we consider the task of open-domain dialogue where two speakers conduct open-ended chit-chat given an initial starting topic and during the course of the conversation the topic can broaden or focus on related themes during such conversations an interlocutor can glean new information and personal points of view from their speaking partner while providing similarly themselves this is a task as it requires several components not found in many standard models we design a set of architectures speciﬁcally for this goal that combine elements of memory network architectures sukhbaatar et al. 2015 to retrieve knowledge and read and condition on it and transformer architectures vaswani et al. 2017 to provide state-of-the-art text representations and sequence models for generating outputs which we term transformer memory networks as to our knowledge no public domain dataset of requisite scale exists we build a supervised dataset of human-human conversations using crowd-sourced workers ﬁrst crowd-sourcing 1365 diverse discussion topics and then conversations involving 201 999 utterances about them each ∗joint ﬁrst authors 1 published as a conference paper at iclr 2019 topic is connected to wikipedia and one of the humans the wizard is asked to link the knowledge they use to sentences from existing articles in this way we have both a natural way to train a knowledgeable conversation agent by employing a memory component that can recall and ground on this existing text and a natural way to evaluate models that we build by assessing their ability at locating and using such knowledge our transformer memory network architectures both in retrieval and generative versions are tested in this setup using both automatic metrics and human evaluations we show their ability to execute engaging knowledgeable conversations with humans compared to a number of baselines such as standard memory networks or transformers our new benchmark publicly in parlai http // parl.ai/projects/wizard of wikipedia/ , aims to encourage and measure further improvements in this important research direction 2 related work many existing dialogue tasks do not study the use of knowledge explicitly for example popular chit-chat datasets such as open-subtitles vinyals le 2015 , persona-chat zhang et al. 2018 and twitter sordoni et al. 2015 have tested the ability of sequence-to-sequence models that attend over the recent dialogue history but do not attempt to recall long-term knowledge beyond encoding it directly into the weights of the feed-forward network in the area of goal-directed dialogue separate from open domain chit-chat such as airline el asri et al. 2017 or restaurant booking henderson et al. 2014 wen et al. 2016 bordes et al. 2017 , knowledge conditioning is typically employed by allowing access to a database through api calls or otherwise in contrast our work investigates unstructured knowledge across a large diverse set of topics potentially spanning all of wikipedia in question answering one does not produce a dialogue response based on a conversation history but a factual answer based on a question in that case it is clear that retrieving and conditioning knowledge is vital for example in squad neural models have been developed that attend to a given paragraph from wikipedia to answer questions rajpurkar et al. 2016 , or open-squad which extends this to answering without being given the paragraph instead performing retrieval over the entirety of wikipedia chen et al. 2017 . recently the quac dataset investigates similar themes but as a sequence of questions and answers in dialogue form instead choi et al. 2018 . in this work we do not address question answering but focus on natural human dialogues which contain a diverse set of utterances not just questions and answers the closest work to ours lies in the area of non-goal directed dialogue incorporating knowledge the work of dodge et al ( 2016 employed memory networks to perform dialogue discussing movies in terms of recommendation and open-ended discussion from reddit conditioning on a structured knowledge base zhou et al ( 2018 also links reddit to structured knowledge both parthasarathi pineau 2018 and ghazvininejad et al ( 2018 use unstructured text instead as we do the former to discuss news articles using wikipedia summaries as knowledge and the latter to discuss local businesses in two-turn dialogues using foursquare tips as knowledge ghazvininejad et al ( 2018 uses an extended encoder-decoder where the decoder is provided with an encoding of the context along with the external knowledge encoding neither involves dialogue authored with the given knowledge so it is unclear when knowledge is useful or not in contrast in our task we know the wikipedia articles and sentences that ground crowdworkers dialogues model-wise parthasarathi pineau 2018 uses a bag-of-words memory network type fact encoder and an rnn decoder our work compares memory networks sukhbaatar et al. 2015 and transformers which have been shown to be on-par or superior to rnn encoder-decoders vaswani et al. 2017 , and develops an architecture that combines these approaches concurrently with our work moghe et al ( 2018 proposed a dataset based on the closed domain of movie chats our paper shows models working on full multi-turn dialogue in an open-domain setting which to our knowledge was not shown before 3 wizard of wikipedia we consider the following general open-domain dialogue setting two participants engage in chitchat with one of the participants selecting a beginning topic and during the conversation the topic published as a conference paper at iclr 2019 is allowed to naturally change the two participants however are not quite symmetric one will play the role of a knowledgeable expert which we refer to as the wizard while the other is a curious learner the apprentice . apprentice at each stage of the conversation the apprentice talks to the wizard freely playing the role of a curious learner eager to chat their goal is to go into depth about a chosen topic that interests themselves or their partner while keeping the conversation engaging and fun note that the instruction to delve deeply into a topic makes this different to more shallow chit-chat tasks in this task the use of knowledge is emphasized more wizard the wizard is given the following instructions “ you have just met the other person who seems quite curious and you are eager to discuss a topic with them ” their goal is to inform their conversation partner about a topic that one of them will choose crucially the wizard has access to an information retrieval system that shows them paragraphs from wikipedia possibly relevant to the conversation which are unobserved by the apprentice before each conversation turn the wizard can read these paragraphs and then potentially base their next reply on that observed knowledge note the wizard is particularly instructed not to simply parrot this knowledge but to use it to craft a relevant reply and to present any relevant knowledge in a fun and engaging way if possible conversation flow the ﬂow of the conversation thus takes place as follows 1. either the wizard or apprentice is picked to choose the topic and speak ﬁrst the other player receives the topic information and the conversation begins 2. when the apprentice sends the wizard a message the wizard is shown relevant knowledge described below , and chooses a relevant sentence in order to construct a response or else chooses the no sentence used option 3. the wizard responds to the apprentice basing their response on their chosen sentence 4. the conversation repeats until one of the conversation partners ends the chat after a mini mum of or turns each randomly chosen beforehand . after collecting data of such wizard-apprentice conversations between humans the goal is to then replace the human wizard with a learned agent that will speak to a human apprentice instead similar to the procedure in wizard of oz experiments bernsen et al. 2012 . topics we crowd-sourced a set of 1365 natural open-domain dialogue topics each linked to a wikipedia article these include diverse topics such as commuting gouda cheese music festivals podcasts bowling and arnold schwarzenegger knowledge retrieval at each step of the dialogue the wizard has access to a set of passages of knowledge which may be relevant to the given dialogue context while this is a potentially learnable part of the model we required for this to be ﬁxed so that we could present the results to the annotator when collecting the dataset we thus used exactly the same retriever that is commonly used for the open-squad dataset in chen et al ( 2017 . it uses a simple inverted index lookup followed by term vector model scoring articles and queries are compared as tf-idf weighted bag-of-word and n-gram vectors using the hashing trick we retrieve the top articles ﬁrst paragraph only for the last two turns of dialogue by wizard and apprentice and the article ﬁrst 10 sentences only for the original topic and present these articles to the wizard as knowledge context along with their titles note that while this system is used to build the dataset a superior method can in principle be learned and used by a model at test time knowledge selection and response generation during data collection the wizard can click on any of the retrieved article titles in the dialogue ui to expand that article at which point they can click on a sentence that is most relevant to the response they want to make only one article and one sentence can be selected on any turn for simplicity . if they see no relevant article or sentence they can choose no sentence used instead the wizard then enters their response to the apprentice an image of the wizard s ui is shown in appendix a.1 3 published as a conference paper at iclr 2019 table : dataset statistics of the wizard of wikipedia task wizard of wikipedia task number of utterances number of dialogues number of topics average turns per dialogue knowledge database train 166,787 18,430 1,247 9.0 valid 17,715 1,948 599 9.1 5.4m articles test seen 8,715 965 533 9.0 93m sentences test unseen 8,782 968 58 9.1 final dialogue dataset the ﬁnal dialogue dataset we collect consists of 22,311 dialogues with 201,999 turns which we divide into 166,787 for train 17,715 for validation and 17,497 for test the test set is split into two subsets test seen and test unseen test seen contains 533 overlapping topics with the training set with new dialogues about those topics test unseen consists of 58 topics never seen before in train or validation overall data statistics can be found in table , and further statistics and examples of collected conversations in appendix a.2 we observe wizards and apprentices both asking and answering questions and providing each other with a mixture of facts and personal feelings during their general discussion 4 models in this work we consider learning dialogue models to replace the wizard in our learning tasks i.e the knowledgeable speaker the dialogue model thus can have access to a knowledge source in this case wikipedia to ground the conversation with we thus develop extensions of the memory network sukhbaatar et al. 2015 and transformer vaswani et al. 2017 models that can i retrieve from a large memory relevant information conditioned on the dialogue history ( ii carefully read and attend over the retrieved set of knowledge and then iii generate the next dialogue utterance this model is then used consecutively on each turn to form an entire dialogue with a user we develop two classes of models capable of leveraging knowledge ( ) retrieval models that produce an output among a set of candidate responses the set of utterances from the training set ; and ii generative models that generate word-by-word using a beam . the input to either model is the same at each dialogue turn where the model is intended to make a response it is given the current dialogue context x1 . . xt of dialogue turns where x1 is always the initial starting topic e.g “ kurt cobain ) and the remaining turns swap between the two speakers the goal at each stage is to output the next utterance xt+1 knowledge retrieval we assume a large knowledge base memory m1 . . mn which is hierarchically organized into documents consisting of paragraphs and sentences as it is infeasible for current neural attention techniques to operate on this scale we use standard information retrieval ir techniques c ir x m ) as a ﬁrst step to return a smaller set of candidates mc1 . . mck for ﬁne-grained selection in our experiments we use the ir system provided to the human annotators during dataset creation detailed in section 3. the retriever operates on the topic x1 and the last two turns xt and xt−1 if they exist effectively calling the ir system three times with three different queries empirically this provided better performance compared to merging into one query likely because it can address quite different topics we retrieve the top articles ﬁrst paragraph only for each lookup and then ﬂatten all the results into separate sentences i.e remove the organization of sentences belonging to articles , but prepend every sentence with its article title in this way the candidates mc1 . . mck given to the neural model in the next stage can be attended to independently without having to deal with hierarchical issues knowledge attention we use an attention mechanism to perform ﬁne-grained selection of which knowledge sentences will be used to produce the next turn of dialogue each sentence in the memory is independently encoded with a transformer encoder vaswani et al. 2017 , and the same trans published as a conference paper at iclr 2019 figure : generative transformer memory network an ir system provides knowledge candidates from wikipedia dialogue context and knowledge are encoded using a shared encoder in the two-stage model the dialogue and knowledge are re-encoded after knowledge selection former is used to encode the dialogue context x. we then perform standard dot-product attention between the memory candidates and the dialogue context utterance prediction given the hidden state derived from the memory attention process described above the ﬁnal stage is to predict the output utterance that will form the next dialogue turn we consider different variants of the two stages above knowledge attention and utterance prediction when considering retrieval and generative variants of our models we will now detail these in turn 4.1 retrieval transformer memory network this model encodes each knowledge sentence mc1 . . mck and the dialogue context with a transformer as described above the ﬁnal input encoding is calculated by performing dot-product attention over enc mc1 , . , enc mck and adding the resulting weighted sum of these vectors to enc x to get the representation replhs mc1 . . mck x . the candidate responses r1 . . rl are encoded with a separate transformer to get reprhs ri for each i. we choose as a response ( ) where cid:96 = arg max i∈ 1 ... l replhs mc1 ··· mck x ( ) replhs mc1 . . mck x ( ) • reprhs ri ( ) reprhs ri ( ) . the model is trained to minimize the cross-entropy loss where the negative candidates for each example are the responses to the other examples in the batch henderson et al. 2017 . 4.2 generative transformer memory network we consider two versions a two-stage and an end-to-end version both models ﬁnd the most relevant piece of knowledge mbest and then perform an encoding step by concatenating it with the dialogue context allowing the decoder to attend over both the knowledge and dialogue when formulating its response we employ a beam search of to select our best response all generative models employ bpe encoding sennrich et al. 2016 , which we found effective at enabling generators to copy rare words from wikipedia sentences fan et al. 2018 . in the end-to-end version a shared transformer encoder is used to encode all candidates mci and the dialogue history the encoded candidates are ﬂattened into vectors using the normalization from cer et al ( 2018 ( summing and normalizing by the square root of the sentence length in order to balance short and long sentences to produce an attention prediction over the memory the full sequence encoding of the single highest selected knowledge mbest is concatenated with the encoding of the dialogue and passed into a transformer decoder an illustration of our end-to-end model is shown in figure 1. we train the model to minimize the negative log-likelihood of the response utterance we can add additional supervision by forcing the knowledge selection to correctly choose the same knowledge candidate as the human wizard in the training set by adding an additional crossentropy loss over the knowledge attention modulated by a weight : in the two-stage version we employ two separately trained models for each of these two tasks knowledge selection and utterance prediction as the knowledge selection step creates a hard deci = 1 λ lnll λlknowledge 5 encoded independently transformer encoder dialog context ir system knowledge memory two-stage transformer encoder attention end-to-end dialogue knowledge lknowledge transformer decoder dialogue response lnll published as a conference paper at iclr 2019 table : test performance of various methods on the knowledge selection task the models must select the gold knowledge sentences chosen by humans given the dialogue context method random ir baseline bow memnet transformer transformer +reddit pretraining transformer +reddit pretraining +squad training r 1 2.7 5.8 23.0 22.5 24.5 25.5 seen test unseen test f1 13.1 23.5 22.9 19.8 35.8 34.2 f1 @ 2.3 7.6 8.9 12.2 23.7 22.9 13.5 21.8 36.3 33.2 36.4 36.2 sion inﬂuencing the output of the generator we ﬁnd maximizing the performance of this component to be vital we can also improve performance of the decoder by employing knowledge dropout k.d ) wherein we artiﬁcially prevent the model from attending to knowledge a fraction of the time during training we ﬁnd this helps the generator be more resilient to errors at the knowledge selection stage and makes training faster k. d. is a novel technique we propose here however it is similar to many other dropout techniques e.g feature dropout used in wu et al ( 2017 . experiments we describe each of our experimental setups and results we ﬁrst investigate the ability of our models to select knowledge appropriately and then consider the full task of dialogue with knowledge 5.1 knowledge selection task before looking at the full wizard dialogue task we assess the ability of models to predict the knowledge selected by human wizards in the dataset given the dialogue history this will inform us of the feasibility of this task and the best models to use in a two-stage architecture we compare transformers against various baselines including a random baseline an information retrieval ir baseline which uses simple word overlap and a bag-of-words memory network sukhbaatar et al. 2015 . where noted the transformer is pretrained on reddit data mazar´e et al. 2018 , and ﬁne-tuned for our task the results are shown in table 2. transformers work best as long as they are pretrained on a large dataset reddit , while multi-tasking on squad provides marginal impact further analysis of this task using other models is provided in appendix b.1 we use the best performing transformer model reported here for our two-stage generative memory network in the full dialogue task 5.2 full task dialogue with knowledge we evaluate our models on the full task of dialogue generation given knowledge in two settings given the gold knowledge sentence chosen by a human or where the model needs to predict which knowledge to use we separately describe experiments for retrieval and generative models retrieval experiments we use similar baselines as in the knowledge selection experiments but now also apply transformer memory networks which attend over knowledge models are evaluated measuring recall 1 when ranking the gold response among 99 randomly chosen candidates and unigram f1 of the model s prediction with the gold response the results are shown in table 3. we ﬁnd that the addition of knowledge improves all models improving bow memnet from 56 to 71 @ and the transformer memnet from 79 to 87 @ ) for predicted knowledge performance improves dramatically when models are provided gold knowledge but otherwise retain similar trends generative experiments we compare our generative end-to-end and two-stage transformer memory network models to two more baselines repeating the last utterance and a generative transformer model trained to respond to dialogue but without access to knowledge models are evaluated using perplexity ppl of the gold response and unigram f1 6 published as a conference paper at iclr 2019 table : retrieval methods on the full wizard task models must select relevant knowledge and retrieve a response from the training set as a dialogue response using knowledge always helps and the transformer memory network with pretraining performs best method random ir baseline bow memnet no knowledge bow memnet transformer no knowledge w/o reddit transformer no knowledge w/ reddit transformer memnet w/ reddit transformer memnet w/ reddit+squad r 1 1.0 17.8 56.1 71.3 60.8 79.0 86.8 87.4 f1 7.4 12.7 14.2 15.6 13.3 15.0 15.4 15.4 predicted knowledge test seen gold knowledge unseen seen @ f1 @ 7.3 1.0 1.0 67.5 73.5 11.6 28.8 56.1 11.6 66.7 84.5 12.3 25.5 60.8 9.7 11.6 79.0 54.0 12.4 82.3 91.6 12.4 92.3 83.1 test unseen @ 1.0 14.2 28.8 33.1 25.5 54.0 69.8 69.8 table : generative models on the full wizard task the two-stage model performs best using predicted knowledge while the end-to-end e2e model performs best with gold knowledge method repeat last utterance transformer no knowledge e2e transformer memnet no auxiliary loss e2e transformer memnet w/ auxiliary loss two-stage transformer memnet two-stage transformer memnet w/ k.d ) predicted knowledge gold knowledge test seen f1 ppl 13.8 66.5 15.9 16.9 63.5 18.6 54.8 46.5 18.9 test unseen f1 ppl 13.7 103.6 14.3 14.4 97.3 17.4 88.5 84.8 17.3 test seen f1 ppl 13.8 17.8 41.8 24.2 33.6 35.5 23.1 30.7 30.0 28.6 30.6 test unseen f1 ppl 13.7 14.0 87.0 35.5 29.5 32.2 32.8 28.6 42.7 43.7 28.0 table : human experiments evaluations of the best generative and retrieval models on full dialogues with humans human ratings are reported as mean stddev . wiki f1 measures unigram overlap with the wikipedia entry for the chosen topic a measure of knowledge used in conversations seen unseen method human performance retrieval transformer no knowledge generative transformer no knowledge retrieval transformer memnet two-stage generative transformer memnet rating wiki f1 11.1 19.8 15.3 23.4 30.0 4.13 1.08 3.33 1.30 2.11 1.11 3.43 1.10 2.92 1.33 rating wiki f1 10.6 13.7 10.1 16.3 26.2 4.34 0.98 3.12 1.34 2.54 1.38 3.14 1.31 2.93 1.30 the results are given in table 4. our experiments show that both the end-to-end and two-stage models employ the knowledge in their response predictions as they outperform their counterpart transformer without knowledge and demonstrate substantial improvements when provided the gold knowledge while the two-stage model produces signiﬁcantly stronger perplexity and f1 scores using the predicted knowledge the end-to-end model outperforms the two-stage model in the gold knowledge experiments this suggests that the two-stage model beneﬁts from the strong knowledge selection module section 5.1 , but that the end-to-end model is better at employing the selected knowledge furthermore we ﬁnd that the additional knowledge selection supervision auxiliary loss in the end-to-end model improves it on every metric suggesting that tightly integrating these tasks is beneﬁcial knowledge dropout k. d. also helps compare last two rows . more evidence published as a conference paper at iclr 2019 model retrieval transformer memnet topic e-book seen human i do not know why but have never gotten into e-books model same here i have a kindle but don t use it for books it s nicer to hold a real book and turn pages human yes i like the physical feel and smell of a real book model i actually had a nook e-book where you had to download the e-book after selecting it online i grew out of it i prefer paper books also human i tried e-books but do not like reading on a screen its disorientated for me model yes reading a book on a tablet just isn t the same human plus like owning the physical copy of a real book i feel like own it model i prefer print and newsreels topic toga party seen model i love toga parties i usually make my toga out of a bed sheet and wear my sandals human me too ! i usually use some ﬂip ﬂops and my blanket model toga parties typically have games involving kegs and excessive drinking human yeah and getting to wear funny outﬁts and a crown of leaves model i love me some toga parties human i love me some too model toga parties are so much fun with the roman or greek theme human yeah i especially like greek themes model two-stage generative transformer memnet topic shower unseen human so ’ ve been looking for a better showerhead with more water pressure for my shower model i love pressure-versed water at nearly constant temperature to a shower human that sounds pretty great love those rainshower showers model the ancient greeks were the ﬁrst people to have showers human well knew they were good for something model the original showers were neither indoor structures nor man made but were common human ah that sounds intimidating model it is required manual transport of both fresh and waste water figure : selected conversations between humans and models for this is shown in appendix b.1 lastly we note that both two-stage models give higher f1 scores than any of the retrieval models shown in table . 5.3 human evaluation we perform human evaluation of our models using crowd-sourced workers humans are paired with our models and asked to chat about a speciﬁc topic given a choice of 2–3 topics for 3–5 dialogue turns following their conversation the humans are asked to rate their dialogue partner on a scale of 1–5 with the rating indicating how much they liked the conversation 5 is best , which we refer to as the engagingness rating using the collected conversations we also calculate a metric we call the wiki f1 sore the f1 overlap of the model s utterances with the ﬁrst 10 sentences of the wikipedia page for the chosen topic as a proxy for how much knowledge the model exhibits we seek a model that can be simultaneously engaging and knowledgeable hence we would like to maximize both these metrics1 for comparison we also collect 100 human-human conversations with only one human choosing the topic and performing evaluation in total we collect a total of 546 conversations with ratings from 464 distinct workers these results are shown in table 5. we ﬁnd that the retrieval models signiﬁcantly outperform the generative models on the human engagingness evaluation student s t-test p .05 . the human engagingness differences between retriever models with and without knowledge are not signiﬁcant but note they both trend toward use of knowledge due to the candidate sentences retrieved with the knowledgeable version obtaining signiﬁcantly higher wiki f1 scores in both seen and unseen test sets for the generative models we ﬁnd human engagingness ratings are signiﬁcantly improved by the use of knowledge p .01 . the signiﬁcantly higher wiki f1 scores indicate that i these models convey more knowledge than their counterparts without knowledge conditioning and ii on both seen and unseen sets they convey more knowledge than the retrieval models in particular on unseen 1for example a model could display knowledge by copying parts of wikipedia but not be engaging at all 8 published as a conference paper at iclr 2019 data the gap between retrieval and generative models is larger this is understandable as retrieval models are limited to producing a response from the training set where the unseen topic did not appear there is still a considerable gap to human ratings of other humans compared to all our models ﬁrst row of table ) figure shows example conversations with the retrieval and generative models additional analysis and examples can be found in appendix b.3 and c. conclusion in this work we build dialogue agents which are able to employ large memory systems containing encyclopedic knowledge about the world in order to conduct engaging open-domain conversations we develop a set of architectures transformer memory network models that are capable of retrieving and attending to such knowledge and outputting a response either in retrieval or generative modes to train and evaluate such models we collect the wizard of wikipedia dataset a large collection of open-domain dialogues grounded by wikipedia knowledge and demonstrated the effectiveness of our models in automatic and human experiments our new publicly available benchmark aims to encourage further model exploration and we expect such efforts will result in signiﬁcant advances in this important research direction there is much future work to be explored using our task and dataset some of these include ( ) bridging the gap between the engagingness of retrieval responses versus the ability of generative models to work on new knowledge and topics ( ii learning to retrieve and reason simultaneously rather than using a separate ir component and iii investigating the relationship between knowledge-grounded dialogue and existing qa tasks which also employ such ir systems the aim is for those strands to come together to obtain an engaging and knowledgeable conversational agent references niels bernsen hans dybkjær and laila dybkjær designing interactive speech systems from ﬁrst ideas to user testing springer science business media 2012. antoine bordes y-lan boureau and jason weston learning end-to-end goal-oriented dialog in proceedings of the international conference on learning representations iclr , 2017. daniel cer yinfei yang sheng yi kong nan hua nicole limtiaco rhomni st. john noah constant mario guajardo-cespedes steve yuan chris tar yun-hsuan sung brian strope and ray kurzweil universal sentence encoder arxiv preprint arxiv:1803.11175 2018. danqi chen adam fisch jason weston and antoine bordes reading wikipedia to answer opendomain questions in proceedings of the 55th annual meeting of the association for computational linguistics pp 1870–1879 association for computational linguistics 2017. eunsol choi he he mohit iyyer mark yatskar wen-tau yih yejin choi percy liang and luke zettlemoyer quac question answering in context in proceedings of the 2018 conference on empirical methods in natural language processing brussels belgium october 2018. jesse dodge andreea gane xiang zhang antoine bordes sumit chopra alexander miller arthur szlam and jason weston evaluating prerequisite qualities for learning end-to-end dialog systems in proceedings of the international conference on learning representations iclr , 2016. layla el asri hannes schulz shikhar sharma jeremie zumer justin harris emery fine rahul mehrotra and kaheer suleman frames a corpus for adding memory to goal-oriented dialogue systems in proceedings of the 18th annual sigdial meeting on discourse and dialogue pp 207–219 saarbr¨ucken germany august 2017. association for computational linguistics angela fan david grangier and michael auli controllable abstractive summarization in proceedings of the 2nd workshop on neural machine translation and generation pp 45–54 association for computational linguistics 2018 9 published as a conference paper at iclr 2019 marjan ghazvininejad chris brockett ming-wei chang bill dolan jianfeng gao wen-tau yih and michel galley a knowledge-grounded neural conversation model in proceedings of the conference on association for the advancement of artiﬁcial intelligence aaai , 2018. matthew henderson blaise thomson and jason williams the second dialog state tracking . in proceedings of the 15th annual meeting of the special interest group on discourse and dialogue sigdial , pp 263–272 2014. matthew henderson rami al-rfou brian strope yun-hsuan sung l aszl o luk acs ruiqi guo sanjiv kumar balint miklos and ray kurzweil efﬁcient natural language response suggestion for smart reply arxiv preprint arxiv:1705.00652 2017. jiwei li will monroe and daniel jurafsky a simple fast diverse decoding algorithm for neural generation arxiv preprint arxiv:1611.08562 2016. pierre-emmanuel mazar´e samuel humeau martin raison and antoine bordes training millions of personalized dialogue agents in proceedings of the 2018 conference on empirical methods in natural language processing brussels belgium october 2018. association for computational linguistics nikita moghe siddhartha arora suman banerjee and mitesh m. khapra towards exploiting background knowledge for building conversation systems in proceedings of the 2018 conference on empirical methods in natural language processing pp 2322–2332 association for computational linguistics 2018. url http //aclweb.org/anthology/d18-1255 prasanna parthasarathi and joelle pineau extending neural generative conversational model using external knowledge sources in proceedings of the 2018 conference on empirical methods in natural language processing brussels belgium october 2018. pranav rajpurkar jian zhang konstantin lopyrev and percy liang squad 100,000+ questions for machine comprehension of text in proceedings of the 2016 conference on empirical methods in natural language processing pp 2383–2392 austin texas november 2016. association for computational linguistics rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units in proceedings of the 54th annual meeting of the association for computational linguistics pp 1715–1725 berlin germany august 2016. association for computational linguistics iulian vlad serban ryan lowe laurent charlin and joelle pineau generative deep neural net works for dialogue a short review arxiv preprint arxiv:1611.06216 2016. alessandro sordoni michel galley michael auli chris brockett yangfeng ji margaret mitchell jian-yun nie jianfeng gao and bill dolan a neural network approach to context-sensitive generation of conversational responses arxiv preprint arxiv:1506.06714 2015. sainbayar sukhbaatar jason weston rob fergus et al end-to-end memory networks in advances in neural information processing systems pp 2440–2448 2015. ilya sutskever oriol vinyals and quoc le sequence to sequence learning with neural networks in advances in neural information processing systems pp 3104–3112 2014. ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez in advances in neural infor łukasz kaiser and illia polosukhin attention is all you need mation processing systems pp 5998–6008 2017. ashwin k. vijayakumar michael cogswell ramprasaath r. selvaraju qing sun stefan lee david j. crandall and dhruv batra diverse beam search decoding diverse solutions from neural sequence models arxiv preprint arxiv:1610.02424 2016. oriol vinyals and quoc le a neural conversational model in proceedings of the 31st international conference on machine learning deep learning workshop lille france 2015 10 published as a conference paper at iclr 2019 tsung-hsien wen david vandyke nikola mrksic milica gasic lina rojas-barahona pei-hao su stefan ultes and steve young a network-based end-to-end trainable task-oriented dialogue system arxiv preprint arxiv:1604.04562 2016. ledell wu adam fisch sumit chopra keith adams antoine bordes and jason weston starspace embed all the things arxiv preprint arxiv:1709.03856 2017. saizheng zhang emily dinan jack urbanek arthur szlam douwe kiela and jason weston personalizing dialogue agents i have a dog do you have pets too in proceedings of the 56th annual meeting of the association for computational linguistics pp 2204–2213 melbourne australia july 2018. association for computational linguistics hao zhou tom young minlie huang haizhou zhao jingfang xu and xiaoyan zhu commonsense knowledge aware conversation generation with graph attention in proceedings of the twenty-seventh international joint conference on artiﬁcial intelligence ijcai , pp 4623–4629 international joint conferences on artiﬁcial intelligence organization 7 2018 11 published as a conference paper at iclr 2019 a dataset collection a.1 human annotation interface for wizard 12 published as a conference paper at iclr 2019 a.2 wizard of wikipedia examples topic apprentice wizard lifeguard so am a lifeguard know anything about saving lives in water i m impressed it s a big responsibility to supervise other people s safety in the water tell me more apprentice well i help make sure people do not drown or get injured while in or near the water knowledge a lifeguard is a rescuer who supervises the safety and rescue of swimmers surfers . . lifeguards are strong swimmers and trained in cpr/aed ﬁrst aid certiﬁed in water . in some areas the lifeguard service also carries out mountain rescues or may function as the primary ems provider i ve heard that in some places lifeguards also help with other sorts of emergencies like mountain rescues is that part of your job too i have i feel like you know much about this what brings you to know so much wizard apprentice wizard oh that s about the extent of my knowledge i ve just been around beaches and ’ ve always admired lifeguards i m not a super strong swimmer myself topic armadillo wizard apprentice wizard i love animals and think armadillos are awesome with their leathery shell i don t think ’ ve ever seen an armadillo in real life i ve seen them at the zoo armadillo means little armored one in spanish apprentice are they native to a spanish-speaking part of the world knowledge armadillos are new world placental mammals in the order cingulata . the word armadillo means little armoured one in spanish the nine-banded armadillo “ dasypus novemcinctus ) or the nine-banded long-nosed armadillo is a medium-sized mammal found in north central and south america wizard yes they are most commonly found in north central and south america topic wizard apprentice knowledge ice cream just love ice cream i love the types with fruits and ﬂavours do you like ice cream i love ice cream as much as any one i especially like gelato foreign ice cream ice cream is a sweetened frozen food typically eaten as a snack or dessert .. . it is usually made from dairy products such as milk and cream and . bacon ice cream or bacon-and-egg ice cream is an ice cream generally created by adding bacon to egg custard and freezing the mixture wizard me too there are some strange combinations though have you heard of bacon ice cream where they add bacon and even egg custard to the freezing mixture surprisingly bacon ice cream doesn t surprise me that doesn t sound appealing to me but perhaps it could be delicious . apprentice ··· ··· ··· figure : the wizard of wikipedia dataset examples of collected conversations from the dataset where both wizard and apprentice are humans the wizard has access to an information retrieval system over wikipedia so that they can ask and answer questions and make statements relevant to the discussion for each utterance knowledge retrieval is performed based on dialogue history giving ∼61 knowledge candidates per turn with wizards clicking no sentence used 6.2 of the time assuming that a question contains a question mark or begins with how , why , who , where , what or when , in the dataset apprentices ask questions in 13.9 of training set utterances and answer questions i.e. the wizard has asked a question 39.5 of the time while saying new or follow-on statements neither asking nor answering a question 49.3 of the time hence the wizard and apprentice conduct conversations with a variety of dialogue acts a.3 topic selection to choose between topics that are natural we employed the existing persona-chat dataset zhang et al. 2018 where crowdworkers where asked to create personas of typical speakers there are 13 published as a conference paper at iclr 2019 ∼1000 personas each of which consists of 4-5 sentences describing that person s interests e.g “ love watching game of thrones , i like to eat cheetos and i recently got a cat . these can thus naturally be seen as topics of interest and using another set of annotators we mapped each sentence to or more relevant wikipedia pages if possible e.g “ ariel is my favorite princess was labeled with the wikipedia page for the little mermaid as some sentences are harder to connect with wikipedia e.g “ am witty , they are left unlabeled we thus obtain 1,431 topics in total to use for our task we retain the persona topic sets and thus present 2-3 related topic choices as conversation starters per dialogue during data collection b additional experiments b.1 knowledge selection table : test performance of the knowledge selection tasks we also tested the performance of our models trained to do the full dialogue task see section 5.2 on the knowledge selection task for our retrieval system this refers to the performance of the knowledge attention the results show that our retrieval system could be improved and the auxiliary loss clearly helps the generative models method random ir baseline bow memnet transformer transformer +reddit pretraining transformer +reddit pretraining +squad training retrieval transformer memnet no auxiliary loss generative e2e transformer memnet no auxiliary loss generative e2e transformer memnet w/ auxiliary loss seen test @ 2.7 5.8 23.0 22.5 24.5 25.5 12.9 13.4 21.1 f1 13.5 21.8 36.3 33.2 36.4 36.2 24.6 28.3 32.8 unseen test f1 @ 2.3 13.1 23.5 7.6 22.9 8.9 19.8 12.2 23.7 35.8 34.2 22.9 26.3 14.6 25.9 11.8 14.3 22.8 b.2 full dialogue with retrieval b.3 human experiments error analysis we perform an analysis of the dialogues produced from the human evaluation experiments detailed in section 5.3. we sample 20 conversations from each experimental setting split between seen and unseen conversations are re-tokenized and lowercased to reduce superﬁcial differences between models and then analyzed in a single-blind setup we note of common errors and behaviors exhibited in each of the different conversations in general the human-human conversations are starkly different than any of the bot conversations humans tend to have more small talk or use the topic of discussion as a mere icebreaker with neither human behaving as a wizard this is in contrast to human-human conversations from the wizard dataset itself where one human has access to wikipedia and the conversation becomes more grounded in factual sentences similarly all models attempt to play the role of wizard and produce more factual sentences too in some rounds humans treat the bot as a sort of question-answer machine suggesting that the models could be improved by additionally employing squad-like training data the retriever without knowledge is particularly prone to non sequiturs or rapidly changing the subject during unseen conversations it is especially likely to discuss something other than the chosen topic in contrast the retriever with knowledge tends to stick to the chosen topic strongly but has difﬁculty if the human changes the subject frequently in unseen topics the retriever with 14 published as a conference paper at iclr 2019 table : retrieval methods on the full wizard task in addition to the models we tested in the paper we also tested a two-stage retrieval system in which we used our best-performing model on the knowledge selection task to choose a single knowledge sentence to condition on for the dialogue retrieval task this outperformed our best retrieval method in terms of f1 but not not in terms of recall 1. furthermore we compared these results to a two-stage retrieval system in which the dialogue retrieval module is optimized for seeing the gold chosen knowledge sentence the performance of this system on the gold knowledge task suggests that the retrieval system could be improved by increasing performance on the knowledge selection subtask predicted knowledge test seen test unseen method random ir baseline bow memnet no knowledge bow memnet transformer no knowledge w/o reddit transformer no knowledge w/ reddit transformer memnet w/ reddit transformer memnet w/ reddit+squad two-stage transformer optimized for predicted knowledge two-stage transformer optimized for gold knowledge r 1 1.0 17.8 56.1 71.3 60.8 79.0 86.8 87.4 84.2 79.6 f1 @ 1.0 7.4 14.2 12.7 28.8 14.2 15.6 33.1 25.5 13.3 54.0 15.0 69.8 15.4 69.8 15.4 63.1 16.2 16.6 60.1 gold knowledge unseen seen f1 @ r 1 1.0 1.0 7.3 67.5 73.5 11.6 28.8 56.1 11.6 12.3 84.5 66.7 25.5 60.8 9.7 54.0 79.0 11.6 82.3 91.6 12.4 12.4 92.3 83.1 13.2 83.1 92.3 96.3 88.3 13.1 table : human experiments we calculate the wiki f1 score for the wizard and apprentice as they appear in the dataset for the sake of comparison to our human evaluations note that this differed from the human-human evaluation set-up in the sense that the wizard had direct access to wikipedia passages in the ui which explains the higher values of wiki f1 both for the wizard who uses wikipedia and for the apprentice who would likely reference that use . seen unseen method human performance wizard performance in dataset apprentice performance in dataset retrieval transformer no knowledge generative transformer no knowledge retrieval transformer memnet two-stage generative transformer memnet rating wiki f1 11.1 43.3 23.2 19.8 15.3 23.4 30.0 4.13 1.08 3.33 1.30 2.11 1.11 3.43 1.10 2.92 1.33 rating wiki f1 10.6 43.1 23.7 13.7 10.1 16.3 26.2 4.34 0.98 3.12 1.34 2.54 1.38 3.14 1.31 2.93 1.30 knowledge produces similar but factually inaccurate answers to user queries for example when one user asks about parts of ireland to visit the model enumerates a list of locations in greece nonetheless its repertoire of available responses often include inviting responses allowing the bot to have a more natural conversational ﬂow selected conversations with the retriever with knowledge may be found in figure , for both seen and unseen topics the generator without knowledge is particularly prone to many of the typical behaviors of seq2seq systems li et al. 2016 vijayakumar et al. 2016 , including local repetition “ cookies are made of ﬂour oil oil and oil ) global repetition producing the near same utterance for multiple turns , or inconsistencies in its personality saying it both likes and dislikes movies . the generator with knowledge has signiﬁcantly fewer issues with repetition as it errs on the side of copying large fragments from the wikipedia knowledge the generator with knowledge can also act as a selﬁsh conversationalist choosing to respond or detail information without inviting a response although it generally produces accurate statements it sometimes produces statements using an incorrect date 15 published as a conference paper at iclr 2019 name or word it also frequently produces formulaic responses like i don t know but do know that wikipedia excerpt ” nonetheless we ﬁnd the generator with knowledge is able to successfully generalize to unseen topics using the knowledge from wikipedia selected conversations with the generator with knowledge may be found in figure . 16 published as a conference paper at iclr 2019 model retrieval transformer memnet topic arnold schwarzenegger seen human hey are you into the movie the running man model yeah that s one of my favorite movies i can t believe that team america world police was made in 2004 14 years ago human another one of arnies movies like is terminator judgment day model the terminator is probably his most iconic role but he got his start from the movie conan the barbarian , which came out a couple years earlier i actually haven t seen that one human which one conan or da terminata model i like arnold schwarzeneggar as well i thought he was a fantastic governator serving from 2003-2011. human haha governator yeah it s pretty crazy they he held higher ofﬁce model yes he won the lifetime achievement award interestingly he joined the u.s. army in 1961 and was a paratrooper ( ) topic steak seen human hello it s lunch time here and ’ in the mood for a great steak model wow that sounds delicious wish could say the same hahah when the word steak is used without qualiﬁcation it normally refers to a beefsteak human interesting i don t really have a favorite steak how about you model i like rib eye it is really good specially medium rare when the word steak is used without qualiﬁcation it generally refers to a beefsteak human i like rib eye too what is it about a rib eye that you enjoy model most steak cuts are very tender meat but think rib eye can be quite tender human they really are how do you choose a good steak model when it comes to steak and other cuts of meat you have rare medium rare medium well done and so on human how do you prefer your steak prepared model i like that you can prepare it many different ways you can fried it grill it or even broil the steak topic ireland unseen model hi have you ever been to dublin it is the capital and largest city in ireland human no i haven t but would love to model it is the largest city in northern ireland ( ) human i ve always found ireland to be fascinating and would love to visit sometime model same here it is the capital of ireland too human what is the most popular place to visit in ireland model i would recommend the thrace crete and the ionian islands as must see places ( ) human i don t know anything about the ionian islands model it was once referred to as the hellenic republic ( ) topic american football unseen model american football can be traced back to britain in the mid-19th century the rules were very similar human i did not know that did they wear equipment back then model am not really certain about it but american football resulted from several major divergences from associa tion football and rugby football human the philadelphia eagles actually have a rugby player on their team this year he never played football until model philadelphia is one of a few cities to have all four teams but they love the eagles more than anything else this year by a long shot human as a life long philadelphia sports fan have to agree with you model i ve always been more of a fan of the american football team from pittsburgh the steelers human being from philadelphia am not much of a pittsburgh fan model are the pittsburgh steelers in the afc human yes they are the afc north to be exact topic list of national parks of the united states unseen human i have never visited a national park in the united states but would really like to model me too i d love to live in a national park human which one do you think is the best to visit ﬁrst model the united nations is an important place to visit the statue of liberty central park is great also human is the statue of liberty considered a national park model yes the statue of liberty is there on an island the liberty island human i had no idea that that was a national park i have heard zion national park is really nice model yes it links the shenandoah national park and the great smoky mountains together ( ) figure : retriever with knowledge conversations selected conversations with a human ( ) indicates clear factual mistakes by the model 17 published as a conference paper at iclr 2019 model two-stage generative transformer memnet topic circus seen model have you ever been to a circus it is a company of performers who put on diverse entertainment shows human i went to the circus once when was a child actually model that s great entertainment is a form of activity that holds the attention and interest of an audience human that s true have you ever been to a circus model i have not but know they were built with various types of seating ring and sometimes a stage human i went to one held in a stadium so we had box seats model that s awesome i m sure you ll have to go to the circus with appropriate music human yes circuses usually have their own kind of music model circuses can be done by circuses topic granny smith seen human i really love granny smith apples they re my favorite type of apple model i love granny smith apples they have hard light green skin and a crisp ﬂesh human yes i really enjoy them i also like honeycrisp apples but they re so expensive model they ve been grown for thousands of years in asia and europe and were brought to north america by euro pean colonists * human oh really they ve been around way longer than thought model they re also consumed raw it s one of the most popular cooking apples human i always eat them raw i hate cooked apples for some reason even in pie model i m not sure if it s cooked but do know that they can be cooked raw topic culture of chicago seen human hi do you like chicago model i love chicago it s the third most populous city in the us human how many people live there model about . million residents it is also the most populous city in the state of illinois human do you like chicago deep dish pizza model chicago-style pizza may be made in deep dish style and as a stuffed pizza human what is the name of the baseball team model dano s pizzeria specializes in chicago style stuffed pizza human is dano s pizzaria open everyday model yes it is it is an american style pizza chain headquartered in midtown brook illinois topic animals in sport unseen model i love to watch animals it s the most popular form of animal-related sport human horse racing is the most awesome animal sport model horse racing is an equestrian performance sport typically involving two or more horses ridden by jockeys or driven over a set distance for competition human yes the kentucky derby and the triple crown is something look forward to every year model the horse has evolved over the past 45 to 55 million years from a small multi toed creature human that is very interesting how did people come to domesticate horses for sport model i m not sure but the domestication is believed to have been widespread by 3000 bc human very interesting i wonder how long people used horses to hunt before to race them model i m not sure but do know that horses have been domesticated for thousands of years topic dairy farming unseen human hi there was wondering how you felt about farming model i do not like dairy farming but do know that in developed countries dairy farms typically consist of high dairy products human oh yes they have alot of cows and push out alot of dairy every hour model other species used in commercial dairy farming include goats sheep and camels human i did not know you could get milk a camel model camel milk has supported bedouin human oh wow thats very interesting model in italy they are growing in popularity to produce an alternative milk source for humans human i have had goat milk before model the earliest evidence of using domesticated cows for dairy production is the seventh millennium bc-the neolithic era human thats so neat i would say cow milk is my favorite model i love cow milk especially since the iphone is used in the iphone human iphone and cow milk model the iphone was ﬁrst released in 2007. human oh ok well thanks for the talk have a great day model no problem the iphone was ﬁrst released in 2007. figure : two-stage generator conversations selected conversations with a human ( ) indicates clear factual mistakes by the model 18 cross-lingual abstractive summarization with limited parallel resources yu bai yang gao heyan huang∗ school of computer science and technology beijing institute of technology beijing china southeast academy of information technology fujian china beijing engineering research center of high volume language information processing and cloud computing applications beijing china yubai gyang hhy63 @ bit.edu.cn abstract parallel cross-lingual summarization data is scarce requiring models to better use the limited available cross-lingual resources existing methods to do so often adopt sequenceto-sequence networks with multi-task frameworks such approaches apply multiple decoders each of which is utilized for a speciﬁc task however these independent decoders share no parameters hence fail to capture the relationships between the discrete phrases of summaries in different languages breaking the connections in order to transfer the knowledge of the high-resource languages to lowresource languages to bridge these connections we propose a novel multi-task framework for cross-lingual abstractive summarization mclas in a low-resource setting employing one uniﬁed decoder to generate the sequential concatenation of monolingual and cross-lingual summaries mclas makes the monolingual summarization task a prerequisite of the cross-lingual summarization cls task in this way the shared decoder learns interactions involving alignments and summary patterns across languages which encourages attaining knowledge transfer experiments on two cls datasets demonstrate that our model signiﬁcantly outperforms three baseline models in both low-resource and full-dataset scenarios moreover in-depth analysis on the generated summaries and attention heads veriﬁes that interactions are learned well using mclas which beneﬁts the cls task under limited parallel resources introduction cross-lingual summarization cls helps people efﬁciently grasp salient information from articles in a foreign language neural approaches to cls require large scale datasets containing millions of cross-lingual document-summary pairs zhu et al. ∗corresponding author figure : an example of the alignments across summaries in different languages each color represents phrases with one speciﬁc meaning 2019 cao et al. 2020 zhu et al. 2020 . however two arise with these approaches 1 most languages are low-resource thereby lacking document-summary paired data 2 large parallel datasets across different languages for neuralbased cls are rare and expensive especially under the current trend of neural networks therefore a low-resource setting is more realistic and , one for cross-lingual summarization to our best knowledge cross-lingual summarization under low-resource settings has not been well investigated and explored therefore in this paper we will develop a new model for cross-lingual abstractive summarization under limited supervision for low-resource settings multi-task learning has been shown to be an effective method since it can borrow useful knowledge from other relevant tasks to use in the target task yan et al. 2015 wang et al. 2020 motiian et al. 2017 . crosslingual summarization can be viewed as the combination of two tasks i.e. monolingual summarization ms and cross-lingual translation zhu et al. 2019 . a wealth of relationships exist across the target summaries of ms and cls tasks such as translation alignments and summarization patterns illustrated in figure , 叙利亚 is mapped to syria , and similar maping is done with the other 6910 proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing pages 6910–6924 august 1–6 2021 ©2021 association for computational linguistics aligned phrases obviously leveraging these relationships is crucial for the task of transferring summarization knowledge from high-resource languages to low-resource languages unfortunately existing multi-task frameworks simply utilize independent decoders to conduct ms and cls task separately zhu et al. 2019 cao et al. 2020 , which leads to failure in capturing these relationships to solve this problem we establish reliant connections between ms and cls tasks making the monolingual task a prerequisite for the crosslingual task speciﬁcally one decoder is shared by both ms and cls tasks this is done by setting the generation target as a sequential concatenation of a monolingual summary and the corresponding cross-lingual summary sequentially generating monolingual and cross-lingual summaries the decoder also conducts the translation task between them which enhances the interactions between different languages these interactions implicitly involve translation alignments similarity in semantic units and summary patterns across different lingual summaries to demonstrate these decoder interactions we further visualize them by probing transformer attention heads in the model based on this process the new structure with these advanced interactions enhances low-resource scenarios which require the model to be capable of transferring summary knowledge from high-resource languages to low-resource language we name our model multi-task cross-lingual abstractive summarization mclas under limited resources in terms of a training strategy under limited resources we ﬁrst pre-train mclas on large-scale monolingual document-summary parallel datasets to well-equip the decoder with general summary capability given a small amount of parallel crosslingual summary samples the model is then ﬁnetuned and is able to transfer the learned summary capability to the low-resource language leveraging the interactions uncovered by the shared decoder experiments on zh2ensum zhu et al. 2019 and a newly developed en2desum dataset demonstrate that mclas offers signiﬁcant improvements when compared with state-of-the-art cross-lingual summarization models in both low-resource scenarios and full-dataset scenario at the same time we also achieved competitive performances in the en2zhsum dataset zhu et al. 2019 . human evaluation results show that mclas produces more ﬂuent concise and informative summaries than baselines models under limited parallel resources in addition we analyzed the length of generated summaries and the success of monolingual generation to verify advantages offered by identifying interactions between languages we further investigate the explainability of the proposed multi-task structure by probing the attention heads in the uniﬁed decoder proving that mclas learns the alignments and interactions between two languages and this facilitates translation and summarization in the decoder stage our analysis provides a clear explanation of why mclas is capable of supporting cls under limited resources our implementation and data are available at : //github.com/woodenwhite/mclas 2 related work 2.1 cross-lingual summarization recently cross-lingual summarization has received attention in research due to the increasing demand to produce cross-lingual information traditional cls systems are based on a pipeline paradigm wan et al. 2010 wan 2011 zhang et al. 2016 . these pipeline systems ﬁrst translate the document and then summarize it or vice versa shen et al ( 2018 propose the use of pseudo summaries to train the cross-lingual abstractive summarization model in contrast duan et al ( 2019a and ouyang et al ( 2019 generate pseudo sources to construct the cross-lingual summarization dataset the ﬁrst large-scale cross-lingual summarization datasets are acquired by use of a round-trip translation strategy zhu et al. 2019 . additionly zhu et al ( 2019 propose a multi-task framework to improve their cross-lingual summarization system following zhu et al ( 2019 , more methods have been proposed to improve the cls task zhu et al ( 2020 use a pointer-generator network to exploit the translation patterns in cross-lingual summarization cao et al ( 2020 utilize two encoders and two decoders to jointly learn to align and summarize in contrast to previous methods mclas generates the concatenation of monolingual and crosslingual summaries thereby modeling relationships between them 2.2 low-resource natural language generation natural language generation nlg for lowresource languages or domains has attracted lots of attention gu et al ( 2018 leverage meta-learning 6911 to improve low-resource neural machine translation meanwhile many pretrained nlg models have been proposed and adapted to low-resource scenarios song et al. 2019 chi et al. 2020 radford et al. 2019 zhang et al. 2019a . however these models require large-scale pretraining our work does not require any large pretrained generation models or translation models enabling a vital decreases in training cost 3 background 3.1 neural cross-lingual summarization given { xa a monolingual summarization system converts the source into a summary sa { ya } where and are the lengths of da and sa respectively a cross-lingual summarization system produces a summary sb { yb ( ) consisting of tokens yb in target language , where ( ) is the length of sb note that the mentioned xa ya and yb are all tokens source } in language a document da , . , yb , . , xa , . , ya , yb , xa , ya a zhu et al ( 2019 propose using the transformer vaswani et al. 2017 to conduct crosslingual summarization tasks the transformer is composed of stacked encoder and decoder layers the encoder layer is comprised of a self-attention layer and a feed-forward layer the decoder layer shares the same architecture as the encoder except for an extra encoder-decoder attention layer which performs multi-head attention over the output of stacked encoder layers the whole transformer model is trained to maximize the conditional probability of the target sequence sb as follows lncls logp yb |yb t da ( ) ( ) t=1 3.2 improving ncls with multi-task frameworks considering the relationship between cls and ms in which they share the same goal to summarize important information in a document zhu et al ( 2019 proposed employing a one-to-many multitask framework to enhance the basic transformer model in this framework one encoder is employed to encode the source document da two separate decoders simultaneously generate a monolingual summary sa and a cross-lingual summary sb leading to a loss as follows lncls+ms ( ) + cid:80 n cid:48 t=1 logp ya |ya t=1 logp yb t da t |yb t da figure : an overview of our proposed mclas a uniﬁed decoder produces both monolingual green and cross-lingual red summaries the green and red lines represent the monolingual and cross-lingual summaries attention respectively this multi-task framework shares encoder representation to enhance cross-lingual summarization however independent decoders in this model are incapable of establishing alignments and connections between cross-lingual summaries 4 mclas with limited parallel resources to strengthen the connections mentioned we propose making the monolingual task a prerequisite for the cross-lingual task through modeling interactions according to previous work wan et al. 2010 yao et al. 2015 zhang et al. 2016 , interactions between cross-lingual summaries important phrase alignments sentence lengths and summary patterns etc are crucial for the ﬁnal summary s quality we leverage these interactions to further transfer the rich-resource language knowledge detailed descriptions of this step are presented in following sections 4.1 multi-task learning in mclas to model interactions between languages we need to share the decoder s parameters inspired by dong et al ( 2019 , we propose sharing the whole decoder to carry out both the translation and the summarization tasks speciﬁcally we substitute the generation target sa with the sequential concatenation of sa and sb sab { bos , ya lsep , yb , . , ya , ( ) [ eos } , . , yb , ya , yb 3 ( ) where bos and eos are the beginning and end token of the output summaries respectively and 6912 lsep is the special token used as the separator of sa and sb with the new generation target the decoder learns to ﬁrst generate sa and then generate sb conditioned on sa and da the whole generation process is illustrated in figure 2. formally we maximize the joint probability for monolingual and cross-lingual summarization lmclas ( ) + cid:80 n cid:48 t=1 logp ya |ya t=1 logp yb t da t |yb t sa da ( ) the loss function can be divided into two terms when generating sa the decoder conducts the ms task based on da corresponding to the ﬁrst term in equation 4 . when generating sb the decoder already knows the information of corresponding monolingual summaries in this way it performs the translation task for sa and the cls task for da , achieved by optimizing the second term in equation 4 . with the modiﬁcation of the target our model can easily capture interactions between cross-lingual summaries the trained model shows effectiveness in aligning the summaries not only the output tokens but also the attention distributions are aligned the model we designed leverages this phenomenon to enable monolingual knowledge to be transferred under low-resource scenarios detailed investigation is presented in section 6. we adopt transformers as our base model in addition we use multilingual bert devlin et al. 2019 to initialize the encoder improving its ability to produce multilingual representations additionally having tried many different position embedding and language segmentation embedding methods we ﬁnd that lsep is enough for the model to distinguish whether it is generating sb hence keeping the original position embedding vaswani et al. 2017 and employing no segmentation embedding are best for performance and efﬁciency 4.2 learning schemes for mclas under limited resources since our proposed framework enforces interactions between cross multilingual summaries it has further beneﬁts to the low-resource scenario as only a few training summary samples are available in a cross-language yet simply training from scratch can not make the best of our proposed model in low-resource scenarios hence we use a pre-training and ﬁne-tuning paradigm to transfer the rich-resource language knowledge scenarios minimum medium maximum full-dataset zh2ensum 5,000 0.3 ) 25,000 1.5 ) 50,000 3.0 ) 1,693,713 en2desum 2,619 0.6 ) 12,925 3.0 ) 25,832 6.0 ) 429,393 en2zhsum 1,500 0.4 ) 7,500 2.0 ) 15,000 4.0 ) 364,687 table : sample sizes of different low-resource scenarios three low-resource scenarios with various sample sizes are created for each dataset minimum medium and maximum represent sample sizes in the minimum low-resource scenario medium low-resource scenario and maximum low-resource scenario respectively first we train the model in a monolingual summarization dataset in this step the model learns how to produce a monolingual summary for a given document then we jointly learn ms and cls with few training samples optimizing equation 4 . we adopt similar initialization to existing cls methods which is introduced in section 5.3 5 experiments 5.1 datasets we conduct experiments on the en2zhsum zh2ensum cls datasets1 zhu et al. 2019 and a newly constructed en2desum dataset en2zhsum is an english-to-chinese dataset containing 364,687 training samples 3,000 validation and 3,000 testing samples the dataset is converted from the union set of cnn/dm hermann et al. 2015 and msmo zhu et al. 2018 using a round-trip translation strategy converted from the lcsts dataset zh2ensum contains 1,693,713 chinese-to-english training samples 3,000 validation and 3,000 testing samples to better verify the cls ability of mclas we construct a new english-to-german dataset en2desum , using the same methods proposed by zhu et al ( 2019 . we use wmt 19 english-german winner2 as our translation model to process the english gigaword dataset.3 we set the threshold t1 0.6 and t2 0.2. the ﬁnal en2desum contains 429,393 training samples 4,305 validation samples and 4,099 testing samples all the training samples contain a source document a monolingual summary and a cross-lingual summary for the full-dataset scenario we train the model with the whole dataset for low-resource scenarios we randomly select different amounts 1www.nlpr.ia.ac.cn/cip/dataset.htm : //github.com/pytorch/fairseq/ tree/master/examples/translation 3ldc2011t07 6913 minimum medium and maximum of training samples for all datasets to evaluate our model s performance under low-resource scenarios detailed numbers are presented in table . 5.2 training and inference we use multilingual bert mbert ( devlin et al. 2019 to initialize our transformer encoder the decoder is a transformer decoder with layers each attention module has different attention heads the hidden size of the decoder s self-attention is 768 and that of the feed-forward network is 2048. the ﬁnal model contains 296,046,231 parameters because the encoder is pretrained when the decoder is randomly initialized we use two separate optimizers for the encoder and the decoder liu and lapata 2019 . the encoder s learning rate ηe is set as 0.005 while the decoder s learning rate ηd is 0.2. warmup-steps for the encoder are 10,000 and 5,000 for the decoder we train the model on two titan rtx gpus for one day with gradient accumulation every steps dropout with a probability 0.1 is applied before all the linear layers we ﬁnd that the target vocabulary type doesn t have much inﬂuence on the ﬁnal result therefore we directly use mbert s subwords vocabulary as our target vocabulary nevertheless in case tokens would be produced in the wrong language we constructe a target token vocabulary for each target language in the inference period we only generate tokens from the corresponding vocabulary during the decoding stage we use beam search size ) and trigram block to avoid repetition length penalty is set between 0.6 and 1. all the hyperparameters are manually tuned using ppl and accuracy metric on the validation set 5.3 baselines we compare mclas in low-resource scenarios with the following baselines ncls cls model proposed by zhu et al ( 2019 . in low-resource scenarios we initialize our model with the pretrained ms model and then use a few samples to optimize equation 1 . ncls+ms multi-task framework proposed by zhu et al ( 2019 . we ﬁnd that ncls+ms fails to converge when it is partly initialized by the pretrained ms model the cls decoder is randomly initialized . hence we fully initialize the multitask model using the pretrained ms model speciﬁcally the two separate decoders are both initialized by the pretrained monolingual decoder then the model is optimized with equation 2 . tltran transformer-based late translation is a pipeline method first a monolingual summarization model summarizes the source document a translation model is then applied to translate the summary the summarization model is trained with monolingual document-summary pairs in three datasets speciﬁcally we continue using wmt 19 english-german winner as the translation model for en2desum some recent proposed models improve the performance of cls task methods ncls+mt tetran zhu et al. 2019 , and the system proposed by ouyang et al ( 2019 require external long document machine translation mt corpora the method proposed by cao et al ( 2020 requires not only parallel summaries but also document pairs translated by mt systems another method proposed by zhu et al ( 2020 requires bilingual lexicons extracted from large parallel mt datasets 2.08m sentence pairs from eight ldc corpora . we choose not to use these models as baselines since comparing mclas with them is unfair 5.4 automatic evaluation results the overall results under low-resource scenarios and full-dataset scenario are shown in table 2. we reimplement a variety of models and evaluate them using f1 scores of the standard rouge metric lin 2004 ( rouge-1 rouge-2 and rougel and bertscore4 zhang et al. 2019b . the following analysis is from our observations in the zh2ensum and en2desum datasets mclas achieves signiﬁcant improvements over baselines in all the low-resource scenarios it is worth noting that combining ncls+ms in our experiments does not bring much improvement to the ncls model we consider that this is because mbert has already provided multilingual encoding for our models however we ﬁnd that in the en2zhsum dataset mclas did not perform as well as that in the other two datasets we speculate that is due to the imbalance of english reference and chinese reference the average length of sa and sb in en2zhsum is 55.21 and 95.96 respectively zhu et al. 2019 . this condition largely breaks the alignment between languages leading to mclas : //github.com/tiiiger/bert_score 6914 zh2ensum en2desum r-l bertscore r-l bertscore bertscore models minimum low-resource scenario medium low-resource scenario maximum low-resource scenario full dataset scenario ncls ncls+ms mclas ncls ncls+ms mclas ncls ncls+ms mclas tltran ncls ncls+ms mclas r-1 20.93 20.50 21.03 26.42 26.86 27.84 29.05 28.63 30.73 33.64 35.60 34.84 35.65 r-2 5.88 5.45 6.03 8.90 9.06 10.41 10.88 10.63 12.26 15.58 16.78 16.05 16.97 17.58 17.25 18.16 22.05 22.47 24.12 24.32 24.00 26.51 29.74 30.27 29.47 31.14 0.5041 0.5025 0.5023 0.5373 0.5377 0.5464 0.5492 0.5485 0.5633 0.5835 0.5807 0.5770 r-1 17.59 17.52 19.19 23.55 23.60 27.22 25.84 25.59 30.31 28.57 31.61 31.33 36.48 r-2 5.01 5.27 5.91 8.09 8.35 10.09 9.78 9.58 12.32 13.31 14.24 13.86 17.21 16.58 16.57 18.43 22.13 22.14 26.00 24.25 23.96 28.88 26.34 29.63 29.31 34.86 0.7202 0.7198 0.7282 0.7400 0.7431 0.7575 0.7483 0.7484 0.7682 0.7680 0.7675 0.7897 en2zhsum r-l 21.20 21.07 21.17 23.79 25.39 25.26 26.52 26.64 26.41 27.02 30.23 29.24 30.09 r-2 12.45 12.38 13.17 15.88 18.09 18.10 19.86 19.87 19.75 12.20 24.28 23.51 24.60 r-1 34.14 33.96 32.03 35.98 38.95 37.28 40.18 39.86 38.35 30.20 44.16 42.68 42.27 0.7096 0.7102 0.6529 0.7298 0.7172 0.6839 0.7435 0.7445 0.6921 0.7407 0.7361 0.7069 table : f1 scores of rouge and bertscore in zh2ensum en2desum and en2zhsum dataset r-1 r-2 and r-l represents rouge-1 rouge-2 and rouge-l respectively models minimum medium cc 0.164 -0.021 0.000 0.236 cc fl if if fl -0.264 0.214 mclas -0.243 -0.386 -0.364 0.036 -0.221 -0.257 -0.129 -0.329 -0.186 ncls ncls+ms -0.371 -0.407 -0.286 -0.343 -0.536 -0.407 -0.179 -0.364 -0.214 gold 0.179 if 0.164 0.057 cc 0.464 0.300 0.529 0.257 0.221 0.879 0.629 0.500 0.671 fl maximum table : human evaluation results in zh2ensum dataset the best results are in bold scenarios minimum medium maxmium fleiss kappa overall agreement 60.48 51.35 50.16 0.37 0.22 0.20 table : fleiss kappa and overall agreement percent of our human evaluation results a higher value indicates higher agreements among participants the performing slightly weaker despite this results in en2desum and zh2ensum demonstrate that our proposed mclas model is effective for cls under limited resources finally our proposed model also has superior performance compared to baseline models given the full training dataset achieving the best rouge score in en2desum and zh2ensum datasets 5.5 human evaluation in addition to automatic evaluation we conduct a human evaluation to verify our model s performance we randomly chose 60 examples 20 for each low-resource scenario from the zh2ensum test dataset seven graduate students with high levels of ﬂuency in english and chinese are asked to assess the generated summaries and gold summaries from independent perspectives informativeness ﬂuency and conciseness we follow the best-worst scaling method kiritchenko and mohammad 2017 . participants are asked to indicate scenarios minimum low-resource scenario medium low-resource scenario maximum low-resource scenario models ncls ncls+ms mclas ncls ncls+ms mclas ncls ncls+ms mclas gold en2desum 13.48 +4.69 12.83 +4.04 7.80 −0.90 13.13 +4.34 12.90 +4.11 8.65 −0.14 13.37 +4.58 13.37 +4.58 8.46 −0.33 8.79 zh2ensum 18.49 +3.51 18.68 +3.70 13.16 −1.82 18.60 +3.62 18.57 +3.59 13.10 −1.88 18.44 +3.46 18.75 +3.77 12.83 −2.15 14.98 table : target summary length generated by various models the best results are in bold models ms-pretrain ncls+ms mclas en2desum zh2ensum r-1 39.16 36.06 45.59 r-2 19.21 16.84 23.77 r-l 35.42 32.72 42.51 r-1 41.71 39.98 41.72 r-2 27.76 26.03 27.69 r-l 37.97 36.13 37.92 table : monolingual summary results in zh2ensum and en2zhsum datasets ms-pretrain refers to the pretrained model for monolingual summarization the best and worst items from each perspective the result scores are calculated based on the percentage of times each system is selected as best minus the times it is selected as worst hence ﬁnal scores range from -1 worst to ( best . results are shown in table 3. as the data size increases all the models achieve better results our proposed mclas outperformed ncls and ncls+ms in all the metrics we notice that mclas is especially strong in conciseness this phenomenon will be analyzed in section 5.7 we show fleiss kappa scores of our conducted human evaluation in table , which demonstrates a good inter-agreement among the participants 6915 metrics r-1 recall r-1 precision r-2 recall r-2 precision r-l recall r-l precision avg length ms-pretrain ncls+ms ground truth 58.37 30.45 30.21 14.65 52.97 27.51 52.62 28.41 25.91 12.99 47.89 25.71 mclas 46.88 46.31 24.54 24.11 43.74 43.15 18.64 +9.53 17.98 +8.87 9.15 +0.04 9.11 table : analysis on monolingual summary generation ability of mclas trained with en2desum dataset figure : an example of generated cross-lingual summaries important phrases are bold while incorrect information generated by each model is italicized nonﬂuent parts in sentences are bold and italicized conditioning cross-lingual summaries on relatively precise monolingual summaries 5.8 analysis on monolingual summarization modeling interactions between languages brings many advantages speciﬁcally we ﬁnd that mclas can preserve more monolingual summarization knowledge than the ncls+ms model during low-resource ﬁne-tuning or even promote its performance we generate monolingual summaries with models trained in the maximum lowresource scenario in table , we can clearly see that mclas retains more monolingual summarization knowledge in the zh2ensum dataset in the en2desum dataset monolingual summarization performance is even signiﬁcantly improved we speculate that this is due to mclas s ability to provide the interactions between languages we focus speciﬁcally on digging into results in en2desum evaluating its detailed rouge and average summary length presented in table 7. we ﬁnd that rouge improvement mainly resulted from precision while recall barely decrease the performances this and the avg length metric shows that mclas produces more precise summaries while retaining most of the important information leading to the metric increase in rouge figure : line and column chart of en2desum and zh2ensum results lines represent models initialized with pretrained monolingual summarization model columns represent models trained from scratch 5.6 analysis on initialization methods we use a monolingual summarization model to initialize our model however whether this initialization method works is still in question therefore we compare our models with non-initialized models shown in figure 3. among the three datasets the initialization methods bring a huge improvement to all of the models 5.7 analysis on summary length one of the goals of automatic summarization is to produce brief text yet many neural auto-regressive models tend to produce a longer summary to improve the recall metric results in table show that interactions enable mclas to generate shorter summaries than other models which more closely resembles human summaries we can safely conclude that mclas can keep the summary in a fairly appropriate length leading to concise generated summaries we speculate that this is due to its ability to capture interactions between languages 6916 figure : different types of self-attention heads in mclas s decoder the x-axis and y-axis are both concatenated source-language summary sa and targetlanguage summary sb tokens darker color shows the more highly related associations between tokens the horizontal line in self head represents the lsep token some attention heads attend to lsep to conﬁrm whether it is generating a cross-lingual summary figure : different types of encoder-decoder attention heads in mclas s decoder the x-axis represents concatenated source-language summary sa and targetlanguage summary sb tokens while the y-axis is the document da tokens in news texts important information often gathers in the front part of the document we only retain the informative part of the y-axis omitting the blank part that the model do not attend to 5.9 case study in figure , on the zh2ensum dataset there is a list comparing the reference summary and outputs of models trained in the maximum low-resource scenario clearly the ncls model loses the information two cars and generates the wrong information no.2 factory . the ncls+ms model is not accurate when describing the number of injured people dropping important information more than . additionally the ncls+ms model also has ﬂuency and repetition issues “ in zhengzhou appears twice in its generated summary in contrast mclas captures all of this information mentioned in both its chinese and english output and the english summary is well aligned with the chinese summary finally all of the models ignore the information foxconn printed on the body of the car . see appendix a for more examples 6 probing into attention heads we have observed a successful alignment between sa and sb produced by our model in section 5.9. in this section we dig into this and analyze how the model learns the relationships for a cls task from document da to sb our hypotheses are ( ) the uniﬁed decoder is implicitly undertaking translation from sa to sb ( ) the uniﬁed decoder also conducts both monolingual and cross-lingual summarization to verify these hypotheses we visualize attention distributions of the transformer decoders trained on en2zhsum neural models can be explicitly explained using probing into the attention heads michel et al. 2019 voita et al. 2019 . we follow the previous work and visualize the function of all attention heads in the decoder to verify the relationships of the concatenated cross-lingual summaries i.e. translation and cross-lingual document-summary pairs i.e. summarization . 6.1 analysis on translation we assume that the decoder translates only if the source summary sa and the target summary sb align well this means that mclas is transferring knowledge from sa to sb we visualize and probe all 48 self-attention heads in the uniﬁed decoder we ﬁnd 23 47.9 ) translation heads deﬁned as the heads attending from yb to the corresponding words in language a. these heads undertake a translation function 19 39.6 ) heads are local heads attending to a few words before them and modeling context information 12 25 ) heads are self heads which only attend to themselves to retain the primary information some of the heads can be categorized into two types note that all of the heads behave similarly across different samples we ﬁnd that most of the heads are translation heads indicating that our uniﬁed decoder is translating sa into sb we sample some representative heads in figure to show their functionalities 6917 6.2 analysis on summarization to analyze whether the decoder for sb is simply translating from sa or that it also summarizes the source document we visualize the distribution of 48 encoder-decoder attention heads we ﬁnd 28 58.3 ) summarization heads that attend to the document s important parts when generating both the monolingual summary and the cross-lingual summary we also ﬁnd 20 41.7 ) translation heads which focus on the source document when generating sa while focusing on nothing when generating sb we speculate that summarization heads are responsible for the summarization function and that translation heads cut down the relation between sb and source document da leaving space for translation again all the heads behave similarly across different samples we select two representative samples in figure 6. the existence of both summarization and translation heads in encoder-decoder attention components supports our views the uniﬁed decoder simultaneously conducts translation and summarization therefore our model enhances the interactions between different languages being able to facilitate cross-lingual summarization under lowresource scenarios see appendix for detailed visualization results 7 discussions an ideal low-resource experiment should be conducted with real low-resource languages although possible it takes much effort to acquire such datasets hence it is the second-best choice that we simulate our low-resource scenarios by artiﬁcially limiting the amount of the available data some may question it about the feasibility of our method in real low-resource languages since machine translation systems which is used to generate document-summary pairs would be of lower quality for truly low-resource languages for this concern we consider it still possible to acquire thousands of high-quality human translated parallel summaries as duan et al ( 2019b adopt on their test set to apply our method 8 conclusion in this paper we propose a novel multi-task learning framework mclas to achieve cross-lingual abstractive summarization with limited parallel resources our model shares a uniﬁed decoder that sequentially generates both monolingual and crosslingual summaries experiments on two crosslingual summarization datasets demonstrate that our framework outperforms all the baseline models in low-resource and full-dataset scenarios acknowledgements this work is supported by the joint funds of the national natural science foundation of china grant no u19b2020 , the funds of the integrated application software project we appreciate the helpful discussions with sanxing chen jia-ao zhan xuyang lu xiao liu and yuxiang zhou we also thank all the anonymous reviewers for their insightful suggestions references yue cao hui liu and xiaojun wan 2020. jointly learning to align and summarize for neural crosslingual summarization in proceedings of the 58th annual meeting of the association for computational linguistics pages 6220–6231 zewen chi li dong furu wei wenhui wang xianling mao and heyan huang 2020. cross-lingual natural language generation via pre-training in aaai pages 7570–7577 jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirectional transformers for language understanding in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 4171–4186 li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao-wuen hon 2019. uniﬁed language model pre-training for natural language understanding and generation in advances in neural information processing systems volume 32. curran associates inc. xiangyu duan mingming yin min zhang boxing chen and weihua luo 2019a zero-shot crosslingual abstractive sentence summarization through teaching generation and attention in proceedings of the 57th annual meeting of the association for computational linguistics pages 3162–3172 xiangyu duan mingming yin min zhang boxing chen and weihua luo 2019b zero-shot crosslingual abstractive sentence summarization through teaching generation and attention in proceedings of the 57th annual meeting of the association for computational linguistics pages 3162–3172 florence italy association for computational linguistics 6918 jiatao gu yong wang yun chen victor ok li and kyunghyun cho 2018. meta-learning for lowresource neural machine translation in proceedings of the 2018 conference on empirical methods in natural language processing pages 3622–3631 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser and illia polosukhin 2017. attention is all in advances in neural information proyou need cessing systems pages 5998–6008 karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom 2015. teaching machines to read and comprehend in advances in neural information processing systems pages 1693–1701 svetlana kiritchenko and saif mohammad 2017. bestworst scaling more reliable than rating scales a case study on sentiment intensity annotation in proceedings of the 55th annual meeting of the association for computational linguistics volume : short papers , pages 465–470 chin-yew lin 2004. rouge a package for automatic evaluation of summaries text summarization branches out yang liu and mirella lapata 2019. text summarizain proceedings of tion with pretrained encoders the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 3721–3731 paul michel omer levy and graham neubig 2019. in adare sixteen heads really better than one vances in neural information processing systems pages 14014–14024 saeid motiian quinn jones seyed mehdi iranmanesh and gianfranco doretto 2017. few-shot adversarial domain adaptation in nips jessica ouyang boya song and kathy mckeown 2019. a robust abstractive system for cross-lingual summarization in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2025–2031 minneapolis minnesota association for computational linguistics elena voita david talbot fedor moiseev rico sennrich and ivan titov 2019. analyzing multi-head self-attention specialized heads do the heavy liftin proceedings of the ing the rest can be pruned 57th annual meeting of the association for computational linguistics pages 5797–5808 xiaojun wan 2011. using bilingual information for in procross-language document summarization ceedings of the 49th annual meeting of the association for computational linguistics human language technologies pages 1546–1555 xiaojun wan huiying li and jianguo xiao 2010. cross-language document summarization based on machine translation quality prediction in proceedings of the 48th annual meeting of the association for computational linguistics pages 917–926 yaqing wang quanming yao james kwok and lionel ni 2020. generalizing from a few examples a survey on few-shot learning acm computing surveys csur , 53 3 :1–34 wang yan jordan yap and greg mori 2015. multitask transfer methods to improve one-shot learning for multimedia event detection in bmvc pages 37– 1. jin-ge yao xiaojun wan and jianguo xiao 2015. phrase-based compressive cross-language summarization in proceedings of the 2015 conference on empirical methods in natural language processing pages 118–127 jiajun zhang yu zhou and chengqing zong 2016. abstractive cross-language summarization via translation model enhanced predicate argument structure fusing ieee/acm transactions on audio speech and language processing 24 10 :1842–1853 alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever 2019. language models are unsupervised multitask learners openai blog 1 8 :9. jingqing zhang yao zhao mohammad saleh and peter liu 2019a pegasus pre-training with extracted gap-sentences for abstractive summarization arxiv preprint arxiv:1912.08777. shi-qi shen yun chen cheng yang zhi-yuan liu mao-song sun et al 2018. zero-shot cross-lingual ieee/acm transacneural headline generation tions on audio speech and language processing 26 12 :2319–2327 kaitao song xu tan tao qin jianfeng lu and tieyan liu 2019. mass masked sequence to sequence pre-training for language generation in international conference on machine learning pages 5926–5936 tianyi zhang varsha kishore felix wu kilian weinberger and yoav artzi 2019b bertscore in internaevaluating text generation with bert tional conference on learning representations junnan zhu haoran li tianshang liu yu zhou jiajun zhang and chengqing zong 2018. msmo multimodal summarization with multimodal output in proceedings of the 2018 conference on empirical methods in natural language processing pages 4154–4164 6919 junnan zhu qian wang yining wang yu zhou jiajun zhang shaonan wang and chengqing zong 2019. ncls neural cross-lingual summarization in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 3045–3055 junnan zhu yu zhou jiajun zhang and chengqing zong 2020. attend translate and summarize an efﬁcient method for neural cross-lingual summarizain proceedings of the 58th annual meeting tion of the association for computational linguistics pages 1309–1321 6920 a samples we list some samples from outputs of various models samples from en2desum dataset are shown in figure 7. samples from zh2ensum dataset are shown in figure 8. we randomly selected one sample from each low-resource scenario b attention distributions in section probing into attention heads , we selected some representative attention heads we list all of our trained attention heads among transformer decoder layers in figure and figure 10 for reference figure : examples of models trained in en2desum dataset 6921 figure : examples of models trained in zh2ensum dataset 6922 figure : visualization of all the 48 self attention heads the x-axis and y-axis are both concatenated sourcelanguage summary sa and target-language summary sb tokens each row contains all of the attention heads of corresponding layer from bottom to the top the darker color shows the more highly related associations between tokens 6923 figure 10 visualization of all the 48 encoder-decoder attention heads the x-axis is concatenated source-language summary sa and target-language summary sb tokens while the y-axis is document da tokens each row contains all of the attention heads of corresponding layer from bottom to the top the darker color shows the more highly related associations between tokens 6924 progressive generation of long text with pretrained language models bowen tan1 zichao yang1 maruan al-shedivat1 eric p. xing1,2,3 zhiting hu1,4 1carnegie mellon university 4uc san diego btan2 zichaoy alshedivat epxing @ andrew.cmu.edu zhh019 ucsd.edu 2petuum inc. 3mbzuai abstract large-scale language models lms pretrained on massive corpora of text such as gpt-2 are powerful open-domain text generators however as our systematic examination reveals it is still for such models to generate coherent long passages of text e.g. 1000 tokens , especially when the models are ﬁne-tuned to the target domain on a small corpus previous planning-then-generation methods also fall short of producing such long text in various domains to overcome the limitations we propose a simple but effective method of generating text in a progressive manner inspired by generating images from low to high resolution our method ﬁrst produces domain-speciﬁc content keywords and then progressively reﬁnes them into complete passages in multiple stages the simple design allows our approach to take advantage of pretrained lms at each stage and effectively adapt to any target domain given only a small set of examples we conduct a comprehensive empirical study with a broad set of evaluation metrics and show that our approach signiﬁcantly improves upon the ﬁne-tuned large lms and various planning-then-generation methods in terms of quality and sample efﬁciency human evaluation also validates that our model generations are more coherent.1 introduction generating coherent long text e.g. 1000s of tokens is useful in myriad applications of creating reports essays and other long-form content yet the problem is particularly as it demands models to capture global context plan content and produce local words in a consistent manner prior studies on long text generation have typically limited to outputs of 50-200 tokens shen et al. 2019 bosselut et al. 2018 zhao et al. 2020 . 1code available at : //github.com/ tanyuqian/progressive-generation figure : results of large-scale lms gpt-2 and bart ﬁne-tuned on 10k stories coherence of text is evaluated by bert next sentence prediction nsp score where x-axis is the position of the evaluated sentences in the passage there is a signiﬁcant gap in coherence between text by human and text by large-scale lms our proposed progen instead generates more coherent samples close to human text recent large-scale pretrained language models lms , such as gpt-2 radford et al. 2019 and bart lewis et al. 2020 , emerged as an impressive open-ended text generator capable of producing surprisingly ﬂuent text the massive lms are typically pretrained on large corpora of generic text once and then ﬁne-tuned with small domainspeciﬁc data the latest work has mostly focused on the regime of relatively short text with low hundreds of tokens for example holtzman et al ( 2020 ; see et al ( 2019 ; hua and wang 2020 studied gpt-2 and bart generations with a maximum length ranging from 150 to 350 tokens in this work we study the problem of generating coherent much longer passages of text e.g. 1000 tokens . gpt-3 brown et al. 2020 was reported to produce long essays yet the results seem to need extensive human curations e.g. marketmuse gardian , and the system is not publicly available to adapt to arbitrary desired domains in this work we examine ﬁne-tuning of largescale lms for domain-speciﬁc generation of extra 4313 proceedings of the 2021 conference of the north american chapter of the association for computational linguistics human language technologies pages 4313–4324 june 6–11 2021 ©2021 association for computational linguistics long text we ﬁnd that samples produced by gpt-2 ﬁne-tuned on small domain-speciﬁc corpora exhibit various imperfections including excessive repetitiveness and incoherence between sentences far apart figure measures the coherence of text generated by the ﬁne-tuned gpt-2 w.r.t the bert next sentence prediction devlin et al. 2019 score as the ﬁgure shows gpt-2 models regardless of the model size exhibit a signiﬁcant gap in the score compared with human text hence falling short in generating coherent text we hypothesize that the problem is mainly caused by the sequential generation order of the lms which makes global content planning of the passage difﬁcult especially when the generated text is long and contains thousands of words one could potentially adopt the recent planning-thengeneration or non-monotonic methods sec ) yet those methods either require specialized neural architectures that need costly retraining for each domain gu et al. 2019 stern et al. 2019 chan et al. 2019 fan et al. 2019 , or rely on dedicated intermediate content plans e.g. summaries srl labels ( fan et al. 2019 yao et al. 2019 with limited ﬂexibility and producing sub-optimal results as shown in our experiments to overcome the limitations we introduce a new method for progressive generation of text progen . we observe that generation of some words e.g. stop words does not require many contexts while other words are decisive and have long-term impact on the whole content of the passage motivated by this observation our approach ﬁrst produces a sequence of most informative words then progressively reﬁnes the sequence by adding ﬁnergrained details in multiple stages until completing a full passage the generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation figure ) the intermediate words produced at each stage are deﬁned based on a simple tf-idf informativeness metric the approach enjoys several core advantages ( ) although the progressive approach implements a conceptually non-monotonic generation process generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic lms the lms at different stages are easily ﬁne-tuned to accommodate a target domain using only small independently constructed data intuitively each lm is addressing a sub-task of mapping a sequence to a ﬁner-resolution one which is much simpler than the overall task of mapping from conditions to full passages of text in this work we use bart lewis et al. 2020 for generation at each stage though one can also plug in other off-the-shelf lms as seen from figure , progen can generate more much coherent text compared with gpt-2 and nearly match human text in terms of the bertnsp score ( ) in contrast to the typical 2-stage planning-then-generation in prior work the simple progressive strategy offers added ﬂexibility for an arbitrary number of intermediate stages yielding improved results ( ) the training data for each stage is extracted from domain corpus using the simple tf-idf metric without need of additional resources e.g. pretrained summarization models as in prior work making the method broadly applicable to various domains and languages we conduct extensive empirical studies on the cnn news hermann et al. 2015 and writingprompts fan et al. 2018 corpora evaluating various systems by a wide-range of automatic metrics as well as human judgement results show that progen achieves strongly improved performance by decomposing the generation into more progressive stages our method produces diverse text passages of higher quality and coherence than a broad set of models including ﬁne-tuned gpt-2 bart and other various planning-then-generation strategies 2 related work content planning in generation the idea of separate content planning and surface realization has been studied in early text generation systems reiter and dale 1997 . recent neural approaches have also adopted similar planning-thengeneration strategies for data-to-text moryossef et al. 2019 puduppully et al. 2019 , storytelling fan et al. 2019 yao et al. 2019 xu et al. 2020 , machine translation ford et al. 2018 , and others hua and wang 2019 yao et al. 2017 . these models often involve customized architectures incompatible with the existing large lms scaling those models for long text generation thus can require expensive training which restricts systematic studies on the other hand it is possible to adopt some of the content planning strategies e.g. summaries or srl sequences as the plans fan et al. 2019 ) and repurpose pretrained lms for generation in each stage however these strategies 4314 with dedicated intermediate plans and a pre-ﬁxed number typically ) of stages can have limited ﬂexibility leading to sub-optimal results as shown in our empirical study besides creating training data for planning requires additional resources e.g. pretrained summarization models or srl models which are not always available e.g. in certain domains or for low-resource languages . in contrast we propose a simple way for designing the intermediate stages based on word informativeness which can ﬂexibly increase the number of stages for improved results and easily create training data for all stages without additional models non-monotonic generation and reﬁnement another relevant line of research is non-monotonic generation welleck et al. 2019 gu et al. 2019 stern et al. 2019 chan et al. 2019 zhang et al. 2020 , inﬁlling zhu et al. 2019 shen et al. 2020 qin et al. 2020 , or reﬁnement lee et al. 2018 novak et al. 2016 mansimov et al. 2019 kasai et al. 2020 that differs from the restricted left-toright generation in conventional lms again those approaches largely depend on specialized architectures and inference making them difﬁcult to be integrated with the powerful pretrained lms the prior studies have focused on generating short text our proposed coarse-to-ﬁne progressive generation conceptually presents a non-monotonic process built upon the pretrained monotonic lms which permits fast adaptation to any target domain and generation of much longer text long text generation previous work has made attempts to generate text of up to two or three hundred tokens those methods often adopt the similar idea of planning-then-generation as above shen et al. 2019 zhao et al. 2020 bosselut et al. 2018 see et al. 2019 hua and wang 2020 rashkin et al. 2020 . another line of work instead focuses on extending the transformer architecture vaswani et al. 2017 to model longer text sequences e.g. dai et al. 2019 wang et al. 2020 choromanski et al. 2021 etc . for example liu et al ( 2018 used a hybrid retrieval-generation architecture for producing long summaries dai et al ( 2019 showed long text samples qualitatively our work systematically examines the pretrained lms in generating long domain-speciﬁc text and proposes a new approach that empowers pretrained lms for producing samples of signiﬁcantly higherquality 3 progressive generation of text one of the main in generating long coherent passages is modeling long-range dependencies across the entire sequences e.g. 1000 tokens . we propose a progressive generation approach that is conceptually simple yet effective intuitively progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences contrary to generating everything from left to right from scratch our progressive generation allows the model to ﬁrst plan globally and then shift attention to increasingly ﬁner details which results in more coherent text figure illustrates the generation process 3.1 generation process let : [ y1 y2 . . yt be the output text where each yi is a token of language a word or a subword . the output sequences are generated either conditionally on any other information ( e.g. generations of a story given a prompt , or unconditionally in which case we assume ≡ while keeping the same notation . instead of generating the full passage directly we propose to add multiple intermediate stages x c1 c2 ··· ck y where for each stage ∈ 1 . . k , ck is an intermediate sequence containing information of the passage at certain granularity for instance at the ﬁrst stage c1 can be seen as a highest-level content plan consisting of the most informative tokens such as key entities then based on the plan we gradually reﬁne them into subsequent ck each of which contains ﬁner-grained information than that of the preceding stage at the ﬁnal stage we reﬁne ck into the full passage by adding the least informative words e.g. stop words . the generation process corresponds to a decomposition of the conditional probability as p y { ck |x = ( c1|x k=2p ck|ck−1 x p y|ck x . πk 1 as the above intuition ck at early stages as the high-level content plans should contain informative or important words to serve as skeletons for subsequent enrichment we next concretely deﬁne the order of generation namely which words should each stage generates speciﬁcally we propose a simple method 4315 condition progressive generation of text generation `` shut the dog up ’ lm1 lmk jeep dog barking ofﬁcer skinny jeep sandy lm2 shouted jeep dog circles vehicle barking ofﬁcer yellow skinny animal circling jeep spit vehicle tumbling rough sandy adjusting gun proceeded canine dog barking x 1 2 y shouted my head ofﬁcer from the jeep the dog was running circles around our vehicle barking at the people inside the ofﬁcer tapped my shoulder and pointed to the yellow skinny animal circling our jeep “ but sir .. ” managed to spit out before he took both his hands and pushed me out of the vehicle i went tumbling out and landed on the rough sandy ground i stood up adjusting the gun hanging from my shoulder and proceeded to walk towards the canine the dog stopped its barking and shifted its black eyes to me “ come here little pup hey come here i m not going to hurt ya ” said trying to coax it nearer to me actually i didn t know if was going to hurt the little mutt or not yet reaching my hand towards my waist i pulled off a tiny bit of my rations i held it out my hand with the ration laying on my open palm the dog perked it s ears and came a few inches closer to me [ ] figure : progressive generation of long text given any condition x. each stage reﬁnes the results from the previous stage by adding ﬁner-grained details added content at each stage is highlighted in different colors that constructs a vocabulary vk for each stage , based on the importance of words in the target domain each particular stage only produces tokens belonging to its vocabulary vk by the progressive nature of the generation process we have v1 ··· vk v. that is v1 contains the smallest core set of words in the domain and the vocabularies gradually expand at later stages until arriving the full vocabulary v. note that vocabularies in later stages are supersets of those in earlier stages this allows the later stages to remedy and polish potential mistakes made in earlier stages when necessary we discuss the construction of the vocabularies in the below stage-wise vocabularies based on word importance given a text corpus of the target domain with the full vocabulary , we deﬁne the importance scores of words in based on the tf-idf metric we then rank all the words and assign the top vk words to the intermediate vocabulary vk here vk is a hyper-parameter controlling the size of vk more concretely for each word ∈ , we ﬁrst compute its standard tf-idf score salton and mcgill 1986 in each document ∈ , which essentially measures how important is to d. the importance of the word in the domain is then deﬁned as the average tf-idf score across all documents containing : importance w d = cid:2 d∈d tf_idf w d df w d , 2 where tf_idf w d is the tf-idf score of word in document ; and df w d is the document algorithm training for progressive text generation inputs domain corpus vocabulary sizes for stages pretrained lms e.g gpt-2 or bart 1 construct stage-wise vocabularies vk based on word : extract intermediate sequences c∗ } using vk ; add importance eq ( ) data noises sec 3.2 3 fine-tune all lms independently sec 3.2 output fine-tuned lms for generation at all stages in a progressive manner frequency i.e. the number of documents in the corpus that contain the word w. pretrained language models as building blocks compared to many of the previous planning-thengeneration and non-monotonic generation methods one of the key advantages of our progressive generation design is the direct compatibility with the powerful pretrained lms that perform left-to-right generation speciﬁcally although our approach implements a non-monotonic generation process that produces importance words ﬁrst we can generate intermediate sequences ck at each stage still in a left-to-right manner thus we can plug pretrained lm such as gpt-2 or bart into each stage to carry out the generation as described more in section 3.2 for each stage , we can conveniently construct stage-speciﬁc training data from the domain corpus using the stage-wise vocabulary vk and ﬁne-tune the stage-k lm in order to generate intermediate sequences at the stage that are pertaining to the target domain one can add masks on the pretrained lm s to 4316 ken distributions to ensure the stage-k lm only produces tokens belonging to vk in practice we found it is not necessary as the pretrained lm can usually quickly learns the pattern through ﬁnetuning and generate appropriate tokens during inference in our experiments we use bart for all stages since bart is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new ( for the ﬁrst stage in an unconditional generation task we simply set = . we note that gpt2 and other relevant pretraiened lms can indeed also be used as a conditional generator radford et al. 2019 liu et al. 2018 and thus be plugged into any of stages 3.2 training our approach permits straightforward training/ﬁnetuning of the pretrained lms at different stages given the domain corpus d. in particular we can easily construct independent training data for each stage and train all lms in parallel note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work making our approach directly applicable to a potentially broader set of domains and languages we plan to explore the use of our method in multi-lingual setting in the future more concretely for each stage , we use the stage vocabularies vk−1 and vk to ﬁlter all relevant tokens in the documents as training data that is given a document we extract the subsequence c∗ k−1 of all tokens from the document that are belonging to vk−1 and similarly extract sub-sequence c∗ k−1 and c∗ are then used as the input and the ground-truth output respectively for training the lm at stage with maximum likelihood learning therefore given the stage-wise vocabularies vk , we can automatically extract training data from the domain corpus for different stages and train the lms separately k belonging to vk the c∗ in the multi-stage generation the intermediate sequences are not natural language yet we found that ﬁne-tuning pretrained lms such as bart and gpt-2 to generate the intermediate sequences is indeed very efﬁcient in terms of data and computation we tried training other models such as small sequence-to-sequence models and n-gram models from scratch which we found is much harder requiring more data or yielding inferior performance this again highlights the importance of using pretrained lms as enabled by our simple method design stage-level exposure bias and data noising in the above training process the outputs of each lm are conditioning on the ground-truth input sequences extracted from the real corpus in contrast at generation time the lm takes as inputs the imperfect sequences produced at the previous stage which can result in new mistakes in the outputs since the lm has never be exposed to noisy inputs during training thus the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages the phenomenon resembles the exposure bias issue ranzato et al. 2016 of sequential generation models at token level where the model is trained to predict the next token given the previous ground-truth tokens while at generation time tokens generated by the model itself are instead used to make the next prediction to alleviate the issue and increase the robustness of each intermediate lm we draw on the rich literature of addressing token-level exposure bias xie et al. 2017 tan et al. 2019 . speciﬁcally during training we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram n { , , , } and replacing it with another randomly sampled n-gram the data noising encourages the lms to learn to recover from the mistakes in inputs leading to a more robust system during generation 4 experiments 4.1 setup domains we evaluate on two text generation domains including ( ) cnn news hermann et al. 2015 for unconditional generation ( ) writingprompts fan et al. 2018 for conditional story generation the task is to generate a story given a prompt the two datasets are chosen since they both contain long documents with cnn s average and maximum length being 512 and 926 and writingprompts s being 437 and 942 respectively to demonstrate the data efﬁciency of our approaches adapting to target domains we sample 1,000 documents in each dataset for training model conﬁgs we use barts for all stages of generation due to computation limitations we experiment models with , , 4-stages generations in 4317 our 2-stage model our ﬁrst stage covers about 25 of all content in the 3-stage model the ﬁrst and second stages cover 15 and 25 of all content respectively and in the 4-stage model our ﬁrst three stages cover 15 , 20 , 25 of all content for model training we follow the same protocol as see et al. 2019 to ﬁne-tune all pretrained models until convergence to combat exposure bias we add noise to the training data as described in sec 3.2 with the probability of replacing 1,2,3,4grams 0.1/0.05/0.025/0.0125 in the generation phase we use top-p decoding holtzman et al. 2020 with = 0.95 to generate 1024 tokens at maximum experiments were conducted with rtx6000 gpus it took around hours for model ﬁne-tuning and generation with a single gpu comparison methods we compare with a wide range of baselines categorized into two groups ( ) the large pretrained lms including bart lewis et al. 2020 and gpt-2 in both small and large sizes radford et al. 2019 . the lms generate text in a standard left-to-right manner ( ) progressive generation with various strategies adopted in the prior planning-then-generation work same as our proposed method each stage adapts a pretrained bart for generation speciﬁcally summary ﬁrst generates a short summary text as the content plan and conditioning on the summary produces the full passage of text fan et al. 2019 . for training summaries are obtained using the state-of-the-art pretrained cnn news summarization model based on bart keyword ﬁrst generates a series of keywords based on which the full text is generated in the next stage following yao et al. 2019 , the keywords are extracted with the rake algorithm rose et al. 2010 for training srl follows the recent work fan et al. 2019 by ﬁrst generating a sequence of predicates and arguments and then producing the full text conditionally the same semantic role labeling tool as in the prior work is used here to create training data srl+ner and srl+coref further augment the srl method by an additional stage of generating entity anonymized text conditioning on the predicates sequence prior to the ﬁnal stage fan et al. 2019 . srl+ner uses an ner model to mask all entities while srl+coref applies coreference resolution to mask all clusters of mentions we use the same ner and coreference tools as in fan et al. 2019 . finally as a reference we also present the results of human-written text i.e. the text in the dev set . 4.2 automatic evaluation 4.2.1 evaluation metrics to evaluate the generation quality for the domainspeciﬁc open-ended generation as studied here we primarily measure the closeness between two sets of text one generated by the model and the other the real text from the target domain we evaluate with a broad array of automatic metrics including lexical-based quality metrics and semanticbased quality metrics we also evaluate the generation diversity ms-jaccard msj is a lexical-based metric montahaei et al. 2019 , where msj-n measures the similarity of n-grams frequencies between two sets of text with jaccard index tf-idf distance tid is deﬁned as the distance between the average tf-idf features of two text sets we use it as an additional lexical-based quality measure fréchet bert distance fbd is a semanticbased metric montahaei et al. 2019 that measures the fréchet distance in the bert feature space between the generated and real text by using the bert features from ( ) medium m , and deep d layers we can compute fbd-s/m/d respectively backward bleu b-bleu is a diversity metric shi et al. 2018 measuring how well the generated text covers n-grams occurred in the test set harmonic bleu ha-bleu ( shi et al. 2018 is an aggregated quality and diversity metric that incorporates both the standard bleu i.e. precision and the backward bleu i.e. recall . 4.2.2 results figures and show the results of the various systems on the news and story domains respectively measured with different metrics against test set we give more complete results in the appendix we can see that our progressive generation approach consistently outperforms the standard single-stage lms gpt2-small gpt2-large and bart by a large margin on almost all metrics in both domains further by increasing the number of progression stages our method steadily achieves even stronger performance this highlights the beneﬁts of the ﬂexible progressive generation strategy the various models using pretrained lms with previous planning-then-generation strategies show 4318 lexical-based quality metrics semantic-based quality metric baselines ours human 18.5 17.5 16.5 15.5 14.5 13.5 human 4 12 16 human 20 40 60 baselines ours baselines ours tid fbd-d diversity baselines ours aggregated diversity and quality human baselines ours msj-4 human 33 31 29 27 25 32 31 30 29 28 27 26 25 b-bleu4 ha-bleu4 figure : results on the cnn news domain measured by different metrics for tid and fbd the lower value the better more results msj-n b-bleun and ha-bleun with different values and fbd-s/m are included in the appendix the three sets of comparison methods are shown in different colors with our progen in red standard large lms in blue and the various models with previous planning strategies in green human results are shown as dashed lines often indicating the best potential performance except for the diversity related metrics . lexical-based quality metrics semantic-based quality metric aggregated diversity and quality human baselines ours baselines ours 22 21 20 19 18 17 16 15 human 10 20 30 40 50 60 baselines ours human 37 36 35 34 33 32 31 30 msj-4 fbd-d ha-bleu4 figure : results on the story domain measured by different metrics more complete results are in appendix mixed results across the different metrics for example summary achieves strong performance in terms of the semantic-based quality metric fbd-d partially because the summaries are closer to the real text in the bert feature space , but signiﬁcantly falls behind other models in terms of diversity b-bleu4 and other quality metrics like msj and ha-bleu similarly the srl-based methods give only mediocre results in terms of the semanticbased fbd-d. in contrast our approach maintains a relatively consistent performance level in particular our 4-stage model progen-4 is steadily among the best across all metrics further validating fluency bart gpt2-small summary progen-4 ours 4.59 4.42 4.39 4.46 coherence passage 2.95 3.41 3.37 3.83 sentence % 70.29 74.69 76.19 86.22 table : human evaluation results on cnn the advantage of the proposed simple yet ﬂexible multi-stage generation these results also indicate the necessity of using a large diverse set of automatic metrics for a comprehensive evaluation and motivate human studies for further assessment 4319 fbd-d msj-4 ha-bl4 30.45 31.39 30.64 30.72 31.32 30.86 16.50 16.25 16.68 16.65 16.96 16.85 39.94 47.18 38.30 39.64 36.49 39.78 progen-2 -noise progen-3 -noise progen-4 -noise table : effect of noise on cnn fbd-d tid 6.2 39.94 30.16 3.5 2.6 25.63 progen-2 goldplan human table : goldplan results on cnn figure : sample efﬁciency on the story domain with the fbd metric the lower the better . first stage beckham \n liverpool bayern chelsea \n beckham chelsea mancini \n liverpool \n club ... second stage beckham newcastle \n footballer acquisition transfer liverpool bayern munich chelsea munich \n beckham chelsea season italian siro gianluigi mancini season italy \n season liverpool season \n wembley club \n ... final stage ( cnn – england striker ashley beckham has joined premier league strugglers newcastle united \n england football association chairman stephen mcclaren conﬁrmed the acquisition of the 20-year-old england international on wednesday making the move ofﬁcial following a lucrative transfer agreement with liverpool bayern munich and chelsea.\n beckham spent the ﬁrst of this year on loan at chelsea and did not start a single match during the season at the italian capital s san siro despite being named the player of the season of the year by italy \n however he played 17 times last season and 12 times this season for liverpool making 33 league appearances and scoring seven goals \n he is currently third on the all-time england goalscoring list behind only england manager alex ferguson and new club teammate paul gascoigne [ ... table : an excerpt of a 3-stage generated example by progen-3 on the cnn news domain 4.3 human evaluation in our human study we asked three university students who are proﬁcient english speakers to evaluate the coherence and ﬂuency of the generated text to better assess the coherence of the long passages of text we evaluate at both the passage level and the ﬁner-grained sentence level more concretely for passage-level coherence human raters assign a coherence score to each full-length text sample on a 5-point likert scale for a more detailed assessment we further evaluate sentencelevel coherence where human raters label each sentence in the text passage with or , indicating whether the particular sentence is coherent with the proceeding context in the passage we then calculate the average percentage of coherent sentences in the generated text by each model human raters also evaluate the language quality for a ﬂuency score on a 5-point likert scale we compare our method with the systems that show highest generation quality in automatic evaluation including bart gpt2-small and summary we evaluated 50 examples for each comparison model on the cnn domain the pearson correlation coefﬁcient of human scores is 0.52 showing moderate inter-rater agreement table shows the results all systems receive close ﬂuency scores our approach obtained significantly higher coherence scores at both passage and sentence levels in particular over 86 sentences in our model generations are considered as coherent with the context improving over other models by at least 10 absolute percent 4.4 ablation study and analysis sample efﬁciency we study how the progressive generation could improve the sample efﬁciency of large lms ﬁne-tuned to target domains the intuition is that by focusing on the subsets of informative words the early stages can more efﬁciently capture the domain-speciﬁc characteristics and then steer the subsequent reﬁnement stages figure shows the results where we report the fbd score averaged over fbd-s/m/d we can see our approach can make more efﬁcient use of the training data in learning to generate high quality samples for example with only 1k training examples our method achieves comparable results with large lms trained on 30k examples generation with gold plans to investigate the importance of dividing the generation process into stages and what the stages learn separately we add another set of text into our comparison it is a 2stages model whose ﬁrst stage is the ground truth gold plan while the second stage kept the same a bart model , shown as goldplan in table 3. note that with gold plan our model greatly decreases the gap with human text in terms of lexical tid and semantic fbd-d quality metrics the results highlight the importance of plans in text 4320 generation the intermediate plans act as an information bottleneck and high-quality plans could lead to high-quality text generation effect of data noising we study the ablation of data noising to check whether the noising operation really helps reduce stage-wise exposure bias sec 3.2 as we expected table shows the comparison between models with and without noise in training the added noise generally brings performance improvement in terms of various metrics example generations table shows an example of text generated via three stages we can see our model ﬁrst generates the key subject beckham and the team name liverpool in the very ﬁrst stage then adds more ﬁne-grained details like acquisition transfer in the second stage and ﬁnally expands the keywords into a full document describing beckham s joining a new team 5 conclusion we have proposed a new approach for domainspeciﬁc generation of long text passages in a progressive manner our method is simple and efﬁcient by ﬁne-tuning large-scale off-the-shelf language models we conduct extensive experiments using a variety of metrics and human studies we demonstrate that our method outperforms a wide range of large pretrained lms with single-stage generation or prior planning-then-generation strategies in terms of quality and coherence of the produced samples the multi-stage generation also opens up new opportunities to enhance controllability of text generation which we would love to explore in the future references antoine bosselut asli celikyilmaz xiaodong he jianfeng gao po-sen huang and yejin choi 2018. discourse-aware neural rewards for coherent text generation in naacl pages 173–184 tom brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell et al 2020. language models are few-shot learners in neurips volume 33 pages 1877–1901 william chan nikita kitaev kelvin guu mitchell stern and jakob uszkoreit 2019. kermit generative insertion-based modeling for sequences arxiv preprint arxiv:1906.01604. krzysztof choromanski valerii likhosherstov david dohan xingyou song andreea gane tamas sarlos peter hawkins jared davis afroz mohiuddin lukasz kaiser et al 2021. rethinking attention with performers iclr zihang dai zhilin yang yiming yang jaime carbonell quoc le and ruslan salakhutdinov 2019. transformer-xl attentive language models beyond a ﬁxed-length context in acl pages 2978–2988 jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirectional transformers for language understanding in naacl pages 4171–4186 angela fan mike lewis and yann dauphin 2018. hiin acl pages erarchical neural story generation 889–898 angela fan mike lewis and yann dauphin 2019. strategies for structuring story generation in acl nicolas ford daniel duckworth mohammad norouzi and george dahl 2018. the importance of generation order in language modeling in emnlp gardian a robot wrote this entire article are you scared yet human jiatao gu qi liu and kyunghyun cho 2019. insertion-based decoding with automatically inferred generation order tacl 7:661–676 karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom 2015. teaching machines to read and comprehend in neurips pages 1693–1701 ari holtzman jan buys maxwell forbes and yejin choi 2020. the curious case of neural text degeneration in iclr xinyu hua and lu wang 2019. sentence-level content planning and style speciﬁcation for neural text generation in emnlp xinyu hua and lu wang 2020. pair planning and iterative reﬁnement in pre-trained transformers for long text generation in emnlp pages 781–793 jungo kasai james cross marjan ghazvininejad and jiatao gu 2020. non-autoregressive machine translation with disentangled context transformer in icml jason lee elman mansimov and kyunghyun cho deterministic non-autoregressive neural in 2018. sequence modeling by iterative reﬁnement emnlp pages 1173–1182 mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer 2020. bart denoising sequence-to-sequence pretraining for natural language generation translation and comprehension in acl pages 7871–7880 4321 peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer 2018. generating wikipedia by summarizing long sequences in iclr elman mansimov alex wang sean welleck and kyunghyun cho 2019. a generalized framework of sequence generation with application to undirected sequence models arxiv preprint arxiv:1905.12790. marketmuse gpt-3 exposed behind the smoke and mirrors ehsan montahaei danial alihosseini and mahdieh soleymani baghshah 2019. jointly measuring diversity and quality in text generation models naacl workshop amit moryossef yoav goldberg and ido dagan 2019. step-by-step separating planning from realization in neural data-to-text generation in naacl roman novak michael auli and david grangier iterative reﬁnement for machine translation 2016. arxiv preprint arxiv:1610.06602. ratish puduppully li dong and mirella lapata 2019. data-to-text generation with content selection and planning in aaai volume 33 pages 6908–6915 lianhui qin vered shwartz peter west chandra bhagavatula jena d. hwang ronan le bras antoine bosselut and yejin choi 2020. back to the future unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning in emnlp pages 794–805 association for computational linguistics alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever 2019. language models are unsupervised multitask learners openai blog 1 8 :9. marc aurelio ranzato sumit chopra michael auli and wojciech zaremba 2016. sequence level training with recurrent neural networks iclr hannah rashkin asli celikyilmaz yejin choi and jianfeng gao 2020. plotmachines outlineconditioned generation with dynamic plot state tracking in emnlp pages 4274–4295 ehud reiter and robert dale 1997. building applied natural language generation systems natural language engineering 3 1 :57–87 stuart rose dave engel nick cramer and wendy cowley 2010. automatic keyword extraction from individual documents text mining applications and theory 1:1–20 gerard salton and michael mcgill 1986. introduc tion to modern information retrieval abigail see aneesh pappu rohun saxena akhila yerukola and christopher d. manning 2019. do massively pretrained language models make better storytellers in conll pages 843–861 dinghan shen asli celikyilmaz yizhe zhang liqun chen xin wang jianfeng gao and lawrence carin 2019. towards generating long and coherent text with multi-level latent variable models in acl pages 2079–2089 tianxiao shen victor quach regina barzilay and tommi jaakkola 2020. blank language models in emnlp pages 5186–5198 zhan shi xinchi chen xipeng qiu and xuanjing huang 2018. toward diverse text generation with inverse reinforcement learning ijcai mitchell stern william chan jamie kiros and jakob insertion transformer flexible in uszkoreit 2019. sequence generation via insertion operations icml volume 97 pages 5976–5985 bowen tan zhiting hu zichao yang ruslan salakhutdinov and eric xing 2019. connecting the dots between mle and rl for sequence generation iclr workshop ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser and illia polosukhin 2017. attention is all you need in neurips pages 5998–6008 sinong wang belinda li madian khabsa han selfarxiv preprint fang and hao ma 2020. attention with linear complexity arxiv:2006.04768. linformer sean welleck kianté brantley hal daumé iii and kyunghyun cho 2019. non-monotonic sequential text generation in icml ziang xie sida wang jiwei li daniel lévy aiming nie dan jurafsky and andrew ng 2017. data noising as smoothing in neural network language models in iclr peng xu mostofa patwary mohammad shoeybi raul puri pascale fung anima anandkumar and bryan catanzaro 2020. megatron-cntrl controllable story generation with external knowledge using large-scale language models in emnlp pages 2831–2845 lili yao nanyun peng ralph weischedel kevin knight dongyan zhao and rui yan 2019. planand-write towards better automatic storytelling in aaai volume 33 pages 7378–7385 lili yao yaoyuan zhang yansong feng dongyan zhao and rui yan 2017. towards implicit contentintroducing for generative short-text conversation systems in emnlp pages 2190–2199 yizhe zhang guoyin wang chunyuan li zhe gan chris brockett and bill dolan 2020. pointer constrained progressive text generation via insertion-based generative pre-training in emnlp pages 8649–8670 4322 liang zhao jingjing xu junyang lin yichang zhang hongxia yang and xu sun 2020. graph-based multi-hop reasoning for long text generation arxiv preprint arxiv:2009.13282. wanrong zhu zhiting hu and eric xing 2019. text inﬁlling arxiv preprint arxiv:1901.00158 4323 appendix complete results we include complete result numbers of experiments here gpt2-s gpt2-l bart summ rake srl 70.25 72.84 b-bl2 46.54 48.53 b-bl3 27.25 28.64 b-bl4 14.88 15.87 b-bl5 73.61 67.47 ha-bl2 44.51 49.26 ha-bl3 25.84 29.21 ha-bl4 13.91 16.22 ha-bl5 49.24 43.85 msj-2 25.90 28.79 msj-3 14.15 15.73 msj-4 7.43 8.38 msj-5 8.7 16.1 tid 14.26 fbd-s 16.21 37.24 fbd-m 24.92 fbd-d 43.07 64.83 71.51 47.55 28.11 15.57 74.56 50.27 30.03 16.77 49.85 29.43 16.24 8.72 6.8 7.76 22.49 44.86 71.89 47.48 28.55 15.62 71.97 47.83 28.26 15.77 46.94 27.29 14.85 7.91 9.2 18.50 29.61 44.15 73.28 49.26 29.31 16.35 74.59 50.32 29.88 16.52 46.97 27.99 15.48 8.25 4.5 2.93 15.00 33.08 69.78 45.39 26.09 14.01 71.63 47.34 27.51 14.84 44.19 26.01 14.12 7.36 7.8 4.17 25.92 54.12 srl-n srl-c progen-2 progen-3 progen-4 dev 74.50 51.19 31.04 17.58 74.51 50.87 30.45 16.94 49.39 29.58 16.33 8.68 5.2 11.42 22.63 43.26 74.57 51.06 30.96 17.53 75.86 51.88 31.32 17.63 51.00 30.56 16.96 9.10 4.0 2.64 17.38 36.49 72.25 48.44 28.88 16.08 74.64 50.64 30.45 17.09 49.46 29.54 16.50 8.90 6.2 3.26 19.05 39.94 74.10 50.38 30.32 17.09 75.17 51.07 30.64 17.18 50.16 30.04 16.68 8.95 5.4 3.16 18.84 38.30 74.71 51.40 31.06 17.41 75.11 51.18 30.49 16.87 44.37 26.92 14.99 8.02 5.2 4.66 20.28 44.34 75.82 52.08 32.29 19.35 75.72 52.01 32.28 19.40 54.51 32.54 18.60 10.87 2.6 5.98 12.26 25.63 table : complete results on the cnn news domain gpt2-s gpt2-l bart summ rake srl 77.98 78.38 b-bl2 55.67 55.51 b-bl3 33.83 33.41 b-bl4 17.93 17.59 b-bl5 78.19 79.24 ha-bl2 57.00 55.39 ha-bl3 34.63 33.32 ha-bl4 18.26 17.46 ha-bl5 55.27 47.51 msj-2 30.88 34.48 msj-3 17.87 19.32 msj-4 9.54 10.16 msj-5 4.6 5.5 tid 6.03 fbd-s 3.49 21.85 fbd-m 19.30 fbd-d 40.18 43.63 76.97 53.86 31.95 16.47 77.19 54.71 32.70 16.86 51 .. 88 32.59 18.33 9.57 5.8 8.28 22.97 44.32 76.96 54.45 33.02 17.55 79.99 57.86 35.63 19.16 55.89 35.46 20.27 10.73 5.1 5.34 21.75 43.97 77.43 54.18 32.20 16.79 76.96 54.33 32.52 16.94 54.21 33.70 18.83 9.90 8.3 3.43 19.41 41.22 77.19 54.45 32.88 17.53 79.30 56.83 34.63 18.47 52.63 33.46 19.17 10.27 4.5 5.06 18.11 33.90 srl-n srl-c proget-2 proget-3 proget-4 dev 77.90 55.49 33.75 17.98 77.73 55.71 33.93 18.03 45.39 29.51 17.11 9.21 5.3 7.49 23.15 45.87 78.64 56.44 34.46 18.32 80.57 58.11 35.43 18.72 55.14 34.81 19.63 10.16 5.1 3.72 19.36 39.82 78.41 56.25 34.52 18.57 80.50 58.35 35.96 19.30 56.18 35.74 20.39 10.76 4.8 3.81 18.62 38.68 78.73 56.50 34.62 18.49 80.72 58.38 35.84 19.14 56.51 35.80 20.29 10.60 5.0 3.90 19.04 39.05 77.62 55.09 33.36 17.63 77.46 55.14 33.36 17.60 43.36 28.22 16.39 8.82 7.0 8.63 25.01 48.92 79.20 56.02 34.08 18.40 79.51 56.39 34.36 18.55 60.07 37.42 21.22 11.34 3.4 1.96 12.23 28.82 table : complete results on the story domain 4324 mlsum the multilingual summarization corpus thomas scialom cid:63 ‡ paul-alexis dray cid:63 , sylvain lamprier‡ benjamin piwowarski cid:5 ‡ jacopo staiano cid:63 ( ) cnrs france sorbonne université cnrs lip6 f-75005 paris france cid:63 recital paris france thomas paul-alexis jacopo @ recital.ai sylvain.lamprier benjamin.piwowarski @ lip6.fr abstract we present mlsum the ﬁrst large-scale multilingual summarization dataset obtained from online newspapers it contains 1.5m+ article/summary pairs in ﬁve different languages namely french german spanish russian turkish together with english news articles from the popular cnn/daily mail dataset the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community we report cross-lingual comparative analyses based on state-of-the-art systems these highlight existing biases which motivate the use of a multi-lingual dataset introduction the document summarization task requires several complex language abilities understanding a long document discriminating what is relevant and writing a short synthesis over the last few years advances in deep learning applied to nlp have contributed to the rising popularity of this task among the research community see et al. 2017 kry´sci´nski et al. 2018 scialom et al. 2019 . as with other nlp tasks the great majority of available datasets for summarization are in english and thus most research efforts focus on the english language the lack of multilingual data is partially countered by the application of transfer learning techniques enabled by the availability of pre-trained multilingual language models this approach has recently established itself as the de-facto paradigm in nlp guzmán et al. 2019 . under this paradigm for encoder/decoder tasks a language model can ﬁrst be pre-trained on a large corpus of texts in multiple languages then the model is ﬁne-tuned in one or more pivot languages for which the task-speciﬁc data are available at inference it can still be applied to the different languages seen during the pre-training because of the dominance of english for large scale corpora english naturally established itself as a pivot for other languages the availability of multilingual pre-trained models such as bert multilingual mbert , allows to build models for target languages different from training data however previous works reported a signiﬁcant performance gap between english and the target language e.g for classiﬁcation conneau et al. 2018 and question answering lewis et al. 2019 tasks a similar approach has been recently proposed for summarization chi et al. 2019 obtaining again a lower performance than for english for speciﬁc nlp tasks recent research efforts have produced evaluation datasets in several target languages allowing to evaluate the progress of the ﬁeld in zero-shot scenarios nonetheless those approaches are still bound to using training data in a pivot language for which a large amount of annotated data is available usually english this prevents investigating for instance whether a given model is as ﬁtted for a speciﬁc language as for any other answers to such research questions represent valuable information to improve model performance for low-resource languages in this work we aim to ﬁll this gap for the automatic summarization task by proposing a largescale multilingual summarization mlsum dataset the dataset is built from online news outlets and contains over 1.5m article-summary pairs in languages french german spanish russian and turkish which complement an already established summarization dataset in english the contributions of this paper can be summa rized as follows 1. we release the ﬁrst large-scale multilingual summarization dataset 2. we provide strong baselines from multilingual abstractive text generation models proceedings of the 2020 conference on empirical methods in natural language processing pages 8051–8067 november 16–20 2020. ( ) 2020 association for computational linguistics 8051 3. we report a comparative cross-lingual analysis of the results obtained by different approaches 2 related work 2.1 multilingual text summarization over the last two decades several research works have focused on multilingual text summarization radev et al ( 2002 developed mead a multidocument summarizer that works for both english and chinese litvak et al ( 2010 proposed to improve multilingual summarization using a genetic algorithm a community-driven initiative multiling giannakopoulos et al. 2015 , benchmarked summarization systems on multilingual data while the multiling benchmark covers 40 languages it provides relatively few examples 10k in the 2019 release . most proposed approaches so far have used an extractive approach given the lack of a multilingual corpus to train abstractive models duan et al. 2019 . more recently with the rapid progress in automatic translation and text generation abstractive methods for multilingual summarization have been developed ouyang et al ( 2019 proposed to learn summarization models for three low-resource languages somali swahili and tagalog , by using an automated translation of the new york times dataset although this showed only slight improvements over a baseline which considers translated outputs of an english summarizer results remain still far from human performance summarization models from translated data usually under-perform as translation biases add to the difﬁculty of summarization following the recent trend of using multi-lingual pre-trained models for nlp tasks such as multilingual bert m-bert ( pires et al. 2019 1 or xlm lample and conneau 2019 , chi et al ( 2019 proposed to ﬁne-tune the models for summarization on english training data the assumption is that the summarization skills learned from english data can transfer to other languages on which the model has been pre-trained however a signiﬁcant performance gap between english and the target language is observed following this process this emphasizes the crucial need of multilingual training data for summarization 2.2 existing multilingual datasets the research community has produced several multilingual datasets for tasks other than summarization we report two recent efforts below noting that both ) rely on human translations and ii only provide evaluation data the cross-lingual nli corpus the snli corpus bowman et al. 2015 is a large scale dataset for natural language inference nli . it is composed of a collection of 570k human-written english sentence pairs associated with their label entailment contradiction or neutral the multigenre natural language inference multinli corpus is an extension of snli comparable in size but including a more diverse range of text conneau et al ( 2018 introduced the cross-lingual nli corpus xnli to evaluate transfer learning from english to other languages based on multinli a collection of 5,000 test and 2,500 dev pairs were translated by humans in 15 languages mlqa given a paragraph and a question the question answering qa task consists in providing the correct answer large scale datasets such as rajpurkar et al. 2016 choi et al. 2018 trischler et al. 2016 have driven fast progress.2 however these datasets are only in english to assess how well models perform on other languages lewis et al ( 2019 recently proposed mlqa an evaluation dataset for cross-lingual extractive qa composed of 5k qa instances in languages xtreme the cross-lingual transfer evaluation of multilingual encoders benchmark hu et al. 2020 covers 40 languages over tasks the summarization task is not included in the benchmark xglue to train and evaluate their performance across a diverse set of cross-lingual tasks liang et al ( 2020 recently released xglue covering both natural language understanding and generation scenarios while no summarization task is included it comprises a news title generation task the data is crawled from a commercial news website and provided in form of article-title pairs for languages german english french spanish and russian . : //github.com/google-research/ bert/blob/master/multilingual.md 2for instance see the squad leaderboard rajpurkar github.io/squad-explorer/ 8052 2.3 existing summarization datasets we describe here the main available corpora for text summarization document understanding conference several small and high-quality summarization datasets in english harman and over 2004 dang 2006 have been produced in the context of the document understanding conference duc .3 they are built by associating newswire articles with corresponding human summaries a distinctive feature of the duc datasets is the availability of multiple reference summaries this is a valuable characteristic since as found by rankel et al ( 2013 , the correlation between qualitative and automatic metrics such as rouge lin 2004 , decreases signiﬁcantly when only a single reference is given however due to the small number of training data available duc datasets are often used in a domain adaptation setup for models ﬁrst trained on larger datasets such as gigaword cnn/dm nallapati et al. 2016 see et al. 2017 or with unsupervised methods dorr et al. 2003 mihalcea and tarau 2004 barrios et al. 2016a . gigaword again using newswire as source data the english gigaword napoles et al. 2012 rush et al. 2015 chopra et al. 2016 corpus is characterized by its large size and the high diversity in terms of sources since the samples are not associated with human summaries prior works on summarization have trained models to generate the headlines of an article given its incipit which induces various biases for learning models new york times corpus this large corpus for summarization consists of hundreds of thousand of articles from the new york times sandhaus 2008 , spanning over 20 years the articles are paired with summaries written by library scientists although grusky et al. 2018 found indications of bias towards extractive approaches several research efforts have used this dataset for summarization hong and nenkova 2014 durrett et al. 2016 paulus et al. 2017 . cnn/daily mail one of the most commonly used dataset for summarization nallapati et al. 2016 see et al. 2017 paulus et al. 2017 dong et al. 2019 , although originally built for question answering tasks hermann et al. 2015a . it consists of english articles from the cnn and the 3http //duc.nist.gov/ daily mail associated with bullet point highlights from the article when used for summarization the bullet points are typically concatenated into a single summary newsroom composed of 1.3m articles grusky et al. 2018 , and featuring high diversity in terms of publishers the summaries associated with english news articles were extracted from the web pages metadata they were originally written to be used in search engines and social media bigpatent sharma et al ( 2019 collected 1.3 million u.s. patent documents across several technological areas using the google patents public datasets the patents abstracts are used as target summaries lcsts the large scale chinese short text summarization dataset hu et al. 2015 is built from million short texts from the sina weibo microblogging platform they are paired with summaries given by the author of each text the dataset includes 10k summaries which were manually scored by human for their relevance 3 mlsum as described above the vast majority of summarization datasets are in english for arabic there exist the essex arabic summaries corpus easc ( el-haj et al. 2010 and kalimat el-haj and koulali 2013 ; those comprise circa 1k and 20k samples respectively pontes et al ( 2018 proposed a corpus of few hundred samples for spanish portuguese and french summaries to our knowledge the only large-scale non-english summarization dataset is the chinese lcsts hu et al. 2015 . with the increasing interest for cross-lingual models the nlp community have recently released multilingual evaluation datasets targeting classiﬁcation xnli and qa lewis et al. 2019 tasks as described in 2.2 though still no large-scale dataset is avaulable for document summarization to ﬁll this gap we introduce mlsum the ﬁrst large scale multilingual summarization corpus our corpus provides more than 1.5 millions articles in french fr , german de , spanish es , turkish tr , and russian ru . being similarly built from news articles and providing a similar amount of training samples per language except for russian , as the previously mentioned cnn/daily mail it can effectively serve as a multilingual extension of the cnn/daily mail dataset 8053 3.1 collecting the corpus the cnn/daily mail cnn/dm dataset see section 2.3 is arguably the most used large-scale dataset for summarization following the same methodology we consider news articles as the text input and their paired highlights/description as the summary for each language we selected an online newspaper which met the following requirements ( ) being a generalist newspaper ensuring that a broad range of topics is represented for each language allows to minimize the risk of training topic-speciﬁc models a fact which would hinder comparative cross-lingual analyses of the models ( ) having a large number of articles in their public online archive ( ) providing human written highlights/summaries for the articles that can be extracted from the html code of the web page after a careful preliminary exploration we selected the online version of the following newspapers le monde4 french , süddeutsche zeitung5 german , el pais6 spanish , moskovskij komsomolets7 russian , and internet haber8 turkish . for each outlet we gathered archived articles from 2010 to 2019. we applied one simple ﬁlter all the articles shorter than 50 words or summaries shorter than 10 words are discarded so as to avoid articles containing mostly audiovisual content each article was archived on the wayback machine,9 allowing interested research to fully re-build or extend mlsum we distribute the dataset as a list of immutable snapshot urls of the articles along with the accompanying corpusconstruction code,10 allowing to replicate the parsing and preprocessing procedures we employed this is due to legal reasons the content of the articles is copyrighted and redistribution might be seen as infringing of publishing rights.11 we provide recommended train/validation/test splits following a chronological ordering based on the articles publication dates in our experiments below we train/evaluate the models on the training/test splits obtained in this manner speciﬁcally 4www.lemonde.fr 5www.sueddeutsche.de 6www.elpais.com 7www.mk.ru 8www.internethaber.com 9web.archive.org we use data from 2010 to 2018 included for training data from 2019 ~10 of the dataset for validation up to may 2019 and test may-december 2019 . while this choice is arguably more , due to the possible emergence of new topics over time we consider it as the realistic scenario a successful summarization system should be able to deal with incidentally this also brings the advantage of excluding most cases of leakage across languages it prevents a model for instance from seeing a training sample describing an important event in one language and then being submitted for inference a similar article in another language published around the same time and dealing with the same event 3.2 dataset statistics we report statistics for each language in mlsum in table , including those computed on the cnn/daily mail dataset english for quick comparison mlsum provides a comparable amount of data for all languages with the exception of russian with ten times less training samples important characteristics for summarization datasets are the length of articles and summaries the vocabulary size and a proxy for abstractiveness namely the percentage of novel n-grams between the article and its human summary from table , we observe that russian summaries are the shortest as well as the most abstractive coupled with the signiﬁcantly lower amount of articles available from its online source the task can be seen as more for russian than for the other languages in mlsum 4 models we experimented on mlsum with the established models and baselines described below those include supervised and unsupervised methods extractive and abstractive models for all the experiments we train models on a per-language basis we used the recommended hyperparameters for all languages in order to facilitate assessing the robustness of the models we also tried to train one model with all the languages mixed together but we did not see any signiﬁcant difference of performance using : //github com/agude/wayback-machine-archiver : //github.com/recitalai/mlsum 11a similar approach has been adopted for several dataset releases in the recent past such as question answering corpus hermann et al. 2015b or xsum narayan et al. 2018a . 4.1 extractive summarization models oracle extracts the sentences within the input text that maximise a given metric in our experiments rouge-l given the reference summary it 8054 dataset size training set size mean article length mean summary length compression ratio novelty 1-gram total vocabulary size occurring 10+ times fr 424,763 392,876 632.39 29.5 21.4 15.21 1,245,987 233,253 de 242,982 220,887 570.6 30.36 18.8 14.96 1,721,322 240,202 es 290,645 266,367 800.50 20.71 38.7 15.34 1,257,920 229,033 ru 27,063 25,556 959.4 14.57 65.8 30.74 649,304 115,144 tr 273,617 249,277 309.18 22.88 13.5 28.90 1,419,228 248,714 en 311,971 287,096 790.24 55.56 14.2 9.45 875,572 184,095 table : statistics for the different languages en refers to cnn/daily mail and is reported for comparison purposes article and summary lengths are computed in words compression ratio is computed as the ratio between article and summary length novelty is the percentage of words in the summary that were not in the paired article total vocabulary size is the total number of different words and occurring 10+ times is the total number of words occurring at least 10 times is an indication of the maximum one could achieve with extractive summarization we rely on the implementation of narayan et al ( 2018b . random to compare the performances of the different models across languages it is useful to include an unbiased model as a point of reference to that purpose we deﬁne a simple random extractive model that randomly extracts words from the source document with ﬁxed as the average length of the summary lead-3 simply selects the three ﬁrst sentences from the input text sharma et al ( 2019 , among others showed that this is a robust baseline for several summarization datasets such as cnn/dm nyt and bigpatent textrank an unsupervised algorithm proposed by and tarau 2004 . it consists in computing the co-similarities between all the sentences in the input text then the most central to the document are extracted and considered as the summary we used the implementation provided by barrios et al ( 2016b . 4.2 abstractive summarization models pointer-generator see et al ( 2017 proposed the addition of the copy mechanism vinyals et al. 2015 on top of a sequence to sequence lstm model this mechanism allows to efﬁciently copy out-of-vocabulary tokens leveraging attention bahdanau et al. 2014 over the input we used the publicly available opennmt implementation12 with the default hyper-parameters 12opennmt.net/opennmt-py/summarization however to avoid biases we limited the preprocessing as much as possible and did not use any sentence separators as recommended for cnn/dm this explains why we obtain lower rouge compared to when using a language-speciﬁc preprocessor m-bert encoder-decoder transformer architectures are a very popular choice for text generation recent research efforts have adapted large pretrained self-attention based models for text generation peters et al. 2018 radford et al. 2018 devlin et al. 2019 . in particular liu and lapata 2019 added a randomly initialized decoder on top of bert avoiding the use of a decoder dong et al ( 2019 proposed to instead add a decoder-like mask during the pre-training to unify the language models for both encoding and generating both these approaches achieved sota results for summarization in this paper we only report results obtained following dong et al ( 2019 , as in preliminary experiments we observed that a simple multilingual bert m-bert , with no modiﬁcation obtained comparable performance on the summarization task 5 evaluation metrics rouge arguably the most used set of metrics for summarization it is deﬁned as the percentage of common n-grams between the evaluated summary and the human reference summary lin 2004 . html 8055 random textrank lead_3 pointer generator m-bert oracle fr 11.9 7.5 12.6 10.8 19.7 12.6 23.6 14.1 25.1 15.1 37.7 24.7 de 10.2 6.7 13.3 13 33.1 23.9 35.1 24.4 42 26.5 52.3 31.7 es 12.6 6.5 9.5 11.1 13.7 10.3 17.7 13.2 20.4 14.9 35.8 26.5 ru 6.7 2.5 3.3 3.8 5.9 5.8 5.7 5.7 9.5 6.8 29.8 20.3 tr 11.3 6.3 21.5 14.4 28.9 20.2 32.6 19.8 32.9 26.3 45.8 26.4 en 11.2 10.6 28.6 20.4 35.2 21.2 33.3 20.8 35.4 22.2 53.6 30 table : rouge-l meteor scores obtained by the models described in section 4.1. en refers to cnn/dm meteor the metric for evaluation of translation with explicit ordering banerjee and lavie 2005 was designed for the evaluation of machine translation output it is based on the harmonic mean of unigram precision and recall with recall weighted higher than precision meteor is often reported in summarization papers see et al. 2017 dong et al. 2019 in addition to rouge novelty because of their use of copy mechanisms some abstractive models have been reported to rely too much on extraction see et al. 2017 kry´sci´nski et al. 2018 . hence it became a common practice to report the percentage of novel ngrams produced within the generated summaries note that several approaches based on neural models have been recently proposed recent works eyal et al. 2019 scialom et al. 2019 have proposed to evaluate summaries with qa based methods the rationale is that a good summary should answer the most relevant questions about the article further kry´sci´nski et al ( 2019 proposed a discriminator trained to measure the factualness of the summary böhm et al ( 2019 learned a metric from human annotation all these models were only trained on english datasets preventing us to report them in this paper the availability of mlsum will enable future works to build such metrics in a multilingual fashion 6 results and discussion the results we now present allow to compare the models across languages and to investigate or hypothesize where their performance variations may come from to explain those we note the following factors ( ) differences in the data independently from the language such as the structure of the article the abstractiveness of the summaries or the quantity and 2 differences due to the language itself either due to metric biases e.g sensitivity to morphology or to biases inherent to the model while the former have more to do with domain adaptation the latter motivate further the development of multilingual datasets since they are instrumental for studying such phenomena turning to the observed results we report in table the rouge-l and meteor scores obtained by each model for all languages we note that the overall order of systems for each language is preserved when using either metric modulo some swaps between lead_3 and pointer generator but with relatively close scores . russian the low-resource language in mlsum for all experimental setups the performance on russian is comparatively low this can be explained by at least two factors first the corpus is the most abstractive see table , limiting the performance obtained by extractive models random lead-3 and oracle . second one order of magnitude less training data is available for russian than for the other mlsum languages a fact which can explain the impressive performance improvement +66 in terms of rouge-l see table ) between a not pretrained model pointer generator and a pretrained model m-bert . how abstractive are the models we report novelty in figure 1. as previous works noted see et al. 2017 , pointer-generator networks are poorly abstractive as they rely heavily on the copy mechanism this is particularly true for russian the lack of data probably makes it easier to learn to copy than to cope with natural language generation as expected pretrained language models such as m-bert are consistently and largely more abstractive since they are exposed to other texts during pretraining 8056 figure : percentage of novel n-grams for different abstractive models neural and human , for the datasets 6.1 model biases toward languages consistency among rouge scores the random model obtains comparable rouge-l scores across all languages except russian this can again be due to the russian corpus characteristics highest novelty shortest summaries and longest input documents see table ) as this limits our ability to draw meaningful interpretations over russian compared to other languages for the pair-wise language-based comparisons below we only focus on french german spanish and turkish abstractiveness of the datasets the oracle performance can be considered as the upper limit for an extractive model since it extracts the sentences that provide the best rouge-l. we can observe that while being similar for english and german and to some extent turkish the oracle performance is lower for french or spanish however as described in figure , the percentage of novel words are similar for german 14.96 , french 15.21 and spanish 15.34 . this may indicate that the relevant information to extract from the article is more spread among sentences for spanish and french than for german this is conﬁrmed with the results of lead-3 german and english have a much higher rouge-l 35.20 and 33.09 than french or spanish 19.69 and 13.70. the case of textrank the textrank performance varies widely across the different languages regardless oracle it is particularly surprising to see the low performance on german whereas for this language lead-3 has a comparatively higher fr de es ru tr cnn/dm en cnn/dm en full preproc duc en newsroom en t/p 0.53 0.37 0.53 0.57 0.65 1.10 0.85 1.21 1.10 b/p 1.06 1.20 1.15 1.65 1.01 1.06 table : rouge-l ratios t/p is the ratio of textrank to pointer-generator and b/p is the ratio of m-bert to pointer-generator the results for cnn/dm-full preprocessing duc and newsroom datasets are those reported in table of grusky et al ( 2018 ( pointer-c in their paper is our pointer-generator . performance on the other hand the performance on english is remarkably high the rouge-l is 33 higher than for turkish 126 higher than for french and 200 higher than for spanish we suspect that the textrank parameters might actually overﬁt english this is conﬁrmed by the results reported in table , where we report the performance ratio between textrank and pointer generator on our corpus as well as on cnn/dm and two other english corpora duc and newsroom . textrank has a performance close to the pointer generator on english corpora ratio between 0.85 to 1.21 but not in other languages ratio between 0.37 to 0.65 . 8057 fr pointer-generator bert-m human e o e a a t 80 60 40 20 1-grams 2-grams 3-grams 4-grams de pointer-generator bert-m human e o e a a t 80 60 40 20 1-grams 2-grams 3-grams 4-grams es pointer-generator bert-m human e o e a a t 80 60 40 20 1-grams 2-grams 3-grams 4-grams ru pointer-generator bert-m human 80 60 40 e o e a a t 20 1-grams 2-grams 3-grams 4-grams tr pointer-generator bert-m human e o e a a t 80 60 40 20 1-grams 2-grams 3-grams 4-grams en pointer-generator bert-m human e o e a a t 80 60 40 20 1-grams 2-grams 3-grams 4-grams morphology of the two languages in statistical parsing tsarfaty et al ( 2010 considered german to be very sensitive to word order due to its rich morphology as opposed to french among other reasons the ﬂexibility of its syntactic ordering is mentioned this corroborates the hypothesis that self-attention might help preserving information for languages with higher degrees of word order freedom 6.2 possible derivative usages of mlsum multilingual question answering originally cnn/dm was a question answering dataset hermann et al. 2015a , under the assumption that the information in the summary is also contained in the pair article questions can thus be generated from the summary sentences by masking the named entities contained therein the masked entities represent the answers and thus a masked question should be answerable given the source article as no multilingual qa training dataset has been proposed so far this methodology could be applied to mlsum to obtain a large-scale multilingual question answering corpus incidentally this would also allow progressing towards multilingual question generation a crucial component of the neural summarization metrics mentioned in section 5. news title generation while the release of mlsum hereby described covers only articlesummary pairs the archived articles also include the titles the accompanying code for parsing the articles allows to easily retrieve the titles and thus use them for news title generation topic detection with the exception of turkish a topic/category can be associated with each article/summary pair by simply parsing the corresponding url a natural application of this data for summarization would be for template based summarization perez-beltrachini et al. 2019 , using it as additional features however it can also be a useful multilingual resource for topic detection 7 conclusion we presented mlsum the ﬁrst large-scale multilingual summarization dataset comprising over 1.5m article/summary pairs in french german russian spanish and turkish we detailed its construction and its complementary nature to the cnn/dm english summarization dataset figure : improvement rates from textrank to oracle in abscissa against rates from pointer generator to mbert in ordinate . the beneﬁts of pretraining we hypothesize that the closer an unsupervised model performance to its maximum limit the less improvement would come from pretraining in figure , we plot the improvement rate from textrank to oracle against that of pointergenerator to m-bert looking at the correlation emerging from the plot the hypothesis appears to hold true for all languages including russian not plotted for scaling reasons x 808 y 40 , with the exception of english this is probably due to the aforementioned bias of textrank towards the english language pointer generator and m-bert we observe that m-bert always outperforms the pointer generator however the ratio is not homogeneous across the different languages see table 3. in particular the improvement for german is much more important than the one for french interestingly this observation is in line with the results reported for machine translation the transformer vaswani et al. 2017 outperforms significantly convs2s gehring et al. 2017 for english to german but obtains comparable results for english to french see table in vaswani et al ( 2017 . neither model is pretrained nor based on lstm hochreiter and schmidhuber 1997 , and they both use bpe tokenization shibata et al. 1999 . therefore the main difference is represented by the self-attention mechanism introduced in the transformer while convs2s used only source-to-target attention we thus hypothesise that self-attention plays an important role for german but has a limited impact for french this could ﬁnd an explanation in the 8058 m r b = o a e e r t o ( % 16 14 12 10 6 2 de es en fr tr 100 150 200 250 300 ( textrank > oracle we reported extensive preliminary experiments highlighting biases observed in existing models and analyzed the relative performances across languages of state-of-the-art models in future works we plan to add other languages including arabic and hindi and to investigate the adaptation of neural metrics to multilingual summarization acknowledgements we thank the emnlp reviewers and rachel keraron for the useful feedback alexander gude for developing the wayback-machine-archiver,13 references dzmitry bahdanau kyunghyun cho and yoshua bengio 2014. neural machine translation by jointly arxiv preprint learning to align and translate arxiv:1409.0473. satanjeev banerjee and alon lavie 2005. meteor an automatic metric for mt evaluation with improved correlation with human judgments in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization pages 65–72 federico barrios federico lópez luis argerich and rosa wachenchauzer 2016a variations of the similarity function of textrank for automated summarization arxiv preprint arxiv:1602.03606. federico barrios federico lópez luis argerich and rosa wachenchauzer 2016b variations of the similarity function of textrank for automated summarization corr abs/1602.03606 florian böhm yang gao christian m. meyer ori shapira ido dagan and iryna gurevych 2019. better rewards yield better summaries learning to sumin proceedings of the marise without references 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 3101–3111 hong kong china association for computational linguistics samuel bowman gabor angeli christopher potts and christopher manning 2015. a large annotated corpus for learning natural language inference arxiv preprint arxiv:1508.05326. zewen chi li dong furu wei wenhui wang xianling mao and heyan huang 2019. cross-lingual natural language generation via pre-training arxiv preprint arxiv:1909.10481 13https //github.com/agude/ wayback-machine-archiver eunsol choi he he mohit iyyer mark yatskar wentau yih yejin choi percy liang and luke zettlemoyer 2018. quac question answering in context arxiv preprint arxiv:1808.07036. sumit chopra michael auli and alexander rush 2016. abstractive sentence summarization with attentive recurrent neural networks in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies pages 93–98 alexis conneau ruty rinott guillaume lample adina williams samuel r. bowman holger schwenk and veselin stoyanov 2018. xnli evaluating crosslingual sentence representations in proceedings of the 2018 conference on empirical methods in natural language processing association for computational linguistics hoa trang dang 2006. duc 2005 evaluation of in proquestion-focused summarization systems ceedings of the workshop on task-focused summarization and question answering pages 48–55 association for computational linguistics jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirectional transformers for language understanding in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 4171–4186 li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao-wuen hon 2019. uniﬁed language model pre-training for natural language arxiv preprint understanding and generation arxiv:1905.03197. bonnie dorr david zajic and richard schwartz 2003. hedge trimmer a parse-and-trim approach to headline generation in proceedings of the hltnaacl 03 on text summarization workshop-volume , pages 1–8 association for computational linguistics xiangyu duan mingming yin min zhang boxing chen and weihua luo 2019. zero-shot crosslingual abstractive sentence summarization through teaching generation and attention in proceedings of the 57th annual meeting of the association for computational linguistics pages 3162–3172 greg durrett taylor berg-kirkpatrick and dan klein 2016. learning-based single-document summarization with compression and anaphoricity constraints in proceedings of the 54th annual meeting of the association for computational linguistics volume : long papers , pages 1998–2008 m el-haj u kruschwitz and fox 2010. using mechanical turk to create a corpus of arabic summaries 8059 mahmoud el-haj and rim koulali 2013. kalimat a multipurpose arabic corpus in second workshop on arabic corpus linguistics wacl-2 , pages 22–25 matan eyal tal baumel and michael elhadad 2019. question answering as an automatic evaluation metin proceedric for news article summarization ings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 3938–3948 jonas gehring michael auli david grangier denis yarats and yann dauphin 2017. convolutional in proceedings sequence to sequence learning of the 34th international conference on machine learning-volume 70 pages 1243–1252 jmlr org george giannakopoulos jeff kubina john conroy josef steinberger benoit favre mijail kabadjov udo kruschwitz and massimo poesio 2015. multiling 2015 multilingual summarization of single and multi-documents on-line fora and call-center conversations in proceedings of the 16th annual meeting of the special interest group on discourse and dialogue pages 270–274 max grusky mor naaman and yoav artzi 2018. newsroom a dataset of 1.3 million summaries with diverse extractive strategies in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long papers , pages 708–719 francisco guzmán peng-jen chen myle ott juan pino guillaume lample philipp koehn vishrav chaudhary and marc aurelio ranzato 2019. the ﬂores evaluation datasets for low-resource machine translation nepali–english and . in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 6100– 6113. donna harman and paul over 2004. the effects of human variation in duc summarization evaluation in text summarization branches out pages 10–17 karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom 2015a teaching machines to read and comprehend in advances in neural information processing systems pages 1693–1701 karl moritz hermann tomáš koˇciský edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom 2015b teaching machines to read and comprehend in advances in neural information processing systems nips . sepp hochreiter and jürgen schmidhuber 1997. neural computation long short-term memory 9 8 :1735–1780 kai hong and ani nenkova 2014. improving the estimation of word importance for news multiin proceedings of the document summarization 14th conference of the european chapter of the association for computational linguistics pages 712– 721. baotian hu qingcai chen and fangze zhu 2015. lcsts a large scale chinese short text summarization dataset in proceedings of the 2015 conference on empirical methods in natural language processing pages 1967–1972 junjie hu sebastian ruder aditya siddhant graham neubig orhan firat and melvin johnson 2020. xtreme a massively multilingual multi-task benchmark for evaluating cross-lingual generalization arxiv preprint arxiv:2003.11080. wojciech kry´sci´nski bryan mccann caiming xiong and richard socher 2019. evaluating the factual consistency of abstractive text summarization arxiv preprint arxiv:1910.12840. wojciech kry´sci´nski romain paulus caiming xiong and richard socher 2018. improving abstraction in text summarization in proceedings of the 2018 conference on empirical methods in natural language processing pages 1808–1817 guillaume lample and alexis conneau 2019. crosslingual language model pretraining arxiv preprint arxiv:1901.07291. patrick lewis barlas o˘guz ruty rinott sebastian riedel and holger schwenk 2019. mlqa evaluating cross-lingual extractive question answering arxiv preprint arxiv:1910.07475. yaobo liang nan duan yeyun gong ning wu fenfei guo weizhen qi ming gong linjun shou daxin jiang guihong cao et al 2020. xglue a new benchmark dataset for cross-lingual prearxiv training understanding and generation preprint arxiv:2004.01401. chin-yew lin 2004. rouge a package for automatic in text summarization evaluation of summaries branches out pages 74–81 marina litvak mark last and menahem friedman 2010. a new approach to improving multilingual in prosummarization using a genetic algorithm ceedings of the 48th annual meeting of the association for computational linguistics pages 927–936 association for computational linguistics yang liu and mirella lapata 2019. text summarizain proceedings of tion with pretrained encoders the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 3721–3731 8060 rada and paul tarau 2004. textrank bringing order into text in proceedings of the 2004 conference on empirical methods in natural language processing pages 404–411 ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang 2016. abstractive text summarization using sequence-to-sequence in proceedings of the 20th rnns and beyond signll conference on computational natural language learning pages 280–290 courtney napoles matthew gormley and benjamin in provan durme 2012. annotated gigaword ceedings of the joint workshop on automatic knowledge base construction and web-scale knowledge extraction pages 95–100 association for computational linguistics shashi narayan shay b. cohen and mirella lapata 2018a don t give me the details just the summary topic-aware convolutional neural networks for exin proceedings of the 2018 treme summarization conference on empirical methods in natural language processing brussels belgium shashi narayan shay b. cohen and mirella lapata 2018b ranking sentences for extractive summarization with reinforcement learning in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long papers , pages 1747–1759 new orleans louisiana association for computational linguistics jessica ouyang boya song and kathleen mckeown 2019. a robust abstractive system for cross-lingual summarization in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2025–2031 romain paulus caiming xiong and richard socher 2017. a deep reinforced model for abstractive summarization arxiv preprint arxiv:1705.04304. laura perez-beltrachini yang liu and mirella lapata 2019. generating summaries with topic templates and structured convolutional decoders arxiv preprint arxiv:1906.04687. matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer 2018. deep contextualized word representations in proceedings of naacl-hlt pages 2227–2237 telmo pires eva schlinger and dan garrette 2019. arxiv how multilingual is multilingual bert preprint arxiv:1906.01502. elvys linhares pontes juan-manuel torres-moreno stéphane huet and andréa carneiro linhares 2018. a new annotated portuguese/spanish corpus for the multi-sentence compression task in proceedings of the eleventh international conference on language resources and evaluation lrec 2018 . dragomir radev simone teufel horacio saggion wai lam john blitzer arda celebi hong qi elliott drabek and danyu liu 2002. evaluation of text summarization in a cross-lingual information retrieval framework center for language and speech processing johns hopkins university baltimore md tech rep. alec radford karthik narasimhan tim salimans and ilya sutskever 2018. improving language understanding by generative pre-training url : //s3-us-west-2 com/openaiassets/researchcovers/languageunsupervised/language understanding paper pdf amazonaws pranav rajpurkar jian zhang konstantin lopyrev and percy liang 2016. squad 100,000+ questions for machine comprehension of text arxiv preprint arxiv:1606.05250. peter a. rankel john m. conroy hoa trang dang and ani nenkova 2013. a decade of automatic content evaluation of news summaries reassessing the state of the art in proceedings of the 51st annual meeting of the association for computational linguistics volume : short papers , pages 131–136 soﬁa bulgaria association for computational linguistics alexander rush sumit chopra and jason weston 2015. a neural attention model for abstractive senin proceedings of the 2015 tence summarization conference on empirical methods in natural language processing pages 379–389 evan sandhaus 2008. the new york times annotated corpus linguistic data consortium philadelphia 6 12 : e26752 thomas scialom sylvain lamprier benjamin piwowarski and jacopo staiano 2019. answers unite unsupervised metrics for reinforced summain proceedings of the 2019 conrization models ference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlpijcnlp , pages 3237–3247 hong kong china association for computational linguistics abigail see peter liu and christopher manning 2017. get to the point summarization with pointergenerator networks in proceedings of the 55th annual meeting of the association for computational linguistics volume : long papers , pages 1073– 1083. eva sharma chen li and lu wang 2019. bigpatent a large-scale dataset for abstractive and coherent summarization arxiv preprint arxiv:1906.03741. yusuxke shibata takuya kida shuichi fukamachi masayuki takeda ayumi shinohara takeshi shinohara and setsuo arikawa 1999. byte pair encoding 8061 a text compression scheme that accelerates pattern matching technical report technical report doitr-161 department of informatics kyushu university adam trischler tong wang xingdi yuan justin harris alessandro sordoni philip bachman and kaheer suleman 2016. newsqa a machine comprehension dataset arxiv preprint arxiv:1611.09830. reut tsarfaty djamé seddah yoav goldberg sandra kübler marie candito jennifer foster yannick versley ines rehbein and lamia tounsi 2010. statistical parsing of morphologically rich languages spmrl : what how and whither in proceedings of the naacl hlt 2010 first workshop on statistical parsing of morphologically-rich languages pages 1–12 association for computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser and illia polosukhin 2017. attention is all in advances in neural information proyou need cessing systems pages 5998–6008 oriol vinyals meire fortunato and navdeep jaitly 2015. pointer networks in advances in neural information processing systems pages 2692–2700 8062 a additional metrics random textrank lead_3 point gen. m-bert oracle fr 17.95 22.13 28.74 31.08 31.59 47.32 de 13.08 20.4 38.57 39.8 44.78 57.23 es 17.34 16.92 21.87 24.63 25.58 45.23 ru 7.68 6.27 9.29 9.19 10.94 36.14 tr 15.92 26.86 34.79 36.9 36.63 50.61 table : rouge-1 scores en refers to cnn/dm random textrank lead_3 point gen. m-bert oracle fr 0.28 6.65 9.84 10.12 10.61 25.95 de 0.12 6.94 25.66 25.96 30.75 39.72 es 0.27 4.85 6.25 6.54 8.61 26.21 ru 0.07 1.3 1.54 1.18 1.75 19.88 tr 0.29 12.59 20.0 21.77 20.15 33.55 table : rouge-2 scores en refers to cnn/dm en 17.55 36.8 41.76 38.19 40.59 59.62 en 0.27 13.61 16.65 14.38 16.39 32.87 topic shift with the exception of turkish the article urls in mlsum allow to identify a category for a given article in figure we show the shift over categories among time in particular we plot the most frequent categories per language figure : distribution of topics for german top-left , spanish top-right , french bottom-left and russian bottom-right , grouped per year the shaded area for 2019 highlights validation and test data 8063 de politik sport wirtschaft panorama digital geld 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 100 80 60 40 20 es actualidad opinion catalunya madrid valencia television 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 100 80 60 40 20 fr social culture politique economie sport europe 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 100 80 60 40 20 ru social culture politics economics sport moscow 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 100 80 60 40 20 c samples french summary terre ’ origine du clan karzaï la ville méridionale de kandahar est aussi un bastion historique des talibans où le mollah omar a vécu et conservé de profondes racines c est sur cette terre pachtoune plus qu à kaboul que ’ avenir long terme du pays pourrait se . body lorsque ’ on parle de ’ afghanistan les yeux du monde sont rivés sur sa capitale kaboul c est là que se concentrent les lieux de pouvoir et où se détermine en principe son avenir c est aussi là que sont réunis les commandements des forces civiles et militaires internationales envoyées sur le sol afghan pour lutter contre ’ insurrection et aider le pays se reconstruire mais à regarder de plus près kaboul ’ est qu une façade face un etat inexistant une structure du pouvoir afghan encore clanique des tribus restées puissantes face une démocratie artiﬁcielle importée de ’ extérieur la vraie légitimité ne vient pas de kaboul la géographie du pouvoir afghan aujourd hui oblige dire qu une bonne partie des clés du destin de la population afghane se trouve au sud en terre pachtoune dans une cité hostile aux étrangers foyer historique des talibans kandahar kandahar est la terre ’ origine du clan karzaï et de sa tribu les popalzaï hamid karzaï président afghan tient son pouvoir du poids de son clan dans la région mi-novembre 2009 dans la grande maison de son frère wali à kandahar se pressaient des chefs de tribu venus de tout ’ afghanistan les piliers de son réseau l objet de la rencontre faire le bilan post-électoral après la réélection contestée de son frère la tête du pays parfois décrié pour ses liens supposés avec la cia et des traﬁquants de drogue wali karzaï joue un rôle politique méconnu il a organisé la campagne de son frère et ce jour-là à kandahar se jouait sous sa houlette l avenir de ceux qui avaient soutenu ou au contraire refusé leur soutien hamid chef ’ orchestre chargé du clan du président wali est la personnalité forte du sud du pays les karzaï adossent leur inﬂuence celle de kandahar dans ’ histoire de ’ afghanistan lorsque ahmad shah le fondateur du pays en 1747 conquit la ville il en ﬁt sa capitale `` jusqu en 1979 lors de ’ invasion soviétique kandahar a incarné le mythe de la création de ’ etat afghan les kandaharis considèrent qu ils ont un droit divin diriger le pays '' résume mariam abou zahab experte du monde pachtoune `` kandahar c est ’ afghanistan explique ceux qui ’ interrogent tooryalaï wesa gouverneur de la province la politique ’ fait et encore aujourd hui la politique sera dictée par les événements qui ’ dérouleront '' cette emprise de kandahar ’ évalue aux places prises au sein du gouvernement par `` ceux du sud '' la composition du nouveau gouvernement le 19 décembre n a pas changé la donne d autant moins que les rivaux des karzaï dans le sud ou ailleurs n ont pas réussi se renforcer au cours du dernier mandat du président l autre terre pachtoune le grand paktia dans le sud-est du pays à la frontière avec le pakistan qui a fourni tant de rois ne dispose plus de ses relais dans la capitale kandahar pèse aussi sur ’ avenir du pays car ’ trouve le coeur de ’ insurrection qui menace le pouvoir en place l otan déﬁée depuis huit ans n a cessé de perdre du terrain dans le sud où les insurgés contrôlent des zones entières les provinces du helmand et de kandahar sont les zones les plus meurtrières pour la coalition et ’ otan semble dépourvue de stratégie cohérente kandahar est la terre natale des talibans ils sont nés dans les campagnes du helmand et de kandahar et le mouvement taliban ’ est constitué dans la ville de kandahar où vivait leur chef spirituel le mollah omar et où il a conservé de profondes racines la pression sur la vie quotidienne des afghans est croissante les talibans suppléent même le gouvernement dans des domaines tels que la justice quotidienne ceux qui collaborent avec les étrangers sont stigmatisés menacés voire tués en guise de premier avertissement les talibans collent la nuit des lettres sur les portes des `` collabos '' `` la progression talibane est un fait dans le sud relate alex strick van linschoten unique spécialiste de la région et du mouvement taliban vivre kandahar sans protection l insécurité l absence de travail poussent vers kaboul ceux qui ont un peu ’ éducation et de compétence seuls restent les pauvres et ceux qui veulent faire de ’ argent '' en réaction cette détérioration les américains ont , sans ’ assumer ouvertement de reprendre le contrôle de situations conﬁées ofﬁciellement par ’ otan aux britanniques dans le helmand et aux canadiens dans la province de kandahar le mouvement a été progressif mais depuis un an les etats-unis ’ ont cessé ’ envoyer des renforts américains au point ’ exercer aujourd hui de fait la direction des opérations dans cette région une tendance qui se renforcera encore avec ’ arrivée des troupes supplémentaires promises par barack obama l histoire a montré que pour gagner en afghanistan il fallait tenir les campagnes de kandahar les britanniques ’ ont expérimenté de façon cuisante lors de la seconde guerre anglo-afghane la ﬁn du xixe siècle et les soviétiques ’ en sont jamais venus bout `` on sait comment cela ’ est terminé pour eux on va essayer ’ éviter de faire les mêmes erreurs '' observait mi-novembre optimiste un ofﬁcier supérieur américain 8064 german summary die wurzeln des elends liegen in der vergangenheit haiti bezahlt immer noch für seine befreiung vor 200 jahren auch damals nahmen die wichtigen der welt den insel-staat nicht ernst body das portrait von 1791 zeigt haitis nationalhelden françois-dominique toussaint ’ ouverture er war einer der anführer der revolution in haiti und autor der ersten verfassung die wurzeln des elends liegen in der vergangenheit haiti bezahlt immer noch für seine befreiung vor 200 jahren auch damals nahmen die wichtigen der welt den insel-staat nicht ernst am vergangenen wochenende schickte der britische architekt und gründer der organisation architecture for humanity eine atemlose verzweifelte e-mail an seine freunde und unterstützer `` nicht erdbeben sondern gebäude töten menschen '' schrieb er in die betreffzeile damit brachte er auf den punkt was auch der geologe und autor simon winchester oder der urbanist mike davis immer wieder geschrieben haben es gibt keine naturkatastrophen es gibt nur gewaltige naturereignisse die tödliche folgen haben die konsequenz aus dieser schlussfolgerung ist die schuldfrage einfach lässt sie sich beantworten gier und korruption sind fast immer die auslöser einer katastrophe in haiti aber liegen die wurzeln der tragödie tief in der geschichte des landes diese begann nach europäischer rechnung im jahre 1492 als christopher kolumbus auf der insel landete die ihre ureinwohner aytí nannten kolumbus benannte die insel in hispaniola um und gründete mit den trümmern der gestrandeten santa maria die erste spanische kolonie in der neuen welt ende des 17. jahrhunderts besetzten französische siedler den westen der insel den frankreich 1691 zur französischen kolonie sainte domingue erklärte ideale der französischen revolution gut hundert jahre währte die herrschaft der beiden kolonialherren über die geteilte insel `` saint domingue war die reichste europäische kolonie in den amerikas '' schrieb der historiker hans schmidt 1789 kam fast die hälfte des weltweit produzierten zuckers aus der französischen kolonie die auch in der produktion von kaffee baumwolle und indigo weltmarktführer war 450000 sklaven arbeiteten auf den plantagen und sie erfuhren bald vom neuen geist ihrer herren die französische revolution brachte die ideale von freiheit gleichheit und brüderlichkeit in die karibik im august 1791 war es so weit der voodoo-priester dutty boukman rief während einer messe zum aufstand einer der erfolgreichsten kommandeure der rebellion war der ehemalige sklave françois-dominique toussaint ’ ouverture nach dem heute der flughafen von port-au-prince benannt ist 1801 gab toussaint dem land seine erste verfassung die gleichzeitig eine unabhängigkeitserklärung war für napoleon sollte haiti eine schmach bleiben daraufhin sandte napoleon bonaparte kriegsschiffe und soldaten toussaint wurde verhaftet und nach frankreich gebracht wo er im kerker starb doch als napoleon im jahr darauf die sklaverei wieder einführen wollte kam es erneut zum aufstand verzweifelt baten die französischen truppen im sommer 1803 um verstärkung da aber hatte napoleon schon das interesse an der neuen welt verloren im april hatte er seine kolonie louisiana an die nordamerikaner verkauft ein gebiet das rund ein viertel des staatsgebietes der heutigen usa umfasste für napoleon sollte haiti eine schmach bleiben am 1. januar 1804 erklärte der rebellenführer jean-jacques dessalines die ehemalige kolonie heiße nun haiti und sei eine freie republik der erste und bis zur abschaffung der sklaverei einzige erfolgreiche sklavenaufstand der neuen welt war ein schock für die großmächte der kolonialära die ihren reichtum auf der sklaverei gegründet hatten ein handel der die geschichte haitis bis heute bestimmt die freiheit hatte ihren preis ein großteil der plantagen war zerstört ein drittel der bevölkerung haitis den kämpfen zum opfer gefallen vor allem aber wollte keine kolonialmacht die junge republik anerkennen im gegenteil -die meisten länder unterstützten das embargo der insel und die forderungen französischer sklavenherren nach reparationszahlungen in der hoffnung als freie nation zugang zu den weltmärkten zu , ließ sich die neue machtelite haitis auf einen handel ein der die geschichte der insel bis heute bestimmt mehr als zwei jahrzehnte nach dem sieg der rebellen entsandte könig karl x. seine kriegsschiffe nach haiti ein emissär stellte die regierung vor die wahl haiti sollte für die anerkennung als staat 150 millionen francs bezahlen sonst würde man einmarschieren und die bevölkerung erneut versklaven haiti nahm schulden auf und bezahlte bis zum jahre 1947 lähmte die schuldenlast die haitianische wirtschaft und legte den grundstein für armut und korruption 2004 ließ der damalige haitianische präsident jean-bertrand aristide errechnen was diese `` reparationszahlungen '' für haiti bedeuteten rund 22 milliarden amerikanische dollar rückzahlung forderten seine anwälte damals von der französischen regierung vergebens lesen sie auf der nächsten seite wie haiti von den akteuren der weltbühne geschnitten wurde 8065 spanish summary el aeropuerto ha estado hasta las 15.00 con sólo dos pistas por ausencia de de los 18 controladores aéreos.varias aerolíneas han denunciado demoras de `` hasta 60 minutos con los pasajeros embarcados '' body el espacio hará un repaso cronológico de la vida de la esteban desde el momento en el que una completa comenzó a aparecer en los medios en 1998 como la novia de jesulín de ubrique hasta llegar a hoy en día convertida en la princesa del pueblo en concreto del popular madrileño distrito de san blas donde vive tal como algunos la han caliﬁcado y protagonista de portadas de revistas diarios portales web de aparecer incluso entre los personajes más populares de google junto a maría teresa campos estarán en el plató patricia pérez presentadora del programa matinal de los sábados en telecinco vuélveme loca quien ha las campanadas en cuatro ocasiones y los comentaristas maribel escalona emilio pineda josé manuel parada.los vuelos han venido registrando este viernes importantes retrasos en barajas a pesar de que desde las 15.00 el aeropuerto opera con las cuatro pistas según han informado fuentes de aena mientras las compañías han denunciado demoras por parte de los controladores de hasta 60 minutos con los pasajeros embarcados según los datos facilitados por aena la ausencia por la mañana de de los 18 controladores que estaban programados en el turno de la torre de control de barajas obligó a cerrar dos de las pistas del aeropuerto lo que generó retrasos medios de 30 minutos – turkish summary ataması yapılmayan ö˘gretmenler miting yaptı ö˘gretmen adaylarına muharrem ˙ince ve tekel i¸sçileri de destek verdi body yetersiz açılan kadrolar nedeniyle ataması yapılamayan ö˘gretmen adayları ankara da miting yaptı tekel i¸sçilerinin de destek verdi˘gi ö˘gretmen adaylarının mitinginde ö˘gretmen kökenli chp milletvekili muharrem ˙ince de hazır bulundu türkiye nin çe¸sitli illerinden gelen ataması yapılmayan ö˘gretmenler platformu üyesi sözle¸smeli ö˘gretmenler ö˘gle saatlerinde abdi ˙ipekçi parkı nda toplandı ” milletvekilli˘gi için kpss getirilsin , 1 kadrolu ö˘gretmen 3 ücretli ö˘gretmen ve ücretli köle olmayaca˘gız yazılı dövizler ta¸sıyan ve aynı içerikli sloganlar atan ö˘gretmenlerin düzenledi˘gi mitinge bazı siyasi parti sivil toplum kurulu¸su temsilcileri ve tekel i¸sçileri de destek verdi chp yalova milletvekili muharrem ˙ince okullarda derslerin bo¸s geçti˘gini öne sürerek ” okullar ö˘gretmensiz ö˘gretmenler ise i¸ssiz dedi hükümetin bu gençlerin sesini duyması gerekti˘gini belirten ˙ince ” bu ülkenin 250 bin e˘gitim fakültesi mezunu genci i¸s bekliyorsa bu hükümetin ve ülkenin ayıbıdır e˘gitim sorununu çözememi¸s bir hükümet bu ülkenin hiçbir sorununu çözememi¸s demektir bu kadar önemli bir soruna kulaklarını tıkayamaz diye konu¸stu ” ankara nın göbe˘ginde derslerin bo¸s geçti˘gini ileri süren ˙ince ” bu ülkede ﬁzik ve matematik ö˘gretmeni atanmıyor ama bunların 100 katı din dersi ö˘gretmeni atanıyor dedi platform adına yapılan açıklamada da türkiye de her yıl üniversite bitirerek diplomasını alan ö˘gretmenlerin e˘gitim alanındaki yetersizlik dolayısıyla i¸ssizler kervanına katıldı˘gı ifade edildi talep edilen hakların insancıl ve makul oldu˘gu belirtilen açıklamada ö˘gretmenlerin haklarını vermeyenlerin kötü niyetli oldu˘gu öne sürüldü açıklamada hükümetin e˘gitim politikası ele¸stirilerek sözle¸smeli ö˘gretmenlerin kadrolu atamalarının yapılması ö˘gretmen yeti¸stiren fakültelere ö˘gretmen ihtiyacı kadar ö˘gretmen adayı alınması ve kpss yerine daha ¸seffaf bir atama sistemi getirilmesi istendi ölüm orucu ba¸slatacaklar çe¸sitli sivil toplum kurulu¸su temsilcilerinin de konu¸stu˘gu mitingde kadrolu atamalar yapılmadı˘gı takdirde i¸s bırakma eylemi ve ölüm orucu yapılaca˘gı duyuruldu 8066 russian summary старший преподаватель института коммунального хозяйства строительства был задержан на днях москве за растление школьника body сладострастник течение трех лет преследовал подростка надежде совратить его как сообщили мк в следственном отделе по хорошевскому району су ск при прокуратуре рф по москве 26 августа 2006 года 13-летний павел вместе другом отдыхал на берегу москвы–реки рядом крылатским мостом там ребятам подошел мужчина новый знакомый представился евгением предложил вместе пообедать ресторане быстрого питания а потом искупаться именно там на берегу педагог начал приставать мальчику школьник убежал но педофил успел снять голого подростка на мобильный телефон после этого жизнь мальчика превратилась сущий ад евгений узнал где живет павел и стал шантажировать его этот кошмар продолжался три года преподаватель угрожал показать фотографию друзьям знакомым павла негодяй исписал непотребными надписями стены подъезда где проживали друзья школьника в один из дней он приехал сергиев посад к бабушке мальчика и там накинулся на школьника ножом наконец отчаявшийся подросток рассказал обо всем матери и та обратилась милицию первый раз стражи порядка упустили 43-летнего педофила к слову он разведен . милиционеры нагрянули его квартиру а евгений под предлогом прощания мамой-инвалидом зашел соседнюю комнату выпрыгнул из окна был таков задержать извращенца стражам порядка помогла случайность несколько дней назад мать павла увидела евгения на станции метро александровский сад . на мужчине красовался парик но женщина узнала негодяя толпе милиционеры дежурившие на станции по просьбе дамы задержали растлителя в его кармане они обнаружили нож кстати за несколько дней до задержания мужчина подкараулил павла на улице ударил его по лицу обзывая нехорошими словами всех чьи дети стали жертвами педофила просят звонить по телефону 8-499-19788-17 или 02 8067 tablegpt few-shot table-to-text generation with table structure reconstruction and content matching heng gong1∗ yawei sun1∗ xiaocheng feng1,2 bing qin1,2 wei bi xiaojiang liu ting liu1,2 1department of computer science and technology harbin institute of technology harbin china 2peng cheng laboratory hgong ywsun xcfeng qinb tliu @ ir.hit.edu.cn xiaojiangliu84 hotmail.com weibi cse.ust.hk abstract although neural table-to-text models have achieved remarkable progress with the help of largescale datasets they suffer insufﬁcient learning problem with limited training data recently pretrained language models show potential in few-shot learning with linguistic knowledge learnt from pretraining on large-scale corpus however beneﬁting table-to-text generation in few-shot setting with the powerful pretrained language model faces three , including 1 the gap between the task s structured input and the natural language input for pretraining language 2 the lack of modeling for table structure and 3 improving text ﬁdelity with less model incorrect expressions that are contradicting to the table to address aforementioned problems we propose tablegpt for table-to-text generation at ﬁrst we utilize table transformation module with template to rewrite structured table in natural language as input for gpt-2 in addition we exploit multi-task learning with two auxiliary tasks that preserve table s structural information by reconstructing the structure from gpt-2 s representation and improving the text s ﬁdelity with content matching task aligning the table and information in the generated text by experimenting on humans songs and books three few-shot table-to-text datasets in different domains our model outperforms existing systems on most few-shot settings 1 introduction table-to-text generation aiming at generating descriptive text about important information in structured data has well application prospect in communicating with human in a comprehensible and natural way such as ﬁnancial report murakami et al. 2017 , medical report hasan and farri 2019 generation etc in recent years data-driven models have shown impressive capability to produce informative and ﬂuent text with the help of large-scale datasets such as wikibio lebret et al. 2016 and e2e duˇsek et al. 2020 . however it is not always feasible to collect large-scale labeled dataset for various domains in the real world resulting in unsatisfying performance due to the insufﬁcient training such few-shot learning setting for table-to-text generation is not well-explored and in this paper we focus on exploring how to efﬁciently model for few-shot table-to-text generation with limited training pairs recently pre-trained language models have shown promising progress in various natural language processing tasks yang et al. 2019b devlin et al. 2019 radford et al. 2019 . they can capture linguistic knowledge by pretraining on large-scale unlabeled dataset and generalize to downstream tasks with little labeled data in target domain effectively modeling for few-shot setting peng et al. 2020 . however efforts to beneﬁt table-to-text generation from the powerful pre-trained language model especially in few-shot setting are non-trivial due to three . 1 there is a gap between the structured data input for table-to-text generation and natural language input that is used for pretraining gpt-2 ( ) also it lacks modeling of the table s structure which contains rich information to understand the input before generating text ( ) additionally it doesn t address how to maintain text s ﬁdelity for table-to-text gener equal contribution this work is licensed under a creative commons attribution 4.0 international license license details http // creativecommons.org/licenses/by/4.0/ proceedings of the 28th international conference on computational linguistics pages 1978–1988 barcelona spain online , december 8-13 2020 1978 attribute value james beattie james scott beattie 27 february 1978 lancaster england striker swansea city assistant ﬁrst team coach name fullname birth position currentclub nationalteam england u21 england article ... reference footballer who played as a striker james beattie footballer ... james scott beattie born 27 february 1978 is an english former professional table : a sample of table-text pair for table-to-text generation from humans domain the highlighted phrases are information mentioned in text ation while exploiting linguistic knowledge from pretraining corpus that is the highlighted information in text table ) should correctly derive from structured data in order to alleviate aforementioned problems we propose tablegpt that focus on generating highﬁdelity text for table-to-text generation with limited training pairs addressing the gap between structured table input and natural language input that gpt-2 processes during pretraining we utilize a table transformation module that employs template to naturally transform structured table into natural language in addition we utilize two auxiliary tasks under the framework of multi-task learning table structure construction and content matching targeting pretrained gpt-2 s lack of modeling for table structure and text s ﬁdelity in detail the table structure reconstruction task is proposed for gpt-2 which force it to embed table structure into its representation when modeling structured table besides we utilize content matching task that help model correctly describe important information from table via optimal-transport chen et al. 2019a technique which measures the distance between the information in generated text and information in table and use the distance as penalty for text with incorrect information we conducted experiments on three data-to-text datasets on different domains chen et al. 2020b : humans books songs in various settings both automatic evaluation and human evaluation results show that our model can achieve new state-of-the-art performance for table-to-text generation in terms of generating ﬂuent and high-ﬁdelity text in most few-shot settings 2 background 2.1 task deﬁnition for the table-to-text task discussed in this paper we can formulate each training instance as pair of table and summary = s t . given a table which can be formulated as sets of records = ri n i=1 the model is expected to generate descriptive text = w1 w2 ... wl n is the number of records and is the number of words in text each record ri consists of two type of information ri.a and ri.v ri.a denotes the attribute of the record e.g name and ri.v denotes the corresponding value e.g james beattie . please note that both ri.a and ri.v can be viewed as a sequence of words 2.2 pre-trained language model recently pre-trained language models such as bert devlin et al. 2019 , gpt-2 radford et al. 2019 , xlnet yang et al. 2019b and more have achieved remarkable progress in various nlp tasks the main idea is to pretrain a neural language model with large number of parameters on large-scale dataset in order to capture the linguistic knowledge at ﬁrst then transfer those knowledge to downstream task 1979 figure : tablegpt s training process tokens in red indicate information in reference and those in blue refers to value tokens corresponding attribute ( a table transformation employs template to rewrite the structured table in natural language and concatenate with reference to form a training pair resulting in language model loss llm with gpt-2 ( ) table structure reconstruction and c content matching are auxiliary tasks the former reconstructs attributes from gpt-2 s representation of value tokens forming lsr the latter use optimal transport to measure distance between information mentioned in text and table using this as content matching loss lcm the model is jointly ﬁnetuned with above three losses via ﬁnetuning on the task s dataset impressively they can outperform various nlp tasks previous stateof-the-arts by a large margin since we investigate table-to-text generation task in this paper instead of natural language understanding tasks we choose gpt-2 as the basis of our model the model structure for gpt-2 is a 12-to-48-layer transformer decoder vaswani et al. 2017 with 117 million to 1542 million parameters each layer consists of a stack of masked multi-head self attention and feed-forward neural network with residual connection and layer normalization the pre-training target of gpt-2 is as the same as language model maximizing gold text s probability the large-scale model is trained on vast and diverse webtext dataset with million documents collected from the internet its success in the area of text generation attributes to both its high capacity model and knowledge learnt from pretraining on vast dataset 3 approach in this section we propose to address the three incompatibility between pretrained language model and table-to-text generation illustrated in section 1. figure presents the overall multi-task training framework of our model we ﬁrst utilize table transformation module to reasonably transform structured table into text sequence and aggregate it with reference text resulting in a suitable training sequence for gpt2 model then two auxiliary tasks table structure reconstruction and content matching with optimal transport chen et al. 2019a are performed on top of gpt-2 s representation of the training sequence those two auxiliary tasks training objectives along with gpt-2 s language model training objective are jointly ﬁnetuned based on the pre-trained gpt-2 under the multi-task training framework the overall objective here is to produce high-ﬁdelity text while maintaining its ﬂuency 3.1 table transformation as noted in section 2.1 the given structured table consists of multiple records as attribute-value pairs in order to adapt to the sequential nature of language model we employ a template-based table serialization 1980 language model loss 𝑳𝑳𝑴 james scott beattie ... endoftext pretrained gpt-2 model name is james beattie full ... transformed table < table2text james scott beattie reference ... attribute name full name birth poistion nationalteam article ... value james beattie james scott beattie 27 february 1978 lancaster england striker england u21 england james beattie footballer ... table a table transformation table serialization james scott beattie born 27 february 1978 is an english former professional footballer who played as a striker concatenation b table structure reconstruction reference structure reconstruction loss 𝑳𝑺𝑹 not not name name not not not not full name full name full name not not not ... name is james beattie full name is james scott beattie birth is ... table2text james scott beattie february 1978 ... is pretrained gpt-2 model transformed table c content matching transformed table name is james beattie full name is james scott beattie birth ... optimal transport distance pretrained gpt-2 model born 27 reference loss 𝑳𝑪𝑴 content matching word embedding hidden state method chen et al. 2019b to encode such structured table as a sequence for example we serialized the attribute-value pair name jack reynolds as a sentence name is jack reynolds. and concatenate all of them into a document according to the order of records in table after obtaining the serialized structured table we connect it with the corresponding natural language description with special token < table2text ” it serves as a functional token that both encodes the overall information of the table and as a starting signal to generate text the whole sequence is ended with special token < endof text ” in this way our model encodes the structured table and learns to predict the target sequence one word at a time as in gpt-2 we denote the ﬁnal input sequence as st st1 ··· stm+n+2 m is the length of serialized table n is the length of text and refers to the special tokens mentioned above the language model s training objective is to maximize the likelihood of the reference text which is equivalent to minimize the negative log likelihood language model loss llm as characterized by equation 1. llm − n 1 logp sti|st i m+n+2 cid:88 i=m+2 3.2 table structure reconstruction as shown in table , unlike many natural language generation tasks that take sentences as input tableto-text generation models need to process table with structural information each data record in the input can be seen as a pair of attribute and value traditionally table-to-text models utilized attribute-value concatenation to represent the tables in this way they are able to capture the structural information by learning the correspondence between value and attribute however when we transform the table into natural language and use pre-trained language model gpt-2 for representation it lacks the explicit modeling to incorporate such structural information inspired by liu et al ( 2019 , our model treats the attribute names as the labels for the model to reconstruct such structural information from gpt-2 s learned table values representation in detail as shown in figure ( ) given a serialized table sk which consists of different attributes and values in natural language form the table structure reconstruction task takes the last layer of gpt-2 s hidden states for each value tokens h ti j i=1 n j=1 mi of the table and classify which attribute does each value token s representation refer to speciﬁcally the means the ith record of the table sk j means the jth value token of ith record n is the number of records and mi is the number of tokens of ith record s value p ai j = softmax hti jwt bt lsr − z logp a cid:63 i j n cid:88 mi cid:88 i=1 j=1 1 ( ) 3 equation shows the detail of the reconstruction classiﬁer please note that the serialized table consists of attribute value and template tokens in this auxiliary task we only take the gpt-2 s hidden states for those value tokens and reconstruct the structural information by classifying their corresponding attribute wt and bt are the trainable parameters of the introduced classiﬁer and ( ai j is probability to classify value token ti j as referring to attribute ai j we use cross entropy as this task s objective function illustrated by equation and 3. a cid:63 i j refers to the gold attribute label for the value token and is the number of value tokens that need to reconstruct the corresponding attribute label by incorporating this auxiliary task tablegpt can be guided to embed structural information when representing the table at the training stage 3.3 content matching take table as an example generating high ﬁdelity text that correctly describe information in the table is the core of table-to-text generation producing ﬂuent but incorrect text still means unsatisfying performance as the text is not reliable for the purpose of disseminating comprehensible information ideally 1981 when generating words that is intended to describe information in table directly copying them from table will result in high-ﬁdelity text however it s non-trivial to integrate a copy mechanism inside the transformer architecture of gpt-2 model since the change of model structure may break syntactic and semantic features contained in the pretrained language model which are essential for text generation especially in few-shot setting also rephrasing sometimes is needed to produce more natural text in order to encourage our model to generate high-ﬁdelity text while keeping gpt-2 s advantage of produing ﬂuent text we utilize another auxiliary task called content matching task during ﬁnetuning on the table-to-text corpus the content matching task is to explicitly match the important information in a table with information in the corresponding generated text an intuitive way is to apply a mis-matching loss by hard-matching key information in table and information in the generated text but that is discrete and non-differentiable and the corresponding gradient descent can t be learned directly inspired by optimal transport ot that can measure the distance between information in source sequence and target sequence chen et al. 2019a without breaking the end-to-end training process we adopt it as a content matching loss that guide the model to generate text containing information that align with the table as in section 3.1 the whole gpt-2 training sequence consists of serialized table and reference text the serialized table sequence x x1 ··· xm can be represented as a discrete distribution ui 1 m is the length and δx is the dirac function centered on x. similar with the serialized table sequence the discrete distribution of reference text sequence j=1 vjδyj under such setting computing the ot optimal transport distance between probability distributions = ui m j=1 is deﬁned as the solution of the following network-ﬂow problem luise et al. 2018 : = cid:80 m i=1 uiδxi where ui 0 and cid:80 y y1 ··· yn can be represented as = cid:80 n i=1 and = vj n lcm min t∈π µ ν tij d xi yj ( ) ( ) ( ) i=1 j=1 where ( , ) { ∈ rm×n |t1n µ t cid:62 1m ν which is the set of joint distribution of the two marginal distribution and , 1n and 1m are n-dimensional all-one vector and m-dimensional all-one vector respectively and ( xi yj denotes the cost of moving xi to yj especially we adopt the cosine distance between two token embedding vectors of xi and yj which is deﬁned as ( xi yj = − in order to overcome this problem we use the recently proposed inexact proximal point method for optimal transport ipot ( chen et al. 2019a as an approximation . exact minimization over is computational intractable ( ) xi cid:107 2 cid:107 yj cid:107 2 xiyj for natural language generation tasks such as neural machine translation ot distance is often applied by matching source sequence with whole target sequence since almost every word in both sequences are supposed to be matched however when it comes to table-to-text task in a realistic way there are some redundant information or words in both table and text in order to apply the ot distance unlike previous adoption wang et al. 2020 based on the assumption that all information in the table should be described in text we propose to only match the record words which appear in both table and reference text in this way the ot distance is able to avoid wrongly penalizing text that doesn t mention redundant information in table 3.4 learning objective for table structure reconstruction and content matching both auxiliary tasks are trained with the main gpt-2 s language model loss together which can be regarded as multi-task learning the loss function of multi-task learning consists of language model loss llm table structure reconstruction loss lsr and content matching loss lcm in this way the loss function lm of the full model is computed as follows lm = llm λ1lsr λ2lcm ( ) where λ1 and λ2 are hyper-parameters that are two scale factors please note that when optimizing the lcm with ipot algorithm the gradients of ot loss are hard to back propagate to model s parameters since the process of sampling words from multinomial distribution which comes from language model 1982 humans 200 100 5.1 6.1 domain books songs base 50 7.3 50 2.9 500 8.3 100 6.8 of training instances 50 500 10.4 12.0 11.6 13.1 base switch lm 25.7 29.5 36.1 41.7 34.3 36.2 37.9 40.3 36.1 37.2 39.4 42.2 base switch lm r 25.1 27.7 33.7 41.6 33.8 35.4 37.6 40.9 35.3 37.8 39.2 41.8 29.8 34.5 40.6 45.6 35.1 37.3 38.5 41.6 36.7 37.8 39.3 42.3 29.1 34.0 40.2 45.5 34.8 37.1 37.1 41.5 35.6 37.7 39.2 41.8 29.2 33.5 40.0 45.6 34.8 37.0 36.5 42.2 35.7 37.4 39.2 41.8 29.0 32.9 40.0 45.3 34.2 36.5 38.4 41.4 35.5 37.0 39.0 41.6 tablegpt 500 8.8 200 7.8 -sr -cm -sr cm 100 200 table : bleu-4 results on humans books and songs domains humans 100 200 0.8 0.4 domain books songs base 50 1.7 50 0.1 500 1.5 100 1.5 of training instances 500 5.8 base switch lm 14.1 16.2 22.1 28.3 22.5 23.1 25.0 27.6 26.2 28.6 30.1 32.6 base switch lm r 12.1 14.1 19.1 28.2 22.2 22.2 24.4 27.3 27.0 28.3 28.8 32.0 16.3 20.6 27.6 32.4 24.0 25.4 26.7 28.9 27.1 29.4 30.6 32.8 16.7 20.2 26.2 32.3 23.4 25.7 26.5 29.2 26.3 29.0 31.0 33.5 15.7 20.0 25.4 31.6 22.3 24.9 26.3 29.7 26.2 28.6 30.9 33.1 16.2 19.3 26.6 32.1 23.7 25.1 26.4 28.7 25.8 25.9 30.6 33.7 tablegpt 500 2.4 200 4.7 100 5.1 200 2.1 50 4.1 -sr -cm -sr cm table : rouge-4 f-measure results on humans books and songs domains is unfortunately non-differentiable in order to back propagate lcm we adopt the soft-argmax trick to approximate each word embedding vector during testing we feed the serialized table with special token table2text into gpt-2 and generate the text word by word 4 experiment 4.1 datasets and experiment settings following chen et al ( 2020b ’ work on few-shot table-to-text generation we conducted experiments on three datasets in different domains humans books and songs respectively each instance in those datasets consists of the structured infobox as the table and ﬁrst sentence of the article as reference same as chen et al ( 2020b , we train our model on different few-shot setting with training dataset size varying from 50 100 200 to 500. also models are chosen based on performance on validation set with 1000 instances test sets for humans books and songs consist of 13587 5252 and 11879 instances respectively we implement tablegpt based on the transformers library wolf et al. 2019 . the conﬁguration of base gpt-2 model is 12 layers and attention heads per layer for optimizer we adopt the openaiadamw optimizer with 100 warm steps we train the model with learning rate set to 2e-4 the batch size is set to 10 for all datasets the weights λ1 λ2 of the table structure reconstruction loss and content matching loss are both 0.2 according to performance on validation set following chen et al ( 2020b ’ way to deal with the vocabulary limitation for all datasets we use the byte pair encoding bpe and subword vocabulary as in radford et al ( 2019 . 4.2 comparing methods we compare our proposed tablegpt with baseline model and previous state-of-the-art model base and base+switch+lm more details can be found in chen et al ( 2020b . 1983 domain metrics reference tablegpt base+switch+lm r humans cont nat 1.27 cid:63 0.36 cid:63 0.96 cid:63 0.85 0.89 0.60 sup 4.20 cid:63 3.20 cid:63 3.67 books cont nat 1.03 cid:63 0.43 0.46 1.15 cid:63 0.83 0.42 songs cont nat 1.26 cid:63 0.64 cid:63 0.75 0.73 cid:63 1.01 0.77 sup 3.22 2.92 3.05 sup 3.54 cid:63 3.32 3.28 table : human evaluation results models with cid:63 perform signiﬁcantly different from tablegpt p 0.05 , using a one-way anova with posthoc tukey hsd tests • base it is based on a seq2seq model with ﬁeld-gating encoder that incorporate the table structure s information during encoding liu et al. 2018 . additionally it utilizes the pre-trained word embedding which is ﬁxed during the training stage since it achieves competitive performance on large-scale dataset it can show how a data-driven seq2seq model typically perform with limited training data • base switch lm it tries to exploit gpt-2 s learnt knowledge from pretraining on vast corpus by proposing a switch policy that choosing between copying from infobox and generating from the gpt-2 language model when generating each word in text we also use the released codes and data1 by chen et al ( 2020b to reproduce its result for human evaluation and report corresponding automatic evaulation results as base switch lm r . tablegpt in this paper we propose tablegpt that exploits gpt-2 s learnt knowledge from pretraining on vast corpus for few-shot learning while enhance it for generating high ﬁdelity text with two auxiliary tasks also we perform ablation studies for evaluating each auxiliary task s contribution -sr represents the variant without table structure reconstruction -cm represents the variant without content matching and -sr cm shows the performance of gpt-2 without auxilary tasks 4.3 automatic evaluation following the previous work chen et al ( 2020b , we adopt bleu-4 papineni et al. 2002 and rouge4 lin 2004 to conduct automatic evaluations table and show corresponding results of comparing methods on different datasets although the base achieves competitive results when training on largescale dataset liu et al. 2018 , the performance drops drastically in few-shot setting while utilizing a switch policy to combine copying words from table and generating words from gpt-2 base switch lm can achieve impressive performance in all few-shot setting a standard gpt-2 model tablegpt sr cm that takes a serialized table as input and generate text afterwards without copying can actually perform better in most of the few-shot setting tablegpt with table structure reconstruction and content matching that preserves structural information during encoding and guide the model to generate high-ﬁdelity text can further improve the performance ablation studies also show that each of the auxiliary task attributes to the performance enhancement and applying both of them can achieve the best performance in most of the few-shot setting 4.4 human evaluation following the settings in chen et al ( 2020b , we conduct human evaluation on tablegpt with previous state-of-the-art model base+switch+lm r and reference from two aspects factual correctness and language naturalness we sampled 100 tables along with corresponding generated text from humans books and songs test set under the few-shot setting of 100 training data respectively resulting in 300 tables in total in order to reduce variance caused by human each example is evaluated by three different graduates who have passed intermediate english test and the scores are averaged in table 4. the ﬁrst evaluation criteria factual correctness focuses on how well the text can correctly describe information provided by the corresponding table each rater was provided with both the text and table they are asked to count how many facts contained in the text are consistent with information from table 1https //github.com/czyssrs/few-shot-nlg 1984 attribute value james beattie james scott beattie 27 february 1978 lancaster england 1 striker swansea city assistant ﬁrst team coach 1995 1996 blackburn rovers 443 131 1996 2003 england u21 england 5 0 2013 2014 accrington stanley james beattie footballer ... name fullname birth height position currentclub youthyears youthclubs totalcaps totalgoals nationalyears nationalteam nationalcaps nationalgoals manageryears managerclubs article ... reference footballer who played as a striker base+switch+lm r : james beattie , born 10 july 1971 is an english former professional association footballer who played for among others england tablegpt james beattie born 27 february 1978 in lancaster is a former english footballer who played as a striker james scott beattie born 27 february 1978 is an english former professional table : a sample table from human test set and the texts generated with different methods trained with 100 training data the facts that both exist in source table and summary text are highlighted with the same color in background the red text indicates the wrong fact of generated text conﬂicts with information in source table the blue text indicates the generated text fragment has grammatical error noted as sup and how many are contradicting with or missing from the table noted as cont we report the average number of supporting facts # sup and contradicting facts # cont on different dataset in table 4. the second evaluation criteria language naturalness tries to evaluate these models from the aspect of grammaticality is the sentence grammatically correct ) and ﬂuency of the text is the sentence ﬂuent and natural ) we arrange text from different models on the same table into pairs for each pair of text without table raters are asked to which one is better or whether both text are of same quality solely in terms of language naturalness when a generated summary is chosen as the better one we assign 1.0 score to the better one and 0.0 score to the worse if two summaries are deemed of same quality we assign 0.5 score to both of them we then calculate the average scores and report results on different dataset in table 4. results show that tablegpt can produce less contradicting facts than previous state-of-the-art model base+switch+lm r on humans and books and achieves comparable performance on songs meanwhile tablegpt can include more supporting facts on humans and songs and generate more natural text than base+switch+lm r on humans and songs overall our tablegpt model can improve text ﬁdelity while preserving the naturalness of the text 1985 4.5 case study compared with the previous state-of-the-art model base+switch+lm r and reference tablegpt performs better it can accurately describe most of the key information in ﬂuent text compared with reference for base+switch+lm r , the design of separate copy mechanism and gpt-2 may attributes to the inconsistent expression played for among others england ” and the expression of wrong birth date which shows the imperfect switch policy on when to copy from table can sometimes hurt model s ability to generate high-ﬁdelity text on the contrary tablegpt enhanced to generate high-ﬁdelity text with two auxiliary tasks without breaking one uniﬁed gpt-2 model for generating text performs better in terms of ﬁdelity and ﬂuency of text in this example 5 related work in recent years neural models for generating texts directly from preprocessed data wiseman et al. 2017 puduppully et al. 2019a puduppully et al. 2019b gong et al. 2019 feng et al. 2020 , have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset mei et al ( 2016 proposes a pre-selector on encoder-aligner-decoder model for generation which strengthens model s content selection ability and obtains considerable improvement over standard seq2seq model sha et al ( 2018 proposes a hybrid attention mechanism for modeling the order of content when generating texts liu et al ( 2018 presents a ﬁeld-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the structure information when decoding in addition bao et al ( 2018 develops a table-aware sequence-to-sequence model on this task however chen et al ( 2020b shows that the well-performed seq2seq model trained on large-scale dataset suffer from limited training data in few-shot setting recently gpt-2 has been successfully adapted to dialogue generation in few-shot setting zhang et al. 2020 peng et al. 2020 , showing potential to address insufﬁcient training data problem for few-shot learning with the help of learnt knowledge from pretraining on vast corpus as for table-to-text generation chen et al ( 2020b propose a switch model that use gpt-2 to generate template-like functional words while generating factual expressions via copying records values from table in few-shot scenario different from this work we model the table and generate text within a gpt-2 model in a uniﬁed way and we show that our proposed tablegpt can perform well in the few-shot scenario in addition different from both works mentioned above we enhance gpt-2 s ability to model table structure and to improve text ﬁdelity another closely related paper is chen et al ( 2019b , which predicts whether a statement align with records in the table since the nature of classiﬁcation task makes it possible to model table records bidirectionally it use bert with templates to transform and model the table meanwhile chen et al ( 2020a explores coarse-to-ﬁne table-to-text generation with standard gpt-2 model different from above two works we adapted gpt-2 in a text generation scenario for structured data input and more importantly address table structure modeling and improve text ﬁdelity in addition one of the auxiliary task content matching is inspired by ideas in machine translation yang et al. 2019a and seq2seq chen et al. 2019a model the closest paper on data-to-text generation is wang et al ( 2020 . they assume that the expected generated text should cover all information in the table but in a more realistic scenario like the task we explore in this paper the table consists of redundant information and only the important ones should be used to constraint model to generate high-ﬁdelity text therefore we propose to match important information only in the table and information in text as an auxiliary task during training 6 conclusion in this work we present tablegpt which enhances gpt-2 for table-to-text generation with two auxiliary tasks table structure reconstruction and content matching for improving text ﬁdelity while exploiting gpt-2 s learnt linguistic knowledge from pretraining on large-scale corpus in detail we use table transformation to bridge the gap between structured table and natural language input for gpt-2 and further enhance gpt-2 with following two auxiliary tasks for table-to-text generation the table structure reconstruction task help model preserve the structural information of input while representing table with powerful pretrained gpt-2 in addition the content matching task guides model to generate high-ﬁdelity 1986 task with less incorrect expressions that are contradicting to the table via measuring distance between table and information in generated text experiments are conducted on three datasets humans books and songs in different domains both automatic evaluation and human evaluation results show that our model achieves new state-of-the-art performance in most few-shot setting acknowledgements we would like to thank the anonymous reviewers for their helpful comments xiaocheng feng is the corresponding author of this work this work is supported by the national key & program of china via grant 2018yfb1005103 and national natural science foundation of china nsfc via grant 61906053 and 61976073. references junwei bao duyu tang nan duan zhao yan yuanhua lv ming zhou and tiejun zhao 2018. table-to-text describing table region with natural language in aaai pages 5020–5027 liqun chen yizhe zhang ruiyi zhang chenyang tao zhe gan haichao zhang bai li dinghan shen improving sequence-to-sequence learning via optimal trans changyou chen and lawrence carin 2019a port in iclr wenhu chen hongmin wang jianshu chen yunkai zhang hong wang shiyang li xiyou zhou and william yang wang 2019b tabfact a large-scale dataset for table-based fact veriﬁcation iclr wenhu chen jianshu chen yu su zhiyu chen and william yang wang 2020a logical natural language generation from open-domain tables in acl pages 7929–7942 zhiyu chen harini eavani wenhu chen yinyin liu and william yang wang 2020b few-shot nlg with pre-trained language model in acl pages 183–190 jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirec tional transformers for language understanding in naacl-hlt pages 4171–4186 ondˇrej duˇsek jekaterina novikova and verena rieser 2020. evaluating the state-of-the-art of end-to-end natural language generation the e2e nlg . computer speech language 59:123–156 xiaocheng feng yawei sun bing qin heng gong yibo sun wei bi xiaojiang liu and ting liu 2020. learning to select bi-aspect information for document-scale text content manipulation in aaai pages 7716– 7723. heng gong xiaocheng feng bing qin and ting liu 2019. table-to-text generation with effective hierarchical encoder on three dimensions row column and time . in emnlp pages 3141–3150 sadid a hasan and oladimeji farri 2019. clinical natural language processing with deep learning in data science for healthcare pages 147–171 r´emi lebret david grangier and michael auli 2016. neural text generation from structured data with applica tion to the biography domain in emnlp pages 1203–1213 chin-yew lin 2004. rouge a package for automatic evaluation of summaries in text summarization branches out pages 74–81 tianyu liu kexiang wang lei sha baobao chang and zhifang sui 2018. table-to-text generation by structure aware seq2seq learning in aaai pages 4881–4888 tianyu liu fuli luo qiaolin xia shuming ma baobao chang and zhifang sui 2019. hierarchical encoder with auxiliary supervision for neural table-to-text generation learning better representation for tables in aaai pages 6786–6793 giulia luise alessandro rudi massimiliano pontil and carlo ciliberto 2018. differential properties of sinkhorn approximation for learning with wasserstein distance in advances in neural information processing systems pages 5859–5870 hongyuan mei tti uchicago mohit bansal and matthew walter 2016. what to talk about and how selective generation using lstms with coarse-to-ﬁne alignment in naacl-hlt pages 720–730 1987 soichiro murakami akihiko watanabe akira miyazawa keiichi goshima toshihiko yanase hiroya takamura and yusuke miyao 2017. learning to generate market comments from stock prices in acl pages 1374–1384 kishore papineni salim roukos todd ward and wei-jing zhu 2002. bleu a method for automatic evaluation of machine translation in acl pages 311–318 baolin peng chenguang zhu chunyuan li xiujun li jinchao li michael zeng and jianfeng gao 2020. few-shot natural language generation for task-oriented dialog arxiv preprint arxiv:2002.12328. ratish puduppully li dong and mirella lapata 2019a data-to-text generation with content selection and planning in aaai pages 6908–6915 ratish puduppully li dong and mirella lapata 2019b data-to-text generation with entity modeling in acl alec radford jeff wu rewon child david luan dario amodei and ilya sutskever 2019. language models are unsupervised multitask learners lei sha lili mou tianyu liu pascal poupart sujian li baobao chang and zhifang sui 2018. order-planning neural text generation from structured data in aaai pages 5414–5421 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n. gomez lukasz kaiser and illia polosukhin 2017. attention is all you need in advances in neural information processing systems pages 5998–6008 zhenyi wang xiaoyang wang bang an dong yu and changyou chen 2020. towards faithful neural table-to text generation with content-matching constraints in acl pages 1072–1086 sam wiseman stuart shieber and alexander rush 2017. in data-to-document generation in emnlp pages 2253–2263 thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pierric cistac tim rault r´emi louf morgan funtowicz and jamie brew 2019. huggingface s transformers state-of-theart natural language processing mingming yang rui wang kehai chen masao utiyama eiichiro sumita min zhang and tiejun zhao 2019a sentence-level agreement for neural machine translation in acl pages 3076–3082 zhilin yang zihang dai yiming yang jaime carbonell russ salakhutdinov and quoc le 2019b xlin advances in neural information net generalized autoregressive pretraining for language understanding processing systems pages 5754–5764 yizhe zhang siqi sun michel galley yen-chun chen chris brockett xiang gao jianfeng gao jingjing liu and bill dolan 2020. dialogpt large-scale generative pre-training for conversational response generation in acl pages 270–278 1988 1203 proceedings of the 2016 conference on empirical methods in natural language processing pages 1203–1213 austin texas november 1-5 2016. ( ) 2016 association for computational linguistics neural text generation from structured data with application to the biography domain r´emi lebret∗ epfl switzerland david grangier facebook ai research michael auli facebook ai research abstract this paper introduces a neural model for concept-to-text generation that scales to large rich domains it generates biographical sentences from fact tables on a new dataset of biographies from wikipedia this set is an order of magnitude larger than existing resources with over 700k samples and a 400k vocabulary our model builds on conditional neural language models for text generation to deal with the large vocabulary we extend these models to mix a ﬁxed vocabulary with copy actions that transfer sample-speciﬁc words from the input database to the generated output sentence to deal with structured data we allow the model to embed words differently depending on the data ﬁelds in which they occur our neural model significantly outperforms a templated kneser-ney language model by nearly 15 bleu 1 introduction renders generation concept-to-text structured records into natural language reiter et al. 2000 . a typical application is to generate a weather forecast based on a set of structured meteorological measurements in contrast to previous work we scale to the large and very diverse problem of generating biographies based on wikipedia infoboxes an infobox is a fact table describing a person similar to a person subgraph in a knowledge base bollacker et al. 2008 ferrucci 2012 . similar generation applications include the generation of product descriptions based on a catalog of millions of items with dozens of attributes each previous work experimented with datasets that contain only a few tens of thousands of records such as weathergov or the robocup dataset while our dataset contains over 700k biographies from ∗r´emi performed this work while interning at facebook wikipedia furthermore these datasets have a limited vocabulary of only about 350 words each compared to over 400k words in our dataset to tackle this problem we introduce a statistical generation model conditioned on a wikipedia infobox we focus on the generation of the ﬁrst sentence of a biography which requires the model to select among a large number of possible ﬁelds to generate an adequate output such diversity makes it difﬁcult for classical count-based models to estimate probabilities of rare events due to data sparsity we address this issue by parameterizing words and ﬁelds as embeddings along with a neural language model operating on them bengio et al. 2003 . this factorization allows us to scale to a larger number of words and ﬁelds than liang et al ( 2009 , or kim and mooney 2010 where the number of parameters grows as the product of the number of words and ﬁelds moreover our approach does not restrict the relations between the ﬁeld contents and the generated text this contrasts with less ﬂexible strategies that assume the generation to follow either a hybrid alignment tree kim and mooney 2010 , a probabilistic context-free grammar konstas and lapata 2013 , or a tree adjoining grammar gyawali and gardent 2014 . our model exploits structured data both globally and locally global conditioning summarizes all information about a personality to understand highlevel themes such as that the biography is about a scientist or an artist while as local conditioning describes the previously generated tokens in terms of the their relationship to the infobox we analyze the effectiveness of each and demonstrate their complementarity 2 related work traditionally generation systems relied on rules and hand-crafted speciﬁcations dale et al. 2003 reiter et al. 2005 green 2006 galanis and androut 1204 sopoulos 2007 turner et al. 2010 . generation is divided into modular yet highly interdependent decisions ( ) content planning deﬁnes which parts of the input ﬁelds or meaning representations should be selected ( ) sentence planning determines which selected ﬁelds are to be dealt with in each output sentence and 3 surface realization generates those sentences data-driven approaches have been proposed to automatically learn the individual modules one approach ﬁrst aligns records and sentences and then learns a content selection model duboue and mckeown 2002 barzilay and lapata 2005 . hierarchical hidden semi-markov generative models have also been used to ﬁrst determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts liang et al. 2009 . sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence barzilay and lapata 2006 . end-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data ratnaparkhi 2002 wong and mooney 2007 belz 2008 lu and ng 2011 . more recently content selection and surface realization have been combined angeli et al. 2010 kim and mooney 2010 konstas and lapata 2013 . at the intersection of rule-based and statistical methods hybrid systems aim at leveraging human contributed rules and corpus statistics langkilde and knight 1998 soricut and marcu 2006 mairesse and walker 2011 . our approach is inspired by the recent success of neural language models for image captioning kiros et al. 2014 karpathy and fei-fei 2015 vinyals et al. 2015 fang et al. 2015 xu et al. 2015 , machine translation devlin et al. 2014 bahdanau et al. 2015 luong et al. 2015 , and modeling conversations and dialogues shang et al. 2015 wen et al. 2015 yao et al. 2015 . our model is most similar to mei et al ( 2016 who use an encoder-decoder style neural network model to tackle the weathergov and robocup tasks their architecture relies on lstm units and an attention mechanism which reduces scalability compared to our simpler design figure : wikipedia infobox of frederick parker-rhodes the introduction of his article reads “ frederick parker-rhodes 21 march 1914 21 november 1987 was an english linguist plant pathologist computer scientist mathematician mystic and mycologist. . language modeling for constrained sentence generation conditional language models are a popular choice to generate sentences we introduce a tableconditioned language model for constraining text generation to include elements from fact tables 3.1 language model given a sentence = w1 . . wt with words from vocabulary , a language model estimates tyt=1 ( ) p wt|w1 . . wt−1 . 1 let ct wt− n−1 , . , wt−1 be the sequence of − context words preceding wt an n-gram language model makes an order markov assumption p s ≈ ( wt|ct . 2 tyt=1 3.2 language model conditioned on tables a table is a set of ﬁeld/value pairs where values are sequences of words we therefore propose language models that are conditioned on these pairs local conditioning refers to the information from the table that is applied to the description of the words which have already generated i.e the previous words that constitute the context of the language 1205 table gf gw name birthdate birthplace occupation spouse children john doe 18 april 1352 oxford uk placeholder jane doe johnnie doe input text ct zct john ct 13944 name,1,2 zct doe unk name,2,1 ( spouse,2,1 ( children,2,1 ( 17 18 37 april 92 1352 is 25 18 12 a ∅ birthd.,1,3 ( birthd.,2,2 ( birthd.,3,1 ∅ output candidates w w q the ∅ zw . . . april 92 . . . placeholder 5302 . . . john 13944 . . . doe unk birthd.,2,2 ( occupation,1,1 ( name,1,2 ( name,2,1 ( spouse,2,1 ( children,2,1 figure : table features right for an example table left ; ∪ is the set of all output words as deﬁned in section 3.3. model the table allows us to describe each word not only by its string or index in the vocabulary but also by a descriptor of its occurrence in the table let deﬁne the set of all possible ﬁelds f. the occurrence of a word in the table is described by a set of ﬁeld position pairs zw ( ) fi pi ( ) i=1 ( ) where is the number of occurrences of w. each pair f p indicates that occurs in ﬁeld at position p. in this scheme most words are described by the empty set as they do not occur in the table for example the word linguistics in the table of figure is described as follows zlinguistics { ﬁelds 8 ; known for 4 } ( ) assuming words are lower-cased and commas are treated as separate tokens conditioning both on the ﬁeld type and the position within the ﬁeld allows the model to encode ﬁeld-speciﬁc regularities e.g. a number token in a date ﬁeld is likely followed by a month token knowing that the number is the ﬁrst token in the date ﬁeld makes this even more likely the ﬁeld position description scheme of the table does not allow to express that a token terminates a ﬁeld which can be useful to capture ﬁeld transitions for biographies the last token of the name ﬁeld is often followed by an introduction of the birth date like ( or was born . we hence extend our descriptor to a triplet that includes the position of the token counted from the end of the ﬁeld zw ( ) fi p+ , p−i ( ) i=1 where our example becomes ( ) zlinguistics { ﬁelds 8 4 ; known for 4 13 } we extend equation to use the above information as additional conditioning context when generating a sentence : ( s|z = ( wt|ct zct , 6 where zct zwt− n−1 , . , zwt−1 are referred to as the local conditioning variables since they describe the local context previous word relations with the table global conditioning refers to information from all tokens and ﬁelds of the table regardless whether they appear in the previous generated words or not the set of ﬁelds available in a table often impacts the structure of the generation for biographies the ﬁelds used to describe a politician are different from the ones for an actor or an athlete we introduce global conditioning on the available ﬁelds gf as ( s|z gf = ( wt|ct zct gf . 7 similarly global conditioning gw on the available tyt=1 tyt=1 1206 words occurring in the table is introduced tyt=1 ( s|z gf gw = ( wt|ct zct gf gw . 8 tokens provide information complementary to ﬁelds for example it may be hard to distinguish a basketball player from a hockey player by looking only at the ﬁeld names e.g teams league position weight and height etc however the actual ﬁeld tokens such as team names league name player s position can help the model to give a better prediction here gf { , } and gw { , } are binary indicators over ﬁxed ﬁeld and word vocabularies figure illustrates the model with a schematic example for predicting the next word wt after a given context ct the language model is conditioned on sets of triplets for each word occurring in the table zct along with all ﬁelds and words from this table 3.3 copy actions so far we extended the model conditioning with features derived from the fact table we now turn to using table information when scoring output words in particular sentences which express facts from a given table often copy words from the table we therefore extend our model to also score special ﬁeld tokens such as name or name which are subsequently added to the score of the corresponding words from the ﬁeld value our model reads a table and deﬁnes an output domain w∪q q deﬁnes all tokens in the table which might include out of vocabulary words /∈ ) for instance park-rhodes in figure is not in w. however park-rhodes will be included in as name ( since it is the second token of the name ﬁeld which allows our model to generate it this mechanism is inspired by recent work on attention based word copying for neural machine translation luong et al. 2015 as well as delexicalization for neural dialog systems wen et al. 2015 . it also builds upon older work such as class-based language models for dialog systems oh and rudnicky 2000 . a neural language model approach equation ) where refers to all learnable parameters of the network this function is a composition of simple differentiable functions or layers 4.1 mathematical notations and layers we denote matrices as bold upper case letters x y z , and vectors as bold lower-case letters a b c . ai represents the ith row of matrix a. when a is a 3-d matrix then ai j represents the vector of the ith ﬁrst dimension and jth second dimension unless otherwise stated vectors are assumed to be column vectors we use v1 v2 to denote vector concatenation next we introduce the notation for the different layers used in our approach embedding layer given a parameter matrix ∈ rn×d the embedding layer is a lookup table that performs an array indexing operation ψx xi = xi rd ( ) where xi corresponds to the embedding of the element xi at row i. when is a 3-d matrix the lookup table takes two arguments ψx xi xj = xi j rd ( 10 where xi j corresponds to the embedding of the pair xi xj at index i j . the lookup table operation can be applied for a sequence of elements = x1 . . xt a common approach is to concatenate all resulting embeddings ψx s = cid:2 ψx x1 ; . ; ψx xt ( ) rt×d ( 11 formation to its inputs ∈ rn linear layer this layer applies a linear trans γθ x = wx b 12 where = w b are the trainable parameters with ∈ rm×n being the weight matrix and ∈ rm is the bias term softmax layer given a context input ct the ﬁnal layer outputs a score for each word wt w φθ ct ∈ r|w| the probability distribution is obtained by applying the softmax activation function a feed-forward neural language model nlm estimates ( wt|ct with a parametric function φθ ( wt w|ct = exp φθ ct w ) i=1 exp φθ ct wi ) 13 p|w| 1207 4.2 embeddings as inputs a key aspect of neural language models is the use of word embeddings similar words tend to have similar embeddings and thus share latent features the probability estimates of those models are smooth functions of these embeddings and a small change in the features results in a small change in the probability estimates bengio et al. 2003 . therefore neural language models can achieve better generalization for unseen n-grams next we show how we map fact tables to continuous space in similar spirit word embeddings formally the embedding layer maps each context word index to a continuous d-dimensional vector it relies on a parameter matrix ∈ r|w|×d to convert the input ct into − vectors of dimension : ψe ct = cid:2 ψe wt− n−1 ) . . ψe wt−1 ( ) ( 14 e can be initialized randomly or with pre-trained word embeddings table embeddings as described in section 3.2 the language model is conditioned on elements from the table embedding matrices are therefore deﬁned to model both local and global conditioning information for local conditioning we denote the maximum length of a sequence of words as l. each ﬁeld fj f is associated with × vectors of dimensions the ﬁrst of those vectors embed all possible starting positions , . , , and the remaining vectors embed ending positions this results in two parameter matrices = z+ z− ∈ r|f|×l×d for a given triplet fj p+ ) and ψz− fj p−i refer to the embedding vectors of the start and end position for ﬁeld fj respectively i p−i , ψz+ fj p+ finally global conditioning uses two parameter matrices gf r|f|×g and gw r|w|×g ψgf fj maps a table ﬁeld fj into a vector of dimension , while ψgw wt maps a word wt into a vector of the same dimension in general gw shares its parameters with , provided = g. aggregating embeddings we represent each occurence of a word as a triplet ﬁeld start end where we have embeddings for the start and end position as described above often times a particular word occurs multiple times in a table e.g. ‘ lin guistics has two instances in figure 1. in this case we perform a component-wise max over the start embeddings of all instances of to obtain the best features across all occurrences of w. we do the same for end position embeddings ψz zwt = max cid:8 ψz+ fj p+ ) ∀ fj p+ max cid:8 ψz− fj p−i , ( fj p+ , p−i ∈ zwt cid:9 ; , p−i ∈ zwt cid:9 i 15 a special no-ﬁeld embedding is assigned to wt when the word is not associated to any ﬁelds an embedding ψz zct for encoding the local conditioning of the input ct is obtained by concatenation for global conditioning we deﬁne q f as the set of all the ﬁelds in a given table , and as the set of all words in q. we also perform max aggregation this yields the vectors ψgf gf = max cid:8 ψgf fj , ∀fj f ( ) ψgw gw = max cid:8 ψgw wt , ∀wt q cid:9 . the ﬁnal embedding which encodes the context input with conditioning is then the concatenation of these vectors ( 16 ( 17 and ψα1 ct zct gf gw = cid:2 ψe ct ; ψz zct ; ψgf gf ; ψgw gw ( ) rd1 18 with α1 { , z+ z− gf gw and d1 ( − ) ( × ) ( × ) for simpliﬁcation purpose we deﬁne the context input = ct zct gf gw in the following equations this context embedding is mapped to a latent context representation using a linear operation followed by a hyperbolic tangent , 19 h x = tanh cid:16 γα2 cid:0 ψα1 x ( ) cid:17 ∈ rnhu where α2 { w2 b2 , with w2 rnhu×d1 and b2 rnhu 4.3 the hidden representation of the context then goes to another linear layer to produce a real value score for each word in the vocabulary in-vocabulary outputs φwα x = γα3 cid:16 h x ( ) r|w| ( 20 1208 where α3 { w3 b3 , with w3 r|w|×nhu and b3 r|w| and = α1 α2 α3 . 4.4 mixing outputs for better copying section 3.3 explains that each word from the table is also associated with zw the set of ﬁelds in which it occurs along with the position in that ﬁeld similar to local conditioning we represent each ﬁeld and position pair fj pi with an embedding ψf fj pi , where ∈ r|f|×l×d these embeddings are then projected into the same space as the latent representation of context input ( ) rnhu using the max operation over the embedding dimension each word is ﬁnally embedded into a unique vector q w = max cid:8 tanh cid:16 γβ cid:0 ψf fj pi ( ) cid:17 , ( fj pi ∈ zw cid:9 , where = w4 b4 with w4 rnhu×d and b4 rnhu a dot product with the context vector produces a score for each word in the table ( 21 φqβ x w = ( ) q w . 22 each word ∈ ∪ receives a ﬁnal score by summing the vocabulary score and the ﬁeld score φθ x w = φwα x w + φqβ x w , 23 with = α β , and where φqβ x w = when /∈ q. the softmax function then maps the scores to a distribution over ∪ , log ( w|x = φθ x w −log xw0∈w∪q exp φθ x w0 . 4.5 training the neural language model is trained to minimize the negative log-likelihood of a training sentence with stochastic gradient descent sgd lecun et al 2012 : lθ s = log ( wt|ct zct gf gw . 24 txt=1 experiments our neural network model section ) is designed to generate sentences from tables for large-scale problems where a diverse set of sentence types need to be generated biographies are therefore a good framework to evaluate our model with wikipedia offering a large and diverse dataset 5.1 biography dataset we introduce a new dataset for text generation wikibio a corpus of 728,321 articles from english wikipedia sep 2015 . it comprises all biography articles listed by wikiproject biography1 which also have a table infobox . we extract and tokenize the ﬁrst sentence of each article with stanford corenlp manning et al. 2014 . all numbers are mapped to a special token except for years which are mapped to different special token field values from tables are similarly tokenized all tokens are lower-cased table summarizes the dataset statistics on average the ﬁrst sentence is twice as short as the table 26.1 vs 53.1 tokens , about a third of the sentence tokens 9.5 also occur in the table the ﬁnal corpus has been divided into three sub-parts to provide training 80 ) validation 10 ) and test sets 10 ) the dataset is available for download2 5.2 baseline our baseline is an interpolated kneser-ney kn language model and we use the kenlm toolkit to train 5-gram models without pruning heaﬁeld et al. 2013 . we also learn a kn language model over templates for that purpose we replace the words occurring in both the table and the training sentences with a special token reﬂecting its table descriptor zw equation ) the introduction section of the table in figure looks as follows under this scheme “ name name ( birthdate birthdate birthdate – deathdate deathdate deathdate ) was an english linguist fields 10 scientist mathematician mystic and mycologist ” during inference the decoder is constrained to emit words from the regular vocabulary or special tokens occurring in the input table when picking a special token we copy the corresponding word from the table , fields pathologist 5.3 training setup for our neural models we train 11-gram language models n 11 with a learning rate set to 0.0025 1https //en.wikipedia.org/wiki/ wikipedia wikiproject_biography : //github.com/davidgrangier/ wikipedia-biography-dataset 1209 model kn nlm local ﬁeld start end template kn table nlm w/ local ﬁeld start + local ﬁeld start end + global ﬁeld + global ﬁeld word perplexity 10.51 7.46 9.40 +− 0.01 8.61 +− 0.01 4.60 +− 0.01† 4.60 +− 0.01† 4.30 +− 0.01† 4.40 +− 0.02† bleu 2.21 rouge 0.38 nist 0.93 10.7 19.8 1.27 +− 0.26 2.41 +− 0.33 1.41 +− 0.11 4.17 +− 0.54 6.08 +− 0.08 26.0 +− 0.39 26.6 +− 0.42 6.20 +− 0.09 33.4 +− 0.18 7.52 +− 0.03 34.7 +− 0.36 25.8 +− 0.36 7.98 +− 0.07 0.52 +− 0.08 1.48 +− 0.23 19.2 +− 0.23 19.7 +− 0.25 23.9 +− 0.12 5.19 table : bleu rouge nist and perplexity without copy actions ﬁrst three rows and with copy actions last ﬁve rows . for neural models we report mean +− standard deviation for ﬁve training runs with different initialization decoding beam width is 5. perplexities marked with and are not directly comparable as the output vocabularies differ slightly mean percentile % 95 46 13 108 20 19 9 36 26.1 53.1 9.5 19.7 tokens per sentence tokens per table table tokens per sent # ﬁelds per table table : dataset statistics parameter word types ﬁeld types max # tokens in a ﬁeld word/ﬁeld embedding size global embedding size hidden units = = table : model hyperparameters value |w| 20 000 |f| , 740 10 d 64 128 nhu 256 table describes the other hyper-parameters we include all ﬁelds occurring at least 100 times in the training data in , the set of ﬁelds we include the 20 000 most frequent words in the vocabulary the other hyperparameters are set through validation maximizing bleu over a validation subset of , 000 sentences similarly early stopping is applied training ends when bleu stops improving on the same validation subset one should note that the maximum number of tokens in a ﬁeld = 10 means that we encode only 10 positions for longer ﬁeld values the ﬁnal tokens are not dropped but their position is capped to 10. we initialize the word embeddings from hellinger pca computed over the set of training biographies this representation has shown to be helpful for various applications lebret and collobert 2014 . 5.4 evaluation metrics we use different metrics to evaluate our models performance is ﬁrst evaluated in terms of perplexity which is the standard metric for language modeling generation quality is assessed automatically with bleu-4 rouge-4 f-measure and nist43 belz and reiter 2006 . results this section describes our results and discusses the impact of the different conditioning variables 6.1 the more the better the results table ) show that more conditioning information helps to improve the performance of our models the generation metrics bleu rouge and nist all gives the same performance ordering over models we ﬁrst discuss models without copy actions the ﬁrst three results and then discuss models with copy actions the remaining results . note that the factorization of our models results in three different output domains which makes perplexity comparisons less straightforward models without copy actions operate over a ﬁxed vocabulary template kn adds a ﬁxed set of ﬁeld/position pairs to this vocabulary while table nlm models a variable set depending on the input table see section 3.3. without copy actions in terms of perplexity the i neural language model nlm is slightly better 3we rely on standard software nist mteval-v13a.pl for nist bleu , and msr rouge-1.5.5 for rouge . 1210 name birthdate 2 2 birthplace 2 nationality occupation 2 than an interpolated kn language model and ii adding local conditioning on the ﬁeld start and end position further improves accuracy generation metrics are generally very low but there is a clear improvement when using local conditioning since it allows to learn transitions between ﬁelds by linking previous predictions to the table unlike kn or plain nlm with copy actions for experiments with copy actions we use the full local conditioning equation ) in the neural language models bleu rouge and nist all improves when moving from template kn to table nlm and more features successively improve accuracy global conditioning on the ﬁelds improves the model by over bleu and adding words gives an additional 1.3 bleu this is a total improvement of nearly 15 bleu over the template kneser-ney baseline similar observations are made for rouge +15 and nist +2.8 < > nellie wong born september 12 1934 is an american poet and activist figure : visualization of attention scores for nellie wong s wikipedia infobox each row represents the probability distribution over ﬁeld position pairs given the previous words i.e the words heading the preceding rows as well as the current row . darker colors depict higher probabilities figure shows that this mechanism adds a large bias to continue a ﬁeld if it has not generated all tokens from the table e.g. it emits the word occurring in name after generating name 1. it also nicely handles transitions between ﬁeld types e.g. the model adds a large bias to the words occurring in the occupation ﬁeld after emitting the birthdate 6.3 sentence decoding we use a standard beam search to explore a larger set of sentences compared to simple greedy search this allows us to explore times more paths which comes at a linear increase in the number of forward computation steps for our language model we compare various beam settings for the baseline template kn and our table nlm figure ) the best validation bleu can be obtained with a beam size of = 5. our model is also several times faster than the baseline requiring only about 200 ms per sentence with = 5. beam search generates many ngram lookups for kneser-ney which requires many figure : comparison between our best model table nlm and the baseline template kn for different beam sizes the x-axis is the average timing in milliseconds for generating one sentence the y-axis is the bleu score all results are measured on a subset of 1,000 samples of the validation set 6.2 attention mechanism our model implements attention over input table ﬁelds for each word in the table equation 23 takes the language model score φwct and adds a bias φqct the bias is the dot-product between a representation of the table ﬁeld in which occurs and a representation of the context equation 22 that summarizes the previously generated ﬁelds and words 1 u l 5 0 5 0 5 0 5 template kn table nlm beam size 3 45 67 810 l l ll l 15 20 25 l l l 8 10 15 2025 l l l l l l l 100 200 500 1000 2000 time in ms 1211 model reference generated sentence frederick parker-rhodes 21 march 1914 21 november 1987 was an english linguist plant pathologist computer scientist mathematician mystic and mycologist baseline template kn frederick parker-rhodes born november 21 1914 march , 1987 was an english cricketer table nlm +local ﬁeld start frederick parker-rhodes 21 november 1914 2 march 1987 was an australian rules footballer who played with carlton in the victorian football league vﬂ during the xxxxs and xxxxs + global ﬁeld frederick parker-rhodes 21 november 1914 2 march 1987 was an english mycology and plant pathology mathematics at the university of uk + global ﬁeld word table : first sentence from the current wikipedia article about frederick parker-rhodes and the sentences generated from the three versions of our table-conditioned neural language model table nlm using the wikipedia infobox seen in figure 1. frederick parker-rhodes 21 november 1914 2 march 1987 was a british computer scientist best known for his contributions to computational linguistics random memory accesses while neural models perform scoring through matrix-matrix products an operation which is more local and can be performed in a block parallel manner where modern graphic processors shine kindratenko 2014 . 6.4 qualitative analysis table shows generations for different variants of our model based on the wikipedia table in figure 1. first of all comparing the reference to the fact table reveals that our training data is not perfect the birth month mentioned in the fact table and the ﬁrst sentence of the wikipedia article are different this may have been introduced by one contributor editing the article and not keeping the information consistent all three versions of our model correctly generate the beginning of the sentence by copying the name the birth date and the death date from the table the model correctly uses the past tense since the death date in the table indicates that the person has passed away frederick parker-rhodes was a scientist but this occupation is not directly mentioned in the table the model without global conditioning can therefore not predict the right occupation and it continues the generation with the most common occupation in wikipedia for a person who has died in contrast the global conditioning over the ﬁelds helps the model to understand that this person was indeed a scientist however it is only with the global conditioning on the words that the model can infer the correct occupation i.e. computer scientist 7 conclusions we have shown that our model can generate ﬂuent descriptions of arbitrary people based on structured data local and global conditioning improves our model by a large margin and we outperform a kneser-ney language model by nearly 15 bleu our task uses an order of magnitude more data than previous work and has a vocabulary that is three orders of magnitude larger in this paper we have only focused on generating the ﬁrst sentence and we will tackle the generation of longer biographies in future work also the encoding of ﬁeld values can be improved currently we only attach the ﬁeld type and token position to each word type and perform a max-pooling for local conditioning one could leverage a richer representation by learning an encoder conditioned on the ﬁeld type e.g a recurrent encoder or a convolutional encoder with different pooling strategies furthermore the current training loss function does not explicitly penalize the model for generating incorrect facts e.g predicting an incorrect nationality or occupation is currently not considered worse than choosing an incorrect determiner a loss function that could assess factual accuracy would certainly improve sentence generation by avoiding such mistakes also it will be important to deﬁne a strategy for evaluating the factual accuracy of a generation beyond bleu rouge or nist 1212 references g. angeli p. liang and d. klein 2010. a simple domain-independent probabilistic approach to generation in proceedings of the 2010 conference on empirical methods in natural language processing pages 502–512 association for computational linguistics d. bahdanau k. cho and y. bengio 2015. neural machine translation by jointly learning to align and translate in international conference on learning representations r. barzilay and m. lapata 2005. collective content selection for concept-to-text generation in proceedings of the conference on human language technology and empirical methods in natural language processing pages 331–338 r. barzilay and m. lapata 2006. aggregation via set partitioning for natural language generation in proceedings of the main conference on human language technology conference of the north american chapter of the association of computational linguistics pages 359–366 association for computational linguistics a. belz and e. reiter 2006. comparing automatic and human evaluation of nlg systems in in proc eacl06 pages 313–320 a. belz 2008. automatic generation of weather forecast texts using comprehensive probabilistic generationspace models natural language engineering 14 04 :431–455 y. bengio r. ducharme p. vincent and c. jauvin 2003. a neural probabilistic language model journal of machine learning research 3:1137–1155 k. bollacker c. evans p. paritosh t. sturge and j. taylor 2008. freebase a collaboratively created graph database for structuring human knowledge in international conference on management of data pages 1247–1250 acm r. dale s. geldof and j.-p. prost 2003. coral using natural language generation for navigational assistance in proceedings of the 26th australasian computer science conference-volume 16 pages 35–44 australian computer society inc. j. devlin r. zbib z. huang t. lamar r. schwartz and j. makhoul 2014. fast and robust neural network joint models for statistical machine translation in proceedings of the 52nd annual meeting of the association for computational linguistics volume , pages 1370–1380 p. a. duboue and k. r. mckeown 2002. content planner construction via evolutionary algorithms and a corpus-based ﬁtness function in proceedings of inlg 2002 pages 89–96 h. fang s. gupta f. iandola r. k. srivastava l. deng p. dollar j. gao x. he m. mitchell j. c. platt l. c. zitnick and g. zweig 2015. from captions to visual concepts and back in the ieee conference on computer vision and pattern recognition cvpr , june d. ferrucci 2012. introduction to this is watson ibm journal of research and development 56 3.4 :1–1 d. galanis and i. androutsopoulos 2007. generating multilingual descriptions from linguistically annotated owl ontologies in proceedings of the eleventh european workshop on natural language generation pages 143–146 association for computational linguistics the naturalowl system n. green 2006. generation of biomedical arguments for lay readers in proceedings of the fourth international natural language generation conference pages 114– 121. association for computational linguistics b. gyawali and c. gardent 2014. surface realisation from knowledge-bases in proc of acl k. heaﬁeld i. pouzyrevsky j. h. clark and p. koehn 2013. scalable modiﬁed kneser-ney language model estimation in proceedings of the 51st annual meeting of the association for computational linguistics pages 690–696 soﬁa bulgaria august a. karpathy and l. fei-fei 2015. deep visual-semantic alignments for generating image descriptions in the ieee conference on computer vision and pattern recognition cvpr , june j. kim and r. j. mooney 2010. generative alignment and semantic parsing for learning from ambiguous supervision in proceedings of the 23rd international conference on computational linguistics posters pages 543–551 association for computational linguistics v. kindratenko 2014. numerical computations with gpus springer r. kiros r. salakhutdinov and r. s. zemel 2014. unifying visual-semantic embeddings with multimodal neural arxiv preprint arxiv:1411.2539. language models i. konstas and m. lapata for concept-to-text generation 48 1 :305–346 october 2013. a global model j. artif int res. i. langkilde and k. knight 1998. generation that exin proc ploits corpus-based statistical knowledge acl pages 704–710 r. lebret and r. collobert 2014. word embeddings through hellinger pca in proceedings of the 14th conference of the european chapter of the association for computational linguistics pages 482–490 gothenburg sweden april association for computational linguistics y. a lecun l. bottou g. b. orr and k.-r. m¨uller 2012. efﬁcient backprop in neural networks tricks of the trade pages 9–48 springer 1213 o. vinyals a. toshev s. bengio and d. erhan 2015. show and tell a neural image caption generator in the ieee conference on computer vision and pattern recognition cvpr , june 2015. t. wen m. gasic n. mrkˇsi´c p. su d. vandyke and s. young semantically conditioned lstmbased natural language generation for spoken dialogue systems in proceedings of the 2015 conference on empirical methods in natural language processing pages 1711–1721 lisbon portugal september association for computational linguistics y. w. wong and r. j. mooney 2007. generation by inverting a semantic parser that uses statistical machine translation in hlt-naacl pages 172–179 k. xu j. ba r. kiros a. courville r. salakhutdinov r. zemel and y. bengio 2015. show attend and tell neural image caption generation with visual attention in proceedings of the 32nd international conference on machine learning volume 37 july k. yao g. zweig and b. peng 2015. attention with intention for a neural network conversation model arxiv preprint arxiv:1510.08565. p. liang m. i. jordan and d. klein 2009. learning in semantic correspondences with less supervision proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp volume 1-volume , pages 91–99 association for computational linguistics w. lu and h. t. ng 2011. a probabilistic forestto-string model for language generation from typed lambda calculus expressions in proceedings of the conference on empirical methods in natural language processing pages 1611–1622 association for computational linguistics m.-t. luong i. sutskever q. le o. vinyals and w. zaremba 2015. addressing the rare word problem in neural machine translation in proc acl pages 11–19 f. mairesse and m. walker 2011. controlling user perceptions of linguistic style trainable generation of personality traits comput linguist. 37 3 :455–488 c. d. manning m. surdeanu j. bauer j. finkel s. j. bethard and d. mcclosky 2014. the stanford corenlp natural language processing toolkit in association for computational linguistics acl system demonstrations pages 55–60 h. mei m. bansal and m. r. walter 2016. what to talk about and how selective generation using lstms with coarse-to-ﬁne alignment in proceedings of human language technologies the 2016 annual conference of the north american chapter of the association for computational linguistics a. oh and a. rudnicky 2000. stochastic language generation for spoken dialogue systems in anlp/naacl workshop on conversational systems pages 27–32 a. ratnaparkhi 2002. trainable approaches to surface natural language generation and their application to conversational dialog systems computer speech language 16 3 :435–455 e. reiter r. dale and z. feng 2000. building natural language generation systems volume 33. mit press e. reiter s. sripada j. hunter j. yu and i. davy 2005. choosing words in computer-generated weather forecasts artiﬁcial intelligence 167 1 :137–169 l. shang z. lu and h. li 2015. neural responding machine for short-text conversation arxiv preprint arxiv:1503.02364. radu soricut and daniel marcu 2006. stochastic language generation using widl-expressions and its application in machine translation and summarization in proc acl pages 1105–1112 r. turner s. sripada and e. reiter 2010. generating approximate geographic descriptions in empirical methods in natural language generation pages 121– 140. springer twag a topic-guided wikipedia abstract generator fangwei zhu1,2 shangqing tu3 jiaxin shi1,2 juanzi li1,2 lei hou1,2∗ and tong cui4 1dept of computer sci. tech. bnrist tsinghua university beijing 100084 china 2kirc institute for artiﬁcial intelligence tsinghua university 3school of computer science and engineering beihang university zfw19 mails. shijx16 mails lijuanzi , houlei } tsinghua.edu.cn 4noah s ark lab huawei inc. tsq buaa.edu.cn cuitong5 huawei.com abstract wikipedia abstract generation aims to distill a wikipedia abstract from web sources and has met signiﬁcant success by adopting multidocument summarization techniques however previous works generally view the abstract as plain text ignoring the fact that it is a description of a certain entity and can be decomposed into different topics in this paper we propose a two-stage model twag that guides the abstract generation with topical information first we detect the topic of each input paragraph with a classiﬁer trained on existing wikipedia articles to divide input documents into different topics then we predict the topic distribution of each abstract sentence and decode the sentence from topic-aware representations with a pointer-generator network we evaluate our model on the wikicatsum dataset and the results show that twag outperforms various existing baselines and is capable of generating comprehensive abstracts our code and dataset can be accessed at : //github.com/thu-keg/twag introduction wikipedia one of the most popular crowd-sourced online knowledge bases has been widely used as the valuable resources in natural language processing tasks such as knowledge acquisition lehmann et al. 2015 and question answering hewlett et al. 2016 rajpurkar et al. 2016 due to its high quality and wide coverage within a wikipedia article its abstract is the overview of the whole content and thus becomes the most frequently used part in various tasks however the abstract is often contributed by experts which is labor-intensive and prone to be incomplete in this paper we aim to automatically generate wikipedia abstracts based on the related documents corresponding author collected from referred websites or search engines which is essentially a multi-document summarization problem this problem is studied in both extractive and abstractive manners the extractive models attempt to select relevant textual units from input documents and combine them into a summary graph-based representations are widely exploited to capture the most salient textual units and enhance the quality of the ﬁnal summary erkan and radev 2004 mihalcea and tarau 2004 wan 2008 . recently there also emerge neural extractive models yasunaga et al. 2017 yin et al. 2019 utilizing the graph convolutional network kipf and welling 2017 to better capture inter-document relations however these models are not suitable for wikipedia abstract generation the reason is that the input documents collected from various sources are often noisy and lack intrinsic relations sauper and barzilay 2009 , which makes the relation graph hard to build the abstractive models aim to distill an informative and coherent summary via sentence-fusion and paraphrasing filippova and strube 2008 banerjee et al. 2015 bing et al. 2015 , but achieve little success due to the limited scale of datasets liu et al ( 2018 proposes an extractive-then-abstractive model and contributes wikisum a large-scale dataset for wikipedia abstract generation inspiring a branch of further studies perez-beltrachini et al. 2019 liu and lapata 2019 li et al. 2020 . the above models generally view the abstract as plain text ignoring the fact that wikipedia abstracts describe certain entities and the structure of wikipedia articles could help generate comprehensive abstracts we observe that humans tend to describe entities in a certain domain from several topics when writing wikipedia abstracts as illustrated in figure , the abstract of the arctic fox contains its adaption biology taxonomy and geographical distribution which is consistent with 4623 proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing pages 4623–4635 august 1–6 2021 ©2021 association for computational linguistics abstract content table and hemisphere the arctic fox vulpes lagopus , also known as the white fox polar fox or snow fox is a small fox native to the arctic regions of the northern common throughout the arctic tundra biome it is well adapted to living in cold environments and is best known for its thick warm fur that is also used as camouflage it has a large and very fluffy tail in the wild most individuals do not live past their first year but some exceptional ones survive up to 11 years its body length ranges from 46 to 68 cm 18 to 27 in , with a generally rounded body shape to minimize the escape of body heat 2 adaptations 2.1 sensory modalities 2.2 physiology size taxonomy 4.1 origins 4.2 subspecies distribution and habitat 5.1 migrations and travel figure : an example of wikipedia article arctic fox the abstract contains three orthogonal topics about an animal description ( ) cid:2 ( ) cid:2 ( ) cid:2 ( ) cid:2 ( ) taxonomy and distribution the right is part of the article s content table showing section labels related to different topics the content table therefore given an entity in a speciﬁc domain generating abstracts from corresponding topics would reduce redundancy and produce a more complete summary in this paper we try to utilize the topical information of entities within its domain wikipedia categories to improve the quality of the generated abstract we propose a novel two-stage topic-guided wikipedia abstract generation model twag . twag ﬁrst divides input documents by paragraph and assigns a topic for each paragraph with a classiﬁer-based topic detector then it generates the abstract in a sentence-wise manner i.e. predicts the topic distribution of each abstract sentence to determine its topic-aware representation and decodes the sentence with a pointer-generator network see et al. 2017 . we evaluate twag on the wikicatsum perezbeltrachini et al. 2019 dataset a subset of the wikisum containing three distinct domains experimental results show that it signiﬁcantly improves the quality of abstract compared with several strong baselines in conclusion the contributions of our work are as follows • we propose twag a two-stage neural abstractive wikipedia abstract generation model utilizing the topic information in wikipedia which is capable of generating comprehensive abstracts • we simulate the way humans recognize entities using a classiﬁer to divide input documents into topics and then perform topic aware abstract generation upon the predicted topic distribution of each abstract sentence • our experiment results against distinct base lines prove the effectiveness of twag 2 related work 2.1 multi-document summarization multi-document summarization is a classic and problem in natural language processing which aims to distill an informative and coherent summary from a set of input documents compared with single-document summarization the input documents may contain redundant or even contradictory information radev 2000 . early high-quality multi-document summarization datasets are annotated by humans e.g. datasets for document understanding conference duc and text analysis conference tac . these datasets are too small to build neural models and most of the early works take an extractive method attempting to build graphs with interparagraph relations and choose the most salient textual units the graph could be built with various information e.g. tf-idf similarity erkan and radev 2004 , discourse relation mihalcea and tarau 2004 , document-sentence two-layer relations wan 2008 , multi-modal wan and xiao 2009 and query information cai and li 2012 . recently there emerge attempts to incorporate neural models e.g. yasunaga et al ( 2017 builds a discourse graph and represents textual units upon the graph convolutional network gcn ( kipf and welling 2017 , and yin et al ( 2019 adopts the entity linking technique to capture global dependencies between sentences and ranks the sentences with a neural graph-based model in contrast early abstractive models using sentence-fusion and paraphrasing filippova and strube 2008 banerjee et al. 2015 bing et al. 2015 achieve less success inspired by the recent success of single-document abstractive models see et al. 2017 paulus et al. 2018 gehrmann et al. 2018 huang et al. 2020 , some works liu et al. 2018 zhang et al. 2018 try to transfer single-document models to multi-document settings to alleviate the limitations of small-scale datasets speciﬁcally liu et al ( 2018 deﬁnes wikipedia generation problem and contributes the large-scale wikisum dataset fabbri et al ( 2019 constructs a middle-scale dataset named multi 4624 news and proposes an extractive-then-abstractive model by appending a sequence-to-sequence model after the extractive step li et al ( 2020 models inter-document relations with explicit graph representations and incorporates pre-trained language models to better handle long input documents 2.2 wikipedia-related text generation sauper and barzilay 2009 is the ﬁrst work focusing on wikipedia generation which uses integer linear programming ilp to select the useful sentences for wikipedia abstracts banerjee and mitra 2016 further evaluates the coherence of selected sentences to improve the linguistic quality liu et al ( 2018 proposes a two-stage extractivethen-abstractive model which ﬁrst picks paragraphs according to tf-idf weights from web sources then generates the summary with a transformer model by viewing the input as a long ﬂat sequence inspired by this work perez-beltrachini et al ( 2019 uses a convolutional encoder and a hierarchical decoder and utilizes the latent dirichlet allocation model lda to render the decoder topic-aware hiersumm liu and lapata 2019 adopts a learning-based model for the extractive stage and computes the attention between paragraphs to model the dependencies across multiple paragraphs however these works view wikipedia abstracts as plain text and do not explore the underlying topical information in wikipedia articles there are also works that focus on generating other aspects of wikipedia text biadsy et al ( 2008 utilizes the key-value pairs in wikipedia infoboxes to generate high-quality biographies hayashi et al ( 2021 investigates the structure of wikipedia and builds an aspect-based summarization dataset by manually labeling aspects and identifying the aspect of input paragraphs with a ﬁnetuned roberta model liu et al. 2019 . our model also utilizes the structure of wikipedia but we generate the compact abstract rather than individual aspects which requires the fusion of aspects and poses a greater to understand the connection and difference among topics 3 problem deﬁnition deﬁnition wikipedia abstract generation accepts a set of paragraphs1 = d1 d2 . . dn 1the input documents can be represented by textual units with different granularity and we choose paragraph as it normally expresses relatively complete and compact semantics of size as input and outputs a wikipedia abstract = s1 s2 . . sm with sentences the goal is to ﬁnd an optimal abstract s∗ that best concludes the input i.e. s∗ ( s|d = arg maxs 1 previous works generally view as plain text ignoring the semantics in wikipedia articles before introducing our idea let s review how wikipedia organizes articles wikipedia employs a hierarchical open category system to organize millions of articles and we name the top-level category as domain as for a wikipedia article we concern three parts i.e. the abstract the content table and textual contents note that the content table is composed of several section labels l , pairing with corresponding textual contents p . as illustrated in figure , the content table indicates different aspects we call them topics of the article and the abstract semantically corresponds to these topics telling us that topics could beneﬁt the abstract generation however general domains like person or animal consist millions of articles with diverse content tables making it not feasible to simply treat section labels as topics considering that articles in speciﬁc domains often share several salient topics we manually merge similar section labels to convert the sections titles to a set of topics formally the topic set is denoted as = t1 t2 ... tnt of size nt where each topic ti { l1 . now our task can be expressed with a topical objective i.e. deﬁnition given the input paragraphs , we introduce the latent topics = z1 z2 . . zn , where zi t is the topic of i-th input paragraph di and our objective of wikipedia abstract generation is re-written as . . lm , l2 i s∗ arg maxz ( z|d arg maxs ( s|d z . 2 therefore the abstract generation could be completed with two sub-tasks i.e. topic detection to optimize arg maxz ( z|d and topic-aware abstract generation to optimize arg maxs ( s|d z . the proposed method as shown in figure , our proposed twag adopts a two-stage structure first we train a topic detector based on existing wikipedia articles to predict the topic of input paragraphs second we group the 4625 cid:1853 ( ) cid:1859 ( ) cid:1853 ( ) cid:2285 ( ) cid:4666 ( ) cid:513 ( ) cid:481 ( ) cid:4667 ( ) cid:2869 ( ) cid:4668 ( ) cid:2869 ( ) cid:485 ( ) arkive species partulina snail partulina semicarinata … topic encoder cid:1853 ( ) cid:1859 ( ) cid:1853 ( ) cid:2292 ( ) cid:4666 ( ) cid:513 ( ) cid:4667 1 0 ······ 2 cid:1833 ( ) cid:3404 ( ) cid:1856 ( ) cid:481 ( ) cid:1856 ( ) cid:2879 ( ) cid:4669 with a broader contour strong keel and dextral coil argmax fc+softmax ······ ······ ······ albert cid:1856 ( ) cid:1856 ( ) cid:1856 ( ) cid:1856 ( ) cid:2879 ( ) cid:1856 ( ) cid:2189 ( ) topic predictor step cid:1872 ) cid:2190 ( ) gru sentence decoder cid:1871 ( ) ( ) cid:3047 ( ) cid:3047 ( step cid:1872 - cid:883 ) cid:2190 ( ) cid:2879 ( ) cid:2187 ( ) cid:2879 ( ) cid:1871 ( ) cid:2879 ( ) cid:1833 ( ) cid:3404 ( ) cid:1856 ( ) cid:481 ( ) cid:481 ( ) cid:3041 ( ) endemic to the remaining wet forests on the hawaiian island of lanai ( ) cid:2870 ( ) cid:3047 ( ) cid:2869 ( ) cid:3047 ( ) cid:2869 ( step cid:1872 + cid:883 ) cid:1871 ( ) cid:2878 ( ) cid:2187 ( ) ······ cid:2189 ( ) topic selector start this species is endemic to hawaii figure : the twag framework we use an example domain with topics for illustration the left is the topic detector which attempts to ﬁnd a topic for each input paragraph and the right is the topic-aware abstract generator to generate the abstract by sentence based on input paragraphs and their predicted topics input paragraphs by detected topics to encode them separately and generate the abstract in a sentencewise manner in each step we predict the topic distribution of the current sentence fuse it with the global hidden state to get the topic-aware representation and generate the sentence with a copy-based decoder next we will detail each module 4.1 topic detection the topic detector aims to annotate input paragraphs with their optimal corresponding topics to formalize given the input paragraphs , det returns its corresponding topics = z1 z2 . . zn , i.e. z det d we view topic detection as a classiﬁcation problem for each paragraph ∈ , we encode it with albert lan et al. 2019 and then predict its topic with a fully-connected layer i.e. d albert d z arg max linear d ) where is the vector representation of , and we ﬁne-tuned the albert model on a pretrained version 4.2 topic-aware abstract generation topic-aware abstract generator utilizes the input paragraphs and the detected topics to generate 3 ( ) 5 the abstract speciﬁcally it contains three modules a topic encoder to encode the input paragraphs into topical representations a topic predictor to predict the topic distribution of abstract sentences and generate the topic-aware sentence representation and a sentence decoder to generate abstract sentences based on the topic-aware representations 4.2.1 topic encoder given the input paragraphs and the detected topics , we concatenate all paragraphs belonging to the same topic tk to form a topic-speciﬁc text group ttg gk which contains salient information about a certain topic of an entity gk concat { di|zi tk ) ( ) to further capture hidden semantics we use a bidirectional gru to encode the ttgs gk uk bigru gk . 7 gk is the ﬁnal hidden state of the gk and uk ( u1 u2 . . ungk represents the hidden state of each token in gk where ngk denotes the number of tokens in gk 4.2.2 topic predictor after encoding the topics into hidden states twag tackles the decoding process in a sentence-wise manner p s|d z = arg maxs ( ) i=1 arg max si ( si|d z s i ( ) 4626 to generate the abstract , we ﬁrst predict the topic distribution of every sentence si with a gru decoder at each time step , the topic predictor produces a global hidden state ht and then estimates the probability distribution qt over topics ht gru ht−1 et−1 qt softmax linear ht ) 9 ( 10 where et−1 denotes the topical information in the last step e0 is initialized as an all-zero vector and et could be derived from qt in two ways the ﬁrst way named hard topic is to directly select the topic with the highest probability and take its corresponding representation i.e. ehard = garg maxi qi . 11 the second way named soft topic is to view every sentence as a mixture of different topics and take the weighted sum over topic representations i.e. for the k-th token the decoder computes an attention distribution ak over tokens in the input paragraphs where each element ai could be viewed as the probability of the i-th token being selected ai = softmax tanh wuui +wssk ba ) 16 where sk denotes the decoder hidden state with s0 rt to incorporate the topic-aware representation and wu ws ba are trainable parameters to generate a token from the vocabulary we ﬁrst use the attention mechanism to calculate the weighted sum of encoder hidden states known as the context vector ( ) ∗ = ai kui ( 17 which is further fed into a two-layer network to obtain the probability distribution over vocabulary t qt g esof ( 12 pvoc softmax linear linear [ sk c k ) ) ( 18 where = g1 g2 . . gnt is the matrix of topic representations with the observation that wikipedia abstract sentences normally contain mixed topics we choose the soft topic mechanism for our model see section 5.3 for details . finally we compute the topic-aware hidden state rt by adding up ht and et which serves as the initial hidden state of sentence decoder rt ht et 13 additionally a stop conﬁrmation is executed at each time step pstop σ linear ht ) 14 where represents the sigmoid function if pstop 0.5 twag will terminate the decoding process and no more abstract sentences will be generated 4.2.3 sentence decoder our sentence decoder adopts the pointer-generator network see et al. 2017 , which picks tokens both from input paragraphs and vocabulary to copy a token from the input paragraphs the decoder requires the token-wise hidden states = u1 u2 . . unu of all nu input tokens which is obtained by concatenating the token-wise hidden states of all ttgs i.e. to switch between these two mechanisms pgen k decoder hid is computed from context vector den state sk and decoder input xk pgen σ wt k wt c sk wt xk bp ( 19 where represents the sigmoid function and and bp are trainable parameters wt the ﬁnal probability distribution of words is2 wt , wt ( ) pgenpvoc w + 1 pgen ( ) : |wwi=w ai ( 20 4.3 training the modules for topic detection and abstract generation are trained separately 4.3.1 topic detector training since there are no public benchmarks for assigning input paragraphs with wikipedia topics we construct the dataset with existing wikipedia articles in each domain we collect all the label-content pairs ( , ) ( deﬁned in section ) and split the content into paragraphs = d1 d2 . . dnp to form a set of label-paragraph pairs ( , ) . afterwards we choose all pairs l d whose section label belongs to a particular topic ∈ to complete the dataset construction i.e. the topicparagraph set ( , ) . besides a noise topic is = u1 u2 . . unu ( 15 2wwi means the token corresponding to ui 4627 set up in each domain which refers to meaningless text like scripts and advertisements and the corresponding paragraphs are obtained by utilizing regular expressions to match obvious noisy texts the details are reported in appendix a. note that the dataset for abstract generation is collected from non-wikipedia websites refer to section for details . these two datasets are independent of each other which prevents potential data leakage in the training step we use the negative log likelihood loss to optimize the topic detector 4.3.2 abstract generator training the loss of topic-aware abstract generation step consists of two parts the ﬁrst part is the average loss of sentence decoder for each abstract sentence lsent and the second part is the cross-entropy loss of stop conﬁrmation lstop following see et al. 2017 , we compute the loss of an abstract sentence by averaging the negative log likelihood of every target word in that sentence and achieve lsent via averaging over all sentences ( ) lsent 1 − log ( wi ( 21 ( ) ( ) nst cid:2 1 nst t=1 i=1 where nst is the length of the t-th sentence of the abstract as for lstop we adopt the cross-entropy loss i.e. lstop −ys log pstop − 1 ys log 1 pstop ( 22 where ys 1 when > and ys 0 otherwise 5 experiments 5.1 experimental settings dataset to evaluate the overall performance of our model we use the wikicatsum dataset proposed by perez-beltrachini et al. 2019 , which contains three distinct domains company film and animal in wikipedia each domain is split into train 90 ) validation 5 ) and test 5 ) set we build the dataset for training and evaluating the topic detector from the 2019-07-01 english wikipedia full dump for each record in the wikicatsum dataset we ﬁnd the article with the same title in wikipedia dump and pick all section labelcontent pairs ( , ) in that article we remove all hyperlinks and graphics in contents split the contents into paragraphs with the spacy library and follow the steps in section 4.3.1 to complete dataset construction finally we conduct an 8:1:1 split for train validation and test table presents the detailed parameters of used datasets evaluation metrics we evaluate the performance of our model with rouge scores lin 2004 , which is a common metric in comparing generated and standard summaries considering that we do not constrain the length of generated abstracts we choose rouge f1 score that combines precision and recall to eliminate the tendency of favoring long or short results implementation details we use the opensource pytorch and transformers library to implement our model all models are trained on nvidia geforce rtx 2080. in topic detection we choose the top 20 frequent section labels in each domain and manually group them into different topics refer to the appendix a for details . for training we use the pretrained albert-base-v2 model in the transformers library keep its default parameters and train the module for epochs with a learning rate of 3e-5 for abstract generation we use a single-layer bigru network to encode the ttgs into hidden states of 512 dimensions the ﬁrst 400 tokens of input paragraphs are retained and transformed into glove pennington et al. 2014 embedding of 300 dimensions the vocabulary size is 50000 and out-of-vocabulary tokens are represented with the average embedding of its adjacent 10 tokens this module is trained for 10 epochs the learning rate is 1e-4 for the ﬁrst epoch and 1e-5 for the rest before evaluation we remove sentences that have an overlap of over 50 with other sentences to reduce redundancy baselines we compare our proposed twag with the following strong baselines • tf-s2s liu et al. 2018 uses a transformer decoder and compresses key-value pairs in self-attention with a convolutional layer • cv-s2d+t perez-beltrachini et al. 2019 uses a convolutional encoder and a two-layer hierarchical decoder and introduces lda to model topical information • hiersumm liu and lapata 2019 utilizes inter the attention mechanism to model 4628 domain company film animal examples r1-r r2-r rl-r .438 .456 .455 62,545 59,973 60,816 .551 .559 .541 .217 .243 .208 topics 5 train 35,506 187,221 51,009 valid 1,999 10,801 2,897 test 2,212 10,085 2,876 table : details about used datasets the left shows parameters about the wikicatsum dataset number of examples and rouge , , recalls the right shows parameters about the dataset for topic detector number of topics and number of topic-paragraph pairs in each split model tf-s2s cv-s2d+t hiersumm bart twag ours company r1 .197 .275 .133 .310 .341 r2 .023 .106 .028 .116 .119 rl .125 .214 .107 .244 .316 film r2 .065 .212 .126 .199 .212 rl .172 .323 .185 .325 .343 animal r2 .099 .279 .069 .226 .244 rl .210 .379 .134 .335 .409 r1 .252 .427 .165 .376 .431 r1 .198 .380 .246 .375 .408 table : rouge f1 scores of different models paragraph relations and then enhances the document representation with graphs ﬁtting due to the dataset scale this phenomenon also proves that twag is data-efﬁcient • bart lewis et al. 2020 is a pretrained sequence-to-sequence model that achieved success on various sequence prediction tasks we ﬁne-tune the pretrained bart-base model on our dataset and set beam size to for all models using beam search at test time the parameters we use for training and evaluation are identical to these in corresponding papers 5.2 results and analysis table shows the rouge f1 scores of different models in all three domains twag outperforms other baselines our model surpasses other models on rouge-1 score by a margin of about 10 , while still retaining advantage on rouge-2 and rouge-l scores in domain company our model boosts the rouge-l f1 score by about 30 , considering that rouge-l score is computed upon the longest common sequence the highest rouge-l score indicates that abstracts generated by twag have the highest holistic quality while cvs2d+t and bart retain reasonable scores tf-s2s and hiersumm do not reach the scores they claim in their papers notice that the wikicatsum dataset is a subset of wikisum which is used as the training dataset of these two models we infer that tf-s2s and hiersumm require more training data to converge and suffer from under 5.3 ablation study learning rate of topic detector we tried two learning rates when training the topic detector module a learning rate of 1e-7 would result in a precision of 0.922 in evaluation while a learning rate of 3e-5 would result in a precision of 0.778. however choosing the former learning rate causes a drop of about 10 in all rouge scores which is the reason why we use the latter one in our full model we infer that human authors occasionally make mistakes assigning paragraphs into section labels that belong to other topics a topic detector with low learning rate overﬁts these mistakes harming the overall performance of our model soft or hard topic to further investigate the effectiveness of twag s soft topic mechanism we compare the results of soft and hard topic and report them in table , from which we can see that hard topic does quite poorly in this task topic detector company film animal hard topic soft topic r1 .266 .355 .407 r2 .074 .159 .223 rl .245 .333 .387 r1 .341 .408 .431 r2 .119 .212 .244 rl .316 .343 .409 table : rouge f1 scores of different topic selectors a possible reason is that some sentences in the 4629 gold abstract manjina there kannada ( ) unk cid:3 ( ) unk cid:3 ) is a 1980 indian kannada ﬁlm directed by bangalore nagesh and produced by k. r. narayana murthy the ﬁlm stars srinath manjula thoogudeepa srinivas and dinesh in lead roles the ﬁlm had musical score by upendra kumar tf-s2s chalo ishq larain is a pakistani ﬁlm directed by sajjad gul and was released across pakistan in april 2002 starring meera ali haider and zara sheikh cv-s2d+t ( ) unk cid:3 ( ) unk cid:3 ( kannada ( ) unk cid:3 ( ) unk cid:3 ) is a 1967 indian kannada ﬁlm directed by b. mallesh and produced by b. s. ranga the ﬁlm stars rajkumar udaykumar narasimharaju and k. s. ashwath in lead roles the ﬁlm had musical score by chellapilla satyam the ﬁlm was a remake of hindi ﬁlm cid:2 unk cid:3 . hiersumm ( kannada ( ) unk cid:3 ( ) unk cid:3 ) is a 1980 indian kannada ﬁlm directed by bangalore nagesh the ﬁlm stars srinath manjulla thoogudeepa shreenivaas the ﬁlm stars srinath manjula manjula thoogudeepa shreenivaas in lead roles the ﬁlm had musical score by upendra kumar the ﬁlm is a remake of telugu movie aakashagiri rao the movie was remade in telugu as manjina in 1986 the movie was remade in telugu as manjina there . bart manjina there is a 1980 kannada family drama ﬁlm directed by bangalore nagesh starring srinath and manjula in the lead roles it was released on 14 january 1980. twag manjina there is a 1980 kannada drama ﬁlm directed by bangalore nagesh the ﬁlm stars srinath vajramuni manjula and thoogudeepa srinivas in lead roles the ﬁlm had musical score by upendra kumar and the ﬁlm opened to positive reviews in 1980. the ﬁlm was a remake of tamil ﬁlm cid:2 unk cid:3 . table : comparison between wikipedia abstracts generated by different models about the ﬁlm majina there non-english characters have been replaced with cid:3 unk cid:4 for readability standard abstract express more than one topic assigning one topic to each sentence will result in semantic loss and thus harm the quality of generated abstract while the soft topic could better simulate the human writing style number of section labels the number of section labels nt plays a key role in our model a small nt would not be informative enough to build topics while a large one would induce noise we can see from figure that the frequency of section labels is long-tailed thus retaining only a small portion is able to capture the major part of information ta figure : the frequency of section labels in three domains when ignoring section labels with extra high or low frequency remaining section labels frequency and rank generally form a straight line in log scale which matches the zipf s law for long-tail distributions ble records the experiment results we conducted on domain company nt 20 reaches a peak on rouge , and scores indicating that 20 is a reasonable number of section labels 5.4 case study table shows the generated wikipedia abstracts by different models about ﬁlm majina there we labels 10 20 30 r1 .337 .340 .336 r2 .117 .118 .117 rl .312 .315 .311 table : rouge f1 scores of different nt can see that the gold abstract contains information about three topics basic information region director and producer , actors and music among the models tf-s2s produces an abstract with a proper pattern but contains wrong information and bart misses the musical information topic cv-s2d+t hiersumm and our twag model both cover all three topics in the gold abstract however cv-s2d+t makes several factual errors like the release date and actors and hiersumm suffers from redundancy twag covers all three topics in the gold abstract and discovers extra facts proving itself to be competent in generating comprehensive abstracts 5.5 human evaluation we follow the experimental setup of perezbeltrachini et al. 2019 and conduct a human evaluation consisting of two parts a total of 45 examples 15 from each domain are randomly selected from the test set for evaluation the ﬁrst part is a question-answering qa scheme proposed in clarke and lapata 2010 in order to examine factoid information in summaries we create 2-5 questions3 based on the golden sum 3example questions are listed in the appendix , and the whole evaluation set is included in the our code repository 4630 model tf-s2s cv-s2d+t hiersumm bart twag ours company score non-0 .694 .075 .660 .237 .255 .896 .813 .591 .665 .903 film score non-0 .000 .000 .143 .040 .213 .327 .796 .452 .669 .918 animal score non-0 .000 .000 .576 .382 .000 .000 .653 .342 .543 .868 table : human evaluation results in qa scheme score represents the mean score and non-0 represents the percentage of answered questions model tf-s2s cv-s2d+t hiersumm bart twag ours company 2.69 2.42 2.96 2.64 2.91 2.71 2.36 2.64 2.82 2.87 2.67 2.73 1.69 3.00 2.91 1.93 2.29 3.13 2.87 3.20 film 2.71 2.69 2.78 3.02 3.16 2.84 2.98 2.04 3.24 3.44 animal 2.96 3.18 3.13 3.11 3.58 2.76 3.18 1.82 3.00 3.40 2.22 2.80 2.80 2.78 3.56 table : human evaluation results in linguistic quality scoring c indicates completeness f indicates ﬂuency and indicates succinctness mary which covers the appeared topics and invite participants to answer the questions by taking automatically-generated summaries as background information the more questions a summary can answer the better it is to quantify the results we assign a score of 1/0.5/0.1/0 to a correct answer a partially correct answer a wrong answer and those can not be answered and report the average score over all questions notice that we give a score of 0.1 even if the participants answer the question incorrectly because a wrong answer indicates the summary covers a certain topic and is superior to missing information results in table shows that ) taking summaries generated by twag is capable of answering more questions and giving the correct answer 2 tf-s2s and hiersumm perform poorly in domain film and animal which is possibly a consequence of under-ﬁtting in small datasets the second part is an evaluation over linguistic quality we ask the participants to read different generated summaries from perspectives and give a score of 1-5 larger scores indicates higher quality : completeness does the summary contain sufﬁcient information ) fluency is the summary ﬂuent and grammatical ) and succinctness does the summary avoid redundant sentences ) speciﬁcally 3 participants are assigned to evaluate each model and the average scores are taken as the nal results table presents the comparison results from which we can see that the linguistic quality of twag model outperforms other baseline models validating its effectiveness 6 conclusion in this paper we propose a novel topic-guided abstractive summarization model twag for generating wikipedia abstracts it investigates the section labels of wikipedia dividing the input document into different topics to improve the quality of generated abstract this approach simulates the way how human recognize entities and experimental results show that our model obviously outperforms existing state-of-the-art models which view wikipedia abstracts as plain text our model also demonstrates its high data efﬁciency in the future we will try to incorporate pretrained language models into the topic-aware abstract generator module and apply the topic-aware model to other texts rich in topical information like sports match reports acknowledgments we thank the anonymous reviewers for their insightful comments this work is supported by the national key research and development program of china 2017yfb1002101 , nsfc key project u1736204 and a grant from huawei inc. 4631 ethical considerations twag could be applied to applications like automatically writing new wikipedia abstracts or other texts rich in topical information it can also help human writers to examine whether they have missed information about certain important topics the beneﬁts of using our model include saving human writers labor and making abstracts more comprehensive there are also important considerations when using our model input texts may violate copyrights when inadequately collected and misleading texts may lead to factual mistakes in generated abstracts to mitigate the risks researches on how to avoid copyright issues when collecting documents from the internet would help references siddhartha banerjee and prasenjit mitra 2016. wikiwrite generating wikipedia articles automatically in proceedings of the 25th international joint conference on artiﬁcial intelligence pages 2740–2746 siddhartha banerjee prasenjit mitra and kazunari sugiyama 2015. multi-document abstractive summarization using ilp based multi-sentence compression in proceedings of the 24th international conference on artiﬁcial intelligence pages 1208–1214 fadi biadsy julia hirschberg and elena filatova 2008. an unsupervised approach to biography production using wikipedia in proceedings of acl-08 hlt pages 807–815 lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau 2015. abstractive multidocument summarization via phrase selection and merging in proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing pages 1587–1597 xiaoyan cai and wenjie li 2012. mutually reinforced manifold-ranking based relevance propagation model for query-focused multi-document summarization ieee transactions on audio speech and language processing 20 5 :1597–1607 james clarke and mirella lapata 2010. discourse constraints for document compression computational linguistics 36 3 :411–441 g¨unes erkan and dragomir radev 2004. lexrank graph-based lexical centrality as salience in text summarization journal of artiﬁcial intelligence research 22:457–479 alexander richard fabbri irene li tianwei she suyi li and dragomir radev 2019. multi-news a largescale multi-document summarization dataset and abstractive hierarchical model in proceedings of the 57th annual meeting of the association for computational linguistics pages 1074–1084 katja filippova and michael strube 2008. sentence fusion via dependency graph compression in proceedings of the 2008 conference on empirical methods in natural language processing pages 177– 185. sebastian gehrmann yuntian deng and alexander rush 2018. bottom-up abstractive summarization in proceedings of the 2018 conference on empirical methods in natural language processing pages 4098–4109 hiroaki hayashi prashant budania peng wang chris ackerson raj neervannan and graham neubig 2021. wikiasp a dataset for multi-domain aspectbased summarization transactions of the association for computational linguistics 9:211–225 daniel hewlett alexandre lacoste llion jones illia polosukhin andrew fandrianto jay han matthew kelcey and david berthelot 2016. wikireading a novel large-scale language understanding task over wikipedia in proceedings of the 54th annual meeting of the association for computational linguistics pages 1535–1545 luyang huang lingfei wu and lu wang 2020. knowledge graph-augmented abstractive summarization with semantic-driven cloze reward in proceedings of the 58th annual meeting of the association for computational linguistics pages 5094– 5107. thomas n. kipf and max welling 2017. semisupervised classiﬁcation with graph convolutional networks in proceedings of the 5th international conference on learning representations zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma and radu soricut 2019. albert a lite bert for self-supervised learning of language representations in international conference on learning representations jens lehmann robert isele max jakob anja jentzsch dimitris kontokostas pablo mendes sebastian hellmann mohamed morsey patrick van kleef s¨oren auer et al 2015. dbpedia–a large-scale multilingual knowledge base extracted from wikipedia semantic web 6 2 :167–195 mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer 2020. bart denoising sequence-to-sequence pretraining for natural language generation translation and comprehension in proceedings of the 58th annual meeting of the association for computational linguistics pages 7871–7880 wei li xinyan xiao jiachen liu hua wu haifeng wang and junping du 2020. leveraging graph 4632 to improve abstractive multi-document summarization in proceedings of the 58th annual meeting of the association for computational linguistics pages 6232–6243 chin-yew lin 2004. rouge a package for automatic in text summarization evaluation of summaries branches out pages 74–81 peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer 2018. generating wikipedia by summarizing long sequences in international conference on learning representations yang liu and mirella lapata 2019. hierarchical transformers for multi-document summarization in proceedings of the 57th annual meeting of the association for computational linguistics pages 5070– 5081. yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov 2019. roberta a robustly optimized bert pretraining approach arxiv pages arxiv–1907 rada and paul tarau 2004. textrank bringing order into text in proceedings of the 2004 conference on empirical methods in natural language processing pages 404–411 romain paulus caiming xiong and richard socher 2018. a deep reinforced model for abstractive summarization in international conference on learning representations jeffrey pennington richard socher and christopher manning 2014. glove global vectors for word representation in proceedings of the 2014 conference on empirical methods in natural language processing , pages 1532–1543 laura perez-beltrachini yang liu and mirella lapata 2019. generating summaries with topic templates and structured convolutional decoders in proceedings of the 57th annual meeting of the association for computational linguistics pages 5107–5116 dragomir radev 2000. a common theory of information fusion from multiple text sources step one cross-document structure in 1st sigdial workshop on discourse and dialogue pages 74–83 pranav rajpurkar jian zhang konstantin lopyrev and percy liang 2016. squad 100,000+ questions for machine comprehension of text in proceedings of the 2016 conference on empirical methods in natural language processing pages 2383–2392 abigail see peter liu and christopher manning 2017. get to the point summarization with pointergenerator networks in proceedings of the 55th annual meeting of the association for computational linguistics pages 1073–1083 xiaojun wan 2008. an exploration of document impact on graph-based multi-document summarization in proceedings of the 2008 conference on empirical methods in natural language processing pages 755– 762. xiaojun wan and jianguo xiao 2009. graph-based multi-modality learning for topic-focused multidocument summarization in proceedings of the 21st international jont conference on artiﬁcal intelligence pages 1586–1591 michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev 2017. graph-based neural multi-document summarization in proceedings of the 21st conference on computational natural language learning pages 452–462 yongjing yin linfeng song jinsong su jiali zeng chulun zhou and jiebo luo 2019. graph-based neural sentence ordering in proceedings of the 28th international joint conference on artiﬁcial intelligence pages 5387–5393 jianmin zhang jiwei tan and xiaojun wan 2018. towards a neural network approach to abstractive multi-document summarization arxiv preprint arxiv:1804.09010. a topic allocation for each domain we sort section labels by frequency and choose the top nt 20 frequent section labels then manually allocate them into different topics section labels with little semantic information like reference and notes are discarded in allocation to reduce noise table shows how we allocate section labels into topics in domain company film and animal an additional noise topic is added to each domain to detect website noises we build training records for noise by ﬁnding noise text in the training set of wikicatsum by regular expressions for example we view all text containing cookie , href or text that seems to be a reference as noise b trivia about baselines christina sauper and regina barzilay 2009. automatically generating wikipedia articles a structureaware approach in proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp pages 208–216 we use bart-base as the baseline for comparison because bart-large performs poorly in experiments bart-large starts generating redundant results when using only % training data and its training loss also decreases much slower than 4633 bart-base we infer that bart-large may overﬁt on training data and bart-base is more competent to be the baseline c human evaluation example table shows an example of gold summary its corresponding question set and system outputs the full dataset we used for human evaluation can be found in our code repository 4634 domain company film animal topic history product location reception cast plot production reception box ofﬁce distribution taxonomy description section labels history company history ownership products services destinations products and services technology fleet operations subsidiaries locations awards controversies controversy criticism accidents and , reception cast casting plot synopsis plot summary production filming development reception critical reception critical response awards accolades awards and nominations box ofﬁce distribution taxonomy species subspecies classiﬁcation description habitat ecology behaviour biology diet feeding breeding reproduction life cycle conservation status status conservation conservation status table : detailed allocation of section labels in domain company orange labels are labels selected when nt 10 green labels are additional labels selected when nt 20 and blue labels are additional labels selected when nt 30. gold abstract l avare is a 1980 french comedy ﬁlm written and directed by louis de fun es and jean girault and starring de fun es the english title of the ﬁlm is the miser it is an adaptation of moli ere s famous comedy ’ avare ou ’ ´ecole du mensonge the miser . de fun es tried to draw out the unhappy side of the character harpagon unloved by humanity is driven to an obsessive love of money questions 1. when and where was ’ avare released 2. who stars in ’ avare 3. who directed ’ avare 4. what is the english name of ’ avare tf-s2s der er et yndigt land is a 1983 danish drama ﬁlm directed by morten arnfred it was entered into the 33rd berlin international ﬁlm festival where it won an honourable mention cv-s2d+t & lt unk gt ’ & lt unk gt is a french comedy ﬁlm from 1954 directed by jean girault starring jean marais and louis de fun es it was screened in the un certain regard section at the 2015 cannes ﬁlm festival hiersumm ( hangul rr l am is a french drama ﬁlm directed by louis de fun es it is based on a play by moli ere it stars louis de fun es it was entered into the 36th berlin international ﬁlm festival the ﬁlm was nominated for the golden globe award for best foreign language ﬁlm it was also nominated for the golden globe award for best foreign language ﬁlm . . bart l avare english the miser is a 1980 french drama ﬁlm directed by louis de fun es and starring jean girault it was based on an original screenplay co-written with julien franc¸ois twag the miser french l avare is a 1980 french drama ﬁlm directed by funes de fun es the ﬁlm stars louis de fun es sanctioning cl´eante and broach harpagon table : example of gold summary question set and system outputs for the qa evaluation study 4635 the thirty-second aaai conference on artificial intelligence aaai-18 a knowledge-grounded neural conversation model marjan ghazvininejad,1∗ chris brockett,2 ming-wei chang,2† bill dolan,2 jianfeng gao,2 wen-tau yih,2‡ michel galley2 1information sciences institute usc 2microsoft ghazvini isi.edu mgalley microsoft.com abstract neural network models are capable of generating extremely natural sounding conversational interactions however these models have been mostly applied to casual scenarios e.g. as chatbots ) and have yet to demonstrate they can serve in more useful conversational applications this paper presents a novel fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentful responses we generalize the widely-used sequence-tosequence seq2seq approach by conditioning responses on both conversation history and external facts , allowing the model to be versatile and applicable in an open-domain setting our approach yields signiﬁcant improvements over a competitive seq2seq baseline human judges found that our outputs are signiﬁcantly more informative introduction recent work has shown that conversational chatbot models can be trained in an end-to-end and completely datadriven fashion without hand-coding ritter cherry and dolan 2011 sordoni et al 2015 shang lu and li 2015 vinyals and le 2015 serban et al 2016 inter alia . however fully data-driven systems still lack grounding in the real world and do not have access to external knowledge textual or structured , which makes it for such systems to respond substantively fig 1 illustrates the difﬁculty while an ideal response would directly reﬂect on the entities mentioned in the query user input , neural models produce responses that while conversationally appropriate seldom include factual content this contrasts with traditional dialog systems which can readily inject entities and facts into responses but often at the cost of signiﬁcant handcoding slot-ﬁller dialog systems are hard put to come up with a natural sounding utterance like the second response in fig 1 in a manner that is generalizable and scalable the goal of this work is to beneﬁt from the versatility and scalability of fully data-driven models while simultaneously seeking to produce models that are usefully grounded in external knowledge permitting them to be deployed in for excopyright ( ) 2018 association for the advancement of artiﬁcial intelligence www.aaai.org . all rights reserved * this work was conducted at microsoft † now at google research ‡ now at allen institute for artiﬁcial intelligence user input going to kusakabe tonight neural model have a great time human you ll love it try omasake the best in town figure : responses of fully data-driven conversation models are often appropriate but generally lack content characteristic of human responses ample recommendation systems e.g. for restaurants , and to adapt quickly and easily to new domains the objective here is not task completion as in traditional dialog systems but the ability to engage a user in a relevant and informative conversation the tie to external data is critical as the requisite knowledge is often not stored in conversational corpora much of this information is not found in structured databases either but is textual and can be mined from online resources such as wikipedia book reviews on goodreads and restaurant reviews on foursquare this paper presents a novel fully data-driven knowledgegrounded neural conversation model aimed at producing contentful responses our framework generalizes the sequence-to-sequence seq2seq approach hochreiter and schmidhuber 1997 sutskever vinyals and le 2014 of previous neural conversation models as it naturally combines conversational and non-conversational data via techniques such as multi-task learning caruana 1997 liu et al 2015 . the key idea is that we can condition responses not only based on conversation history sordoni et al 2015 , but also on external facts that are relevant to the current context for example foursquare entries as in fig 1 . our approach only requires a way to infuse external information 5110 a looking forward to trying pizzalibretto tonight my expectations are high b get the rocco salad can you eat calamari a anyone in chi have a dentist ofﬁce they recommend i m never going back to ... and would love a reco b really looved ora in wicker park a i m at california academy of sciences : make sure you catch the show at the planetarium tickets are usually limited a i m at new wave cafe b try to get to dmitri s for dinner their pan fried scallops and shrimp scampi are to die for a i just bought [ ... 4.3-inch portable gps navigator for my wife shh don t tell her b i heard this brand loses battery power figure : social media datasets include many contentful and useful exchanges e.g. here recommendation dialog excerpts extracted from real tweets while previous models e.g. seq2seq succeed in learning the backbone of conversations they have difﬁculty modeling and producing contentful words such as named entities which are sparsely represented in conversation data to help solve this issue we rely on non-conversational texts which represent such entities much more exhaustively based on conversation context e.g. via simple entity name matching , which makes it highly versatile and applicable in an open-domain setting using this framework we have trained systems at a large scale using 23m general-domain conversations from twitter and 1.1m foursquare tips showing signiﬁcant improvements in terms of informativeness human evaluation over a competitive large-scale seq2seq model baseline to the best of our knowledge this is the ﬁrst large-scale fully data-driven neural conversation model that effectively exploits external knowledge related work the present work situates itself within the data-driven paradigm of conversation generation in which statistical and neural machine translation models are derived from conversational data ritter cherry and dolan 2011 sordoni et al 2015 serban et al 2016 shang lu and li 2015 vinyals and le 2015 li et al 2016a . the introduction of contextual models by sordoni et al 2015 was an important advance within this framework and we extend their basic approach by injecting side information from textual data introduction of side information has been shown to be beneﬁcial to machine translation hoang cohn and haffari 2016 , as has also the incorporation of images into multi-modal translation huang et al 2016 delbrouck dupont and seddati 2017 . similarly ( he et al 2017 employ a knowledge graph to embed side information into dialog systems multi-task learning can be helpful in tasks ranging from query classiﬁcation to machine translation caruana 1997 dong et al 2015 liu et al 2015 luong et al 2016 . we adopt this approach in order to implicitly encode relevant external knowledge from textual data this work should be seen as distinct from more goaldirected neural dialog modeling in which question-answer slots are explicitly learned from small amounts of crowdsourced data customer support logs or user data wen et al 2015 2017 zhao et al 2017 . in many respects that paradigm can be characterized as the neural extension of conventional dialog models with or without statistical modeling e.g. ( oh and rudnicky 2000 ratnaparkhi 2002 banchs and li 2012 ameixa et al 2014 nio et al 2014 . our purpose is to explore the space of less clearly goaldirected but nonetheless informative i.e. informational conversation that does not demand explicit slot-ﬁlling also relevant is bordes and weston 2017 , who employ memory networks to handle restaurant reservations using a small number of keywords to handle entity types in a structured knowledge base similarly liu and perez 2017 use memory networks to manage dialog state these works utilize datasets that are relatively small and unlikely to scale whereas we leverage free-form text to draw on datasets that are several orders of magnitude larger allowing us to cover a greater diversity of domains and forms and thereby learn a more robust conversational backbone grounded response generation a primary in building fully data-driven conversation models is that most of the world s knowledge is not represented in any existing conversational datasets while these datasets serban et al 2015 have grown dramatically in size thanks in particular to social media ritter cherry and dolan 2011 , such datasets are still very far from containing discussions of every entry in wikipedia foursquare goodreads or imdb this problem considerably limits the appeal of existing data-driven conversation models as they are bound to respond evasively or deﬂectively as in fig 1 especially with regard to those entities that are poorly represented in the conversational training data on the other hand even where conversational data representing most entities of interest may exist we would still face as such huge dataset would be difﬁcult to apply in model training and many conversational patterns exhibited in the data e.g. for similar entities would be redundant our approach aims to avoid redundancy and attempts to better generalize from existing conversational data as illustrated in fig 2. while the conversations in the ﬁgure are about speciﬁc venues products and services conversational patterns are general and equally applicable to other entities the learned conversational behaviors could be used to e.g. recommend other products and services a traditional dia 5111 cid:9 ( ) cid:26 ( ) cid:27 ( ) cid:11 ( ) cid:10 $ cid:3 ( ) cid:20 % ( ) cid:10 ( ) cid:4 ( ) cid:2 ( ) cid:4 ( ) cid:6 ( ) cid:8 ( ) cid:7 ( ) cid:10 ( ) cid:12 ( ) cid:12 ( ) cid:15 ( ) cid:8 ( ) cid:5 ( ) cid:6 ( ) cid:8 ( ) ( ) cid:5 ( ) ( ) cid:26 ( ) cid:3 ( ) cid:27 ( ) cid:6 ( ) cid:9 ( ) cid:6 ( ) cid:4 ( ) cid:4 ( ) cid:11 & cid:3 ( ) cid:11 ( ) cid:17 ( ) cid:19 ( ) cid:3 ( ) cid:12 ( ) cid:12 ( ) cid:15 ( ) cid:7 ( ) cid:16 ( ) cid:7 ( ) cid:15 ( ) cid:8 ( ) cid:4 ( ) cid:7 ( ) cid:3 ( ) cid:5 ( ) cid:8 ( ) cid:10 ( ) cid:27 ( ) cid:9 ( ) cid:6 ( ) cid:4 ( ) cid:12 ( ) cid:14 ( ) cid:14 ( ) cid:17 ( ) cid:16 ( ) cid:19 ( ) cid:16 ( ) cid:17 ( ) cid:22 ( ) cid:14 ( ) cid:20 ( ) cid:23 ( ) cid:25 ( ) cid:14 ( ) cid:36 ( ) cid:68 ( ) cid:76 ( ) cid:74 ( ) cid:86 ( ) cid:86 ( ) cid:76 ( ) cid:87 ( ) cid:86 ( ) cid:76 ( ) cid:74 ( ) cid:62 ( ) cid:64 ( ) cid:10 ( ) cid:17 ( ) cid:20 ( ) ( ) cid:20 ( ) '' cid:16 ( ) cid:12 # cid:20 ( ) cid:24 ( ) cid:14 '' cid:15 ( ) cid:171 ( ) cid:3 ( ) cid:2 ( ) cid:2 ( ) cid:3 ( ) cid:10 ( ) cid:28 ( ) cid:29 ( ) cid:5 ( ) cid:30 ( ) cid:4 ( ) cid:5 ( ) cid:31 ( ) cid:26 ( ) cid:7 ( ) cid:9 ( ) cid:11 ( ) cid:2 ( ) cid:2 ( ) cid:3 ( ) cid:5 ( ) cid:7 ( ) cid:9 ( ) cid:11 figure : knowledge-grounded model architecture log system would use predeﬁned slots to ﬁll conversational backbone bold text with content here we present a more robust and scalable approach in order to infuse the response with factual information relevant to the conversational context we propose the knowledge-grounded model architecture depicted in fig 3. first we have available a large collection of world facts,1 which is a large collection of raw text entries e.g. foursquare wikipedia or amazon reviews indexed by named entities as keys then given a conversational history or source sequence , we identify the focus in , which is the text span one or more entities based on which we form a query to link to the facts this focus can either be identiﬁed using keyword matching e.g. a venue city or product name , or detected using more advanced methods such as entity linking or named entity recognition the query is then used to retrieve all contextually relevant facts f { f1 ... fk .2 finally both conversation history and relevant facts are fed into a neural architecture that features distinct encoders for conversation history and facts we will detail this architecture in the subsections below this knowledge-grounded approach is more general than seq2seq response generation as it avoids the need to learn the same conversational pattern for each distinct entity that we care about in fact even if a given entity e.g. @ pizzalibretto in fig 2 is not part of our conversational training data and is therefore out-of-vocabulary our approach is still able to rely on retrieved facts to generate an appropriate response this also implies that we can enrich our system with new facts without the need to retrain the full system we train our system using multi-task learning caruana 1997 luong et al 2016 as a way of combining conversational data that is naturally associated with external data e.g. discussions about restaurants and other businesses as in fig 2 , and less informal exchanges e.g. a response to hi how are you . more speciﬁcally our multi-task setup contains two types of tasks 1for presentation purposes we refer to these items as facts , but a fact here is simply any snippet of authored text which may contain subjective or inaccurate information 2in our work we use a simple keyword-based ir engine to retrieve relevant facts from the full collection see datasets section . 1 one purely conversational where we expose the model without fact encoder to s r training examples s representing the conversation history and the response full model with { f1 . . fk s , ) training examples exposes 2 the other task the this decoupling of the two training conditions offers several advantages including first it allows us to pre-train the conversation-only dataset separately and start multi-task training warm start with a dialog encoder and decoder that already learned the backbone of conversations second it gives us the ﬂexibility to expose different kinds of conversational data in the two tasks finally one interesting option is to replace the response in task 2 with one of the facts r fi , which makes task 2 similar to an autoencoder and helps produce responses that are even more contentful dialog encoder and decoder the dialog encoder and response decoder form together a sequence-to-sequence seq2seq model hochreiter and schmidhuber 1997 sutskever vinyals and le 2014 , which has been successfully used in building end-to-end conversational systems sordoni et al 2015 vinyals and le 2015 li et al 2016a . both encoder and decoder are recurrent neural network rnn models an rnn that encodes a variablelength input string into a ﬁxed-length vector representation and an rnn that decodes the vector representation into a variable-length output string this part of our model is almost identical to prior conversational seq2seq models except that we use gated recurrent units gru ( chung et al 2014 instead of lstm hochreiter and schmidhuber 1997 cells encoders and decoders in the present model do not share weights or word embeddings facts encoder the facts encoder of fig 3 is similar to the memory network model ﬁrst proposed by weston chopra and bordes 2015 sukhbaatar et al 2015 . it uses an associative memory for modeling the facts relevant to a particular problem— in our case an entity mentioned in a conversation–then retrieves and weights these facts based on the user input and conversation history to generate an answer memory network models have been successfully used in question answering to make inferences based on the facts saved in the memory weston et al 2016 . in our adaptation of memory networks we use an rnn encoder to turn the input sequence conversation history into a vector instead of a bag of words representation as used in the original memory network models this enables us to better exploit interlexical dependencies between different parts of the input and makes this memory network model facts encoder more directly comparable to a seq2seq model more formally we are given an input sentence = s1 s2 ... sn , and a fact set = f1 f2 ... fk that are relevant to the conversation history the rnn encoder reads the input string word by word and updates its hidden state after reading the whole input sentence the hidden state of the rnn encoder u is the summary of the input sentence 5112 by using an rnn encoder we have a rich representation for a source sentence let us assume is a dimensional vector and ri is the bag of words representation of fi with dimension v. based on sukhbaatar et al 2015 we have mi ari ci cri pi softmax ut mi o pici ( ) i=1 1 ( ) 3 ( ) ˆu o u 5 where a c rd×v are the parameters of the memory network then unlike the original version of the memory network we use an rnn decoder that is good for generating the response the hidden state of the rnn is initialized with ˆu which is a symmetrization of input sentence and the external facts to predict the response sentence word by word as alternatives to summing up facts and dialog encodings in equation , we also experimented with other operations such as concatenation but summation seemed to yield the best results the memory network model of weston chopra and bordes 2015 can be deﬁned as a multi-layer structure in this task however 1-layer memory network was used since multi-hop induction was not needed datasets the approach we describe above is quite general and is applicable to any dataset that allows us to map named entities to free-form text e.g. wikipedia imdb tripadvisor etc. . for experimental purposes we utilize datasets derived from two popular social media services twitter conversational data and foursquare non-conversational data . foursquare foursquare tips are comments left by customers about restaurants and other usually commercial establishments a large proportion of these describe aspects of the establishment and provide recommendations about what the customer enjoyed or otherwise we extracted from the web 1.1m tips relating to establishments in north america this was achieved by identifying a set of 11 likely foodie cities and then collecting tip data associated with zipcodes near the city centers while we targeted foodie cities the dataset is very general and contains tips applicable to many types of local businesses restaurants theaters museums stores etc ) in the interests of manageability for experimental purposes we ignored establishments associated with fewer than 10 tips but other experiments with up to 50 tips per venue yield comparable results we limited the tips to those for which twitter handles were found in the twitter conversation data twitter we collected a 23m general dataset of 3-turn conversations this serves as a background dataset not associated with facts and its massive size is key to learning the conversational structure or backbone separately on the basis of twitter handles found in the foursquare tip data we collected approximately million two-turn conversations that contain entities that tie to foursquare we refer to this as the 1m grounded dataset speciﬁcally we identify conversation pairs in which the ﬁrst turn contained either a handle of the business name preceded by the @ symbol or a hashtag that matched a handle.3 because we are interested in conversations among real users as opposed to customer service agents , we removed conversations where the response was generated by a user with a handle found in the foursquare data grounded conversation datasets we augment the 1m grounded dataset with facts here foursquare tips relevant to each conversation history the number of contextually relevant tips for some handles can sometimes be enormous up to 10k to ﬁlter them for relevance to the input the system vectorizes the input as tf-idf weighted word counts and each of the retrieved facts and calculates cosine similarity between the input sentence and each of the tips and retains 10 tips with the highest score tips containing that handle for each handle we created two scoring functions furthermore for a signiﬁcant portion of the 1m twitter conversations collected using handles found on foursquare the last turn was not particularly informative e.g. when it provides a purely socializing response e.g. have fun there . as one of our goals is to evaluate conversational systems on their ability to produce contentful responses we select a dev and test set 4k conversations in total designed to contain responses that are informative and useful • perplexity according to a 1-gram lm trained on all the χ-square score which measures how much content each token bears in relation to the handle each tweet is then scored according to the average content score of its terms in this manner we selected 15k top-ranked conversations using the lm score and 15k using the chi-square score a further 15k conversations were randomly sampled we then randomly sampled 10k conversations from these 45k conversations crowdsourced human judges were then presented with these 10k sampled conversations and asked to determine whether the response contained actionable information i.e. did they contain information that would permit the respondents to , e.g. whether or not they should patronize an establishment from this we selected the topranked 4k conversations to be held out as validation set and test set these were removed from our training data experimental setup multi-task learning we use multi-task learning with these tasks • facts task we expose full model the { f1 ... fn s , ) training examples 3this mechanism of linking conversations to facts using exact match on the handle is high precision but low recall but low recall seems reasonable as we are far from exhausting all available twitter and foursquare data to 5113 coder to s r examples • nofacts task we expose the model without fact en• autoencoder task this is similar to the facts task except that we replace the response with each of the facts i.e. this model is trained on { f1 ... fn s , fi examples there are times many samples for this task than for the facts task.4 the tasks facts and nofacts are representative of how our model is intended to work but we found that the autoencoder tasks helps inject more factual content into the response the different variants of our multi-task learned system exploit these tasks as follows • seq2seq trained on task nofacts with the 23m general conversation dataset since there is only one task it is not per se a multi-task setting • mtask trained on two instances of the nofacts task respectively with the 23m general dataset and 1m grounded dataset but without the facts . while not an interesting system in itself we include it to assess the effect of multi-task learning separately from facts • mtask-r trained on the nofacts task with the 23m dataset and the facts task with the 1m grounded dataset • mtask-f trained on the nofacts task with the 23m dataset and the autoencoder task with the 1m dataset • mtask-rf blends mtask-f and mtask-r as it incorporates tasks nofacts with the 23m general dataset facts with the 1m grounded dataset and autoencoder again with the 1m dataset we trained a one-layer memory network structure with two-layer seq2seq models more speciﬁcally we used 2layer gru models with 512 hidden cells for each layer for encoder and decoder the dimensionality of word embeddings is set to 512 and the size of input/output memory representation is 1024. we used the adam optimizer with a ﬁxed learning rate of 0.1. batch size is set to 128. all parameters are initialized from a uniform distribution in 3/d , where is the dimension of the parameter [ ( ) 3/d ( ) gradients are clipped at to avoid gradient explosion encoder and decoder use different sets of parameters the top 50k frequent types from conversation data is used as vocabulary which is shared between both conversation and non-conversation data we use the same learning technique as luong et al 2016 for multi-task learning in each batch all training data is sampled from one task only for task we deﬁne its mixing ratio value of αi and for each batch we select randomly a new task with probability of αi/ αj and train the system by its training data ( ) decoding and reranking we use a beam-search decoder similar to sutskever vinyals and le 2014 with beam size of 200 and maximum response length of 30. following li et al 2016a , we 4this is akin to an autoencoder as the fact fi is represented both in the input and output but is of course not strictly an autoencoder perplexity general data grounded data model seq2seq seq2seq-s mtask mtask-r mtask-f mtask-rf 55.0 125.7 57.2 55.1 77.3 67.2 214.4 82.6 82.5 77.6 448.8 97.7 table : perplexity of different models seq2seq-s is a seq2seq model that is trained on the nofacts task with 1m grounded dataset without the facts . generate n-best lists containing three features ( ) the loglikelihood log ( r|s f according to the decoder ( ) word count ( ) the log-likelihood log ( s|r of the source given the response the third feature is added to deal with the issue of generating commonplace and generic responses such as don t know which is discussed in detail in li et al 2016a . our models often do not need the third feature to be effective but—since our baseline needs it to avoid commonplace responses—we include this feature in all systems this yields the following reranking score log ( r|s f + log ( s|r + γ|r| and are free parameters which we tune on our development n-best lists using mert och 2003 by optimizing bleu to estimate ( s|r we train a sequence-tosequence model by swapping messages and responses in this model we do not use any facts evaluation metrics following sordoni et al 2015 li et al 2016a wen et al 2017 , we use bleu automatic evaluation while liu et al 2016 suggest that bleu correlates poorly with human judgment at the sentence-level,5 we use instead corpus-level bleu which is known to better correlate with human judgments przybocki peterson and bronsart 2008 , including for response generation galley et al 2015 . we also report perplexity and lexical diversity the latter as a raw yet automatic measure of informativeness and diversity automatic evaluation is augmented with human judgments of appropriateness and informativeness results automatic evaluation we computed perplexity and bleu papineni et al 2002 for each system these are shown in tables and respectively we notice that the seq2seq model speciﬁcally trained on general data has high perplexity on grounded data.6 we observe that the perplexity of mtask and mtask-r models on both general 5this corroborates earlier ﬁndings that accurate sentence-level automatic evaluation is indeed difﬁcult even for machine translation graham baldwin and mathur 2015 , as bleu and related metrics were originally designed as corpus-level metrics 6training the system on just 1m grounded data with facts doesn t solve this problem as its perplexity on general data is also high not in table . 5114 model seq2seq mtask mtask-f mtask-r mtask-rf diversity bleu 1-gram 2-gram 0.55 4.14 14.4 2.35 0.80 5.9 9.23 26.6 0.48 1.08 7.08 21.9 8.71 26.0 0.58 table : bleu-4 and lexical diversity and grounded data is as low as the seq2seq models that are trained speciﬁcally on general and grounded data respectively as expected injecting more factual content into the response in mtask-f and mtask-rf increased the perplexity especially on grounded data bleu scores are low but this is not untypical of conversational systems e.g. ( li et al 2016a 2016b ) table shows that the mtask-r model yields a signiﬁcant performance boost with a bleu score increase of 96 and 71 jump in 1-gram diversity compared to the competitive seq2seq baseline in terms of bleu scores mtask-rf improvements is not signiﬁcant but it generates the highest 1-gram and 2-gram diversity among all models human evaluation we crowdsourced human evaluations we had annotators judge 500 randomly-interleaved paired conversations asking them which was better on two parameters appropriateness to the context and informativeness the crowd workers were instructed to decide which response is more appropriate i.e. which is the best conversational ﬁt with what was said then which of the two is more informative i.e. knowledgeable helpful speciﬁc about the establishment under discussion judges were asked to select among clearly 1 maybe number , about the same maybe 2 and clearly 2. these were converted to scores between and , and assigned to the pair members depending on the order in which the pair was presented seven judges were assigned to each pair.7 the results of annotation are shown in table 3. our primary system mtask-r which performed best on bleu signiﬁcantly outperforms the seq2seq baseline on informativeness p 0.003 and shows a small but nonstatistically-signiﬁcant gain with respect appropriateness other systems are included in the table for completeness the vanilla mtask shows no signiﬁcant gain in informativeness mtask-f performed signiﬁcantly better than baseline p 0.005 on informativeness but was significantly worse on appropriateness mtask-rf came in slightly better than baseline on informativeness but worse on appropriateness though in neither case is the difference statistically signiﬁcant by the conventional standard of = 0.05. in sum our best performing mtask-r system appears to have successfully balanced the needs of informativeness and maintaining contextual appropriateness 7annotators whose variance fell greater than two standard de viations from the mean variance were dropped ( ) cid:3 ( ) cid:5 ( ) cid:3 ( ) cid:3 ( ) cid:4 ( ) cid:3 ( ) cid:3 ( ) cid:5 ( ) cid:3 ( ) cid:3 ( ) cid:4 ( ) cid:3 ( ) cid:8 ( ) cid:9 ( ) cid:7 ( ) cid:8 ( ) cid:10 ( ) cid:7 ( ) cid:12 ( ) cid:6 ( ) cid:12 ( ) cid:14 ( ) cid:2 ( ) cid:16 ( ) cid:18 figure : judge preference counts appropriateness for mtask-r versus seq2seq ( ) cid:8 ( ) cid:9 ( ) cid:7 ( ) cid:12 ( ) cid:6 ( ) cid:12 ( ) cid:4 ( ) cid:7 ( ) cid:15 ( ) cid:11 ( ) cid:17 ( ) cid:10 ( ) cid:7 figure : judge preference counts informativeness for mtask-r versus seq2seq the narrow differences in averages in table tend to obfuscate the judges voting trends to clarify the picture we translated the scores into the ratio of judges who preferred that system and binned the counts figs 4 and compare mtask-r with the seq2seq baseline bin on the left corresponds to the case where all judges voted for the system bin to that where out of judges voted for the system and so on.8 other bins are not shown since these are a mirror image of bins through 4. the distributions in fig 5 are sharper and more distinctive than in fig 4. indicating that judge preference for the mtask-r model is relatively stronger when it comes to informativeness discussion figure presents examples from the mtask-rf model and illustrates that our responses are generally both appropriate and informative first we note that our models preserve the ability of earlier work sordoni et al 2015 vinyals and le 2015 to respond contextually and appropriately on a variety of topics with responses such as me too 1 and have a safe ﬂight 2 . second our grounded models often incorporate information emerging from facts , while usually keeping the responses contextually appropriate for example in 3 , those facts revolve mostly around jewelry such as bracelets which leads the system to respond obsessed with their bracelets and the meaning behind them 8partial scores were rounded up affecting both systems equally 5115 comparison seq2seq vs mtask seq2seq vs mtask-f seq2seq vs mtask-rf seq2seq vs mtask-r * informativeness 0.501 ±0.016 0.478 ±0.015 0.492 ±0.013 0.479 ±0.017 0.499 ±0.015 0.522 ±0.015 0.508 ±0.013 0.521 ±0.013 appropriateness 0.530 ±0.017 0.537 ±0.016 0.502 ±0.015 0.495 ±0.015 0.470 ±0.017 0.463 ±0.017 0.498 ±0.014 0.505 ±0.015 table : mean differences in judgments in human evaluation together with 95 conﬁdence intervals differences sum to 1.0. boldface items are signiﬁcantly better p 0.05 than their comparator ( ) main system pre-selected on the basis of bleu a i would give almost anything for some mexican restaurant right now b me too creamy jalapeno sauce is really good a visiting the celebs at los angeles international airport lax [ ... w/ 70 others : nice airport terminal have a safe ﬂight a obsessed with jewelry company : * : oh my gosh obsessed with their bracelets and the meaning behind them a live right now on fm radio tune in ! ! b listen to lisa paige a is that ... in your photos it s on my list of places to visit in nyc b don t forget to check out the 5th ﬂoor while you are here it s a great view a are you coworking community today or tomorrow b i think so try the cookies and coffee a cant wait for my pizzeria tonight : order bacon pepperoni thin crust pizza a so ... is down to one copy of pound foolish i m curious to see if they are re-ordering it b check out the video feed on and take a picture of the simpsons on the 3rd ﬂoor 1 3 5 7 9 a i wish ... would introduce another vegetarian option besides the shroomburger it s delicious but kind of ridiculous b this is the best j.crew in the world try the lemonade 10 a just had an awesome dinner at restaurant great recommendation person b one of my favorite places ’ ve ever been to in nyc the food is great and the service is lackluster figure : responses produced by our neural response generation system mtask-rf entities marked ... are anonymized while meaning behind them does not belong to any of the facts and is instead inferred by the model which associates jewelry with sentimental value . responses inﬂuenced mostly by facts may occasionally contain a single unedited fact 4-5 , but otherwise generally combine text from different facts 6-8 .9 examples and 10 are negative examples that illustrate the two main causes of system errors the presence of an irrelevant fact e.g. j.crew in example ) and the system combining self-contradictory facts 10 . despite such errors judges found that our best grounded system is generally on par with the seq2seq system in terms of appropriateness while signiﬁcantly improving informativeness table ) conclusions we have presented a novel knowledge-grounded conversation engine that could serve as the core component of a 9facts grab a cup of coffee and get productive and try the cookies in the vending machine of local food 6 ; sit with and take a picture of the simpsons on the 3rd ﬂoor and check out the video feed on and simpsons/billiards on ! 8 . multi-turn recommendation or conversational qa system the model is a large-scale scalable fully data-driven neural conversation model that effectively exploits external knowledge and does so without explicit slot ﬁlling it generalizes the seq2seq approach to neural conversation models by naturally combining conversational and non-conversational data through multi-task learning our simple entity matching approach to grounding external information based on conversation context makes for a model that is informative versatile and applicable in open-domain systems acknowledgements we thank xuetao yin and leon xu for helping us obtain foursquare data and kevin knight chris quirk nebojsa jojic lucy vanderwende vighnesh shiv yi luan john wieting alan ritter donald brinkman and puneet agrawal references ameixa d. coheur l. fialho p. and quaresma p. 2014. luke i am your father dealing with out-of-domain requests by using movies subtitles in intelligent virtual agents 5116 och f. . 2003. minimum error rate training in statistical machine translation acl oh a. h. and rudnicky a. . 2000. stochastic language generation for spoken dialogue systems anlp/naacl workshop on conversational systems papineni k. roukos s. ward t. and zhu w.-j 2002. bleu a method for automatic evaluation of machine translation acl przybocki m. peterson k. and bronsart s. 2008. ofﬁcial results of the nist 2008 metrics for machine translation . in metricsmatr08 workshop ratnaparkhi a 2002. trainable approaches to surface natural language generation and their application to conversational dialog systems computer speech language 16 3 :435–455 ritter a. cherry c. and dolan w. . 2011. data-driven response generation in social media emnlp serban i. v. lowe r. charlin l. and pineau j 2015. a survey of available corpora for building data-driven dialogue systems corr abs/1512.05742 serban i. v. sordoni a. bengio y. courville a. and pineau j 2016. building end-to-end dialogue systems using generative hierarchical neural network models aaai shang l. lu z. and li h. 2015. neural responding machine for short-text conversation acl-ijcnlp sordoni a. galley m. auli m. brockett c. ji y. mitchell m. nie j.-y ; gao j. and dolan b 2015. a neural network approach to context-sensitive generation of conversational responses naacl-hlt sukhbaatar s. weston j. fergus r. et al 2015. end-toend memory networks nips sutskever i. vinyals o. and le q. v. 2014. sequence to sequence learning with neural networks nips vinyals o. and le q 2015. a neural conversational model icml wen t.-h. gasic m. mrkˇsi´c n. su p.-h. vandyke d. and young s. 2015. semantically conditioned lstm-based natural language generation for spoken dialogue systems emnlp wen t.-h. miao y. blunsom p. and young s. 2017. latent intent dialog models icml weston j. bordes a. chopra s. rush a. m. van merri¨enboer b. joulin a. and mikolov t. 2016. towards ai-complete question answering a set of prerequisite toy tasks iclr weston j. chopra s. and bordes a 2015. memory networks iclr zhao t. lu a. lee k. and eskenazi m. 2017. generative encoder-decoder models for task-oriented spoken dialog systems with chatting capability acl banchs r. e. and li h. 2012. iris a chat-oriented dialogue system based on the vector space model acl bordes a. and weston j 2017. learning end-to-end goaloriented dialog iclr 2017. caruana r. 1997. multitask learning machine learning 28 1 :41–75 chung j. g¨ulc¸ehre c¸ ; cho k. and bengio y 2014. empirical evaluation of gated recurrent neural networks on sequence modeling corr abs/1412.3555 delbrouck j.-b ; dupont s. and seddati o 2017. visually grounded word embeddings and richer visual features for improving multimodal neural machine translation in grounding language understanding workshop dong d. wu h. he w. yu d. and wang h. 2015. multi-task learning for multiple language translation acl galley m. brockett c. sordoni a. ji y. auli m. quirk c. mitchell m. gao j. and dolan b 2015. deltableu a discriminative metric for generation tasks with intrinsically diverse targets acl-ijcnlp graham y. baldwin t. and mathur n. 2015. accurate evaluation of segment-level machine translation metrics naacl he h. balakrishnan a. eric m. and liang p. 2017. learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings acl hoang c. d. v. cohn t. and haffari g. 2016. incorporating side information into recurrent neural network language models naacl-hlt hochreiter s. and schmidhuber j 1997. long short-term memory neural computation ( ) :1735–1780 huang p.-y ; liu f. shiang s.-r. oh j. and dyer c. 2016. attention-based multimodal neural machine translation wmt li j. galley m. brockett c. gao j. and dolan b 2016a a diversity-promoting objective function for neural conversation models naacl-hlt li j. galley m. brockett c. gao j. and dolan b 2016b a persona-based neural conversation model acl liu f. and perez j 2017. dialog state tracking a machine reading approach using memory network eacl liu x. gao j. he x. deng l. duh k. and wang y.-y 2015. representation learning using multi-task deep neural networks for semantic classiﬁcation and information retrieval naacl-hlt liu c.-w. lowe r. serban i. noseworthy m. charlin l. and pineau j 2016. how not to evaluate your dialogue system an empirical study of unsupervised evaluation metrics for dialogue response generation emnlp luong m.-t. le q. v. sutskever i. vinyals o. and kaiser l. 2016. multi-task sequence to sequence learning iclr nio l. sakti s. neubig g. toda t. adriani m. and nakamura s. 2014. developing non-goal dialog system based on examples of drama television in natural interaction with robots knowbots and smartphones springer 5117 towards coherent and cohesive long-form text generation woon sang cho cid:63 pengchuan zhang† yizhe zhang† xiujun li† michel galley† chris brockett† mengdi wang cid:63 jianfeng gao† { penzhan yizzhang xiul mgalley chrisbkt jfgao @ microsoft.com cid:63 { woonsang mengdiw @ princeton.edu cid:63 princeton university †microsoft research ai abstract generating coherent and cohesive long-form texts is a task previous works relied on large amounts of human-generated texts to train neural language models however few attempted to explicitly improve neural language models from the perspectives of coherence and cohesion in this work we propose a new neural language model that is equipped with two neural discriminators which provide feedback signals at the levels of sentence cohesion and paragraph coherence . our model is trained using a simple yet efﬁcient variant of policy gradient called negative-critical sequence training which is proposed to eliminate the need of training a separate critic for estimating baseline results demonstrate the effectiveness of our approach showing improvements over the strong baseline recurrent attention-based bidirectional mle-trained neural language model introduction the terms coherence and cohesion in linguistics are commonly deﬁned as follows williams and colomb 1995 . cohesion sentence pairs ﬁtting together the coherence what all the sentences in a piece of writing add up to the way all the pieces in a puzzle add up to the picture on the box way two pieces of a jigsaw puzzle do in layman s terms cohesion indicates that two consecutive sentences are locally well-connected and coherence indicates that multiple sentences globally hold together generating cohesive and coherent natural language texts that span multiple sentences is a task for two principal reasons first there is no formal speciﬁcation of cross-sentence linguistic properties such as coherence and cohesion of a text secondly there is no widely accepted model to measure the two properties most state-of-the-art neural approaches to natural language generation rely on a large amount of human-generated text to train language models cho et al. 2014 graves 2013 sutskever et al. 2014 . although these models can generate sentences that if judged individually are similar to human-generated ones they often fail to capture the local and global dependencies among sentences resulting in a text that is neither coherent nor cohesive for example neural language models based on recurrent neural networks rnns are widely applied to response generation for dialogue vinyals and le 2015 shang et al. 2015 sordoni et al. 2015 li et al. 2015 . although the responses by themselves look reasonable they are detached from the whole dialogue session see gao et al ( 2018 for a comprehensive survey in this paper we address the in a principled manner employing a pair of discriminators to score whether and to what extent a text is coherent or cohesive the coherence discriminator measures the compatibility among all sentences in a paragraph the cohesion discriminator measures the compatibility of each pair of consecutive sentences these models given a conditional input text and multiple candidate output texts are learned to score the candidates with respect to the criterion the scores are used as reward signals to train an rnn-based language model to generate more coherent and cohesive texts contributions our main contributions are ( ) we propose two neural discriminators for modeling coherence and cohesion of a text for longform text generation ( ) we present a simple yet effective training mechanism to encode these linguistic properties ( ) we propose negative-critical sequence training a policy gradient method that uses negative samples to estimate its reward baseline and therefore eliminates the need for a sepa proceedings of the first workshop on narrative understanding pages 1–11 minneapolis minnesota june , 2019. ( ) 2019 association for computational linguistics rate critic function and 4 we develop a new neural language model that generates more coherent and cohesive long-form texts and empirically validate its effectiveness using the tripadvisor and yelp english reviews datasets 2 related work coherence and cohesion coherence and cohesion have been extensively studied in the computational linguistics community particularly in the pre-deep-learning era lack of formal speciﬁcations for coherence and cohesion mani et al. 1998 , resulted in many different formalisms such as rhetorical structure theory mann and thompson 1988 , and other forms of coherence and cohesion relations and their quantiﬁcation mani et al. 1998 hobbs 1985 hovy 1988 mckeown 1985 cohen and levesque 1985 hovy 1991 cristea et al. 1998 halliday and hasan 1996 liddy 1991 van dijk 2013 edmundson 1969 barzilay and lapata 2008 . this list is not exhaustive however prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention reinforcement learning for text generation the text generation task can be framed as a reinforcement learning rl problem daum´e et al. 2009 , in which the generator is acting as a policy , with parameters θπ and each generated word at time , wt can be viewed as an action to be chosen by the policy from a large discrete space or vocabulary conditioned on state st−1 w≤t−1 let rt be the reward for a partially generated text sequence w≤t we deﬁne the long-term ext=1 γt−1rt , where is the initial distribution of conditional input texts following sutton et al ( 1999 , the gradient of with respect to θπ is ∇θπj es∼ρπ a∼π ·|s [ qπ s a ∇θπ log πθπ a|s ] pected reward ( ) es0∼q π ( ) where ρπ is the stationary distribution and qπ s a is the expected return from state and taking action a both following policy π. for brevity we omit the derivation in this work we formulate text generation as an episodic rl problem with episode length , rewards rl being available only at the end of episode and = 1. there are many works on training neural language models using rewards such as ranzato et al ( 2015 and paulus et al ( 2017 . these works directly optimize for speciﬁc metrics such as bleu papineni et al. 2002 or rouge lin and hovy 2003 , using reinforce williams 1992 . however these metrics do not give a complete picture of the text generation quality only recently have there been efforts to provide more relevant objectives such as consistency and repetition in a text li et al. 2015 2016a holtzman et al. 2018 . but these works use the objectives to re-rank candidate outputs not to reward or penalize them li et al ( 2016b constructed a set of reward models for the dialogue task such as information ﬂow and semantic coherence to tune the generator yet they do not provide an ablation study on the relative contribution of these reward models individually it is not clear that these reward models can be generalized to other tasks in particular long-form text generation tasks the most relevant to our work is bosselut et al ( 2018 , which promotes text generation in the correct order and discourages in its reverse order using rewards however this may not be sufﬁcient in capturing coherence since there are many negative orderings given a paragraph from this pool we assess the relative quality of generations furthermore we model cohesion between consecutive sentence pairs using word-level features gans for text generation another line of research involves the use of generative adversarial networks gans ( goodfellow et al. 2014 to incorporate feedback signals for text generation yu et al. 2017 lin et al. 2017 zhang et al. 2017 guo et al. 2017 fedus et al. 2018 zhang et al. 2018 . the discriminators in these works are trained to distinguish real texts from generated ones operating as a black-box than providing feedback on linguistic aspects yang et al ( 2018 partially addressed this issue by using a trained language model as the discriminator although the discriminator provides a ﬁne-grained feedback at the word level it does not model linguistic properties such as cohesion and coherence many text generator models are inadequate for generating a cohesive and coherent long-form text that span multiple sentences as a result human readers can easily distinguish the generated texts from real ones in this paper we argue that the primary reason is the lack of an effective mechanism to measure and control for the local and global consistency in model-generated texts 2 coherence and cohesion models we assume that global coherence of a text depends to a large degree upon how its individual sentences with different meanings are organized therefore we focus our evaluation of coherence solely based on the sentence-level features if the sentences are not organized properly the intention of the paragraph as a whole is obscure regardless of seamless local connectivity between consecutive sentences this is not to say that local connections between any two neighboring sentences can be overlooked one can easily distinguish a generated sentence from a real one by judging whether it is semantically cohesive with its neighboring sentences we strive to embody these two different yet important concepts by developing coherence and cohesion discriminators operating on the sentence level and word level respectively our design of these two discriminators is inspired by the deep structured semantic model dssm which was originally developed to measure the semantic similarity between two texts huang et al. 2013 gao et al. 2014 palangi et al. 2016 xu et al. 2017 . in this study we extend semantic similarity to coherence and cohesion in a long-form text 3.1 coherence discriminator dcoherence the coherence discriminator models the coherence score which measures how likely two text chunks add up to a single coherent paragraph let : [ s1 s2 ... sn be the source text chunk that consists of sentences t = t1 t2 ... tm be the real target text chunk that consists of sentences and cid:101 t = cid:2 ( ) t1 ( ) t2 ... ( ) ( ) ( ) be the artithat consists of cid:101 m sentences dcoherence is de s t from a negative incoherent pair s ( ) ) by dcoherence s ( ) ) assigning different scores i.e. dcoherence s t > ﬁcially constructed incoherent target text chunk signed to distinguish a positive coherent pair model architecture the model takes a form of dual encoder given source text chunk and target text chunk , the coherence discriminator dcoherence computes the coherence score in three steps as illustrated in figure ( upper . first each sentence is encoded by the bag-of-words bow embedding i.e. the average of its word vectors from a pre-trained word embedding pennington et al. 2014 . secondly an encoder which can be implemented using a convolutional neural network figure : illustration of coherence and cohesion discriminators dcoherence takes in bag-of-words sentence embeddings as inputs and dcohesion takes in the raw word embeddings of consecutive sentences as inputs the source encoder ( or ) is different from the target encoder ( or ) ( cnn 1 or rnn2 denoted as , takes as input the bow vectors of the source text chunk and encodes it into a single vector ( ) similarly g encodes the target text chunk into ( ) the two encoders ( ) and ( ) share the same architecture but do not share parameters i.e. θf cid:54 = θg and thus dcoherence s t is not symmetric thirdly dcoherence s t is computed as the cosine similarity of the two vectors ( ) and ( ) the score is a real value between −1 and , where indicates maximal coherence and −1 minimal coherence note that we use the simple bow vectors to encode sentences in the coherence discriminator which is different from the cnn sentence embedding scheme in the cohesion discriminator that we introduce in section 3.2. although the bow vector ignores the word-order information in the sentence it is empirically shown to be effective in preserving the high-level semantic information in the sentences and achieves success in sentence similarity and entailment tasks wieting et al. 2016 arora et al. 2017 . because high-level semantic information of sentences is sufﬁcient to determine whether a paragraph is coherent we choose to use bow vectors to encode sentences in dcoherence the parameters of dcoherence θf and θg are optimized using a pairwise ranking loss to this end we need both positive and negative pairs while the positive coherent pairs come from the train 1we explored with deeper networks however the performance difference was marginal for simplicity we to use a 1-layer convolutional network architecture kim 2014 collobert et al. 2011 . 2for clarity in our model description we omit rnn hereafter we present results using both cnn and rnn encoders in table . j=1 ative pairs ing data negative incoherent pairs need to be artiﬁcially constructed the next section describes the way these negative pairs are generated constructing negative given a training minibatch ( si ti } struct 2∗ b− negative pairs ( si ( ) ti j } 2b−1 incoherent pairs i=1 we confor every positive pair si ti using three different methods inspired by wieting et al ( 2016 . for notation simplicity we omit the minibatch index in the rest of this section for each positive pair s t in the minibatch • we rotate with ﬁxed and thus obtain all j=1 as neg• we shufﬂe the sentence order in once known as a derangement to break its coher• we combine the previous two methods that is we rotate in the minibatch and shufﬂe sentences within the target chunk yielding j=1 these 2b−1 negative pairs and a single positive pair in total pose a for the discriminator in learning to retrieve the correct pair training using a pairwise ranking loss the parameters of ( ) and ( ) are optimized in such a way that a positive pair scores higher than its negative pairs i.e. dcoherence s t > b−1 mismatched pairs ( , cid:101 tj } b−1 ence this yields one negative pair s ( ) ) another − negative pairs ( , cid:101 tj } b−1 dcoherence s ( ) tj for any j. to achieve this we propose to minimize the following pairwise ranking loss gong et al. 2013 with margin : lcoherence θf θg : max cid:16 + avgλ cid:16 { dcoherence s ( ) tj } 2b−1 j=1 = cid:80 n eλxj ( ) where avgλ { xj n j=1 eλxk ( ) cid:17 . , − dcoherence s t j=1 wjxj and wj notice that avgλ is the mean operator when = and approaches the max operator when → . these two extreme cases correspond to ranking against the average of all negative pairs and ranking against the single most negative pair respectively empirically training the models using the weighted average 0 λ cid:28 ∞ , which assigns larger weights to more negative pairs stabilizes the training and expedites the convergence 3.2 cohesion discriminator dcohesion the cohesion discriminator models the cohesion score which measures how likely two sentences 1 k s2 k+1 s2 , ... sn k+1 ... sm form a cohesive pair of consecutive sentences let k+1 be the real next sentence that consists of sk = cid:2 s1 cid:3 be the kth sentence that consists of words sk+1 = cid:2 s1 cid:3 : ( ) cid:101 s1 cid:3 be words and cid:101 sk+1 k+1 ( ) s2 tence that consists of cid:101 m words dcohesion is sk ( ) sk+1 by assigning them with different scores i.e. dcohesion sk sk+1 > dcohesion sk ( ) sk+1 . designed to distinguish a positive cohesive pair sk sk+1 from a negative incohesive pair the artiﬁcially constructed incohesive next sen k+1 ... ( ) ( ) k+1 model architecture like the coherence discriminator this model also takes a form of dual encoder given sk sk+1 , dcohesion computes the cohesion score in three steps as illustrated in figure ( lower . the ﬁrst step is to obtain two sequences of word embedding to represent the two sentences then a pair of source network ( ) and target network ( ) are utilized to encode both sk and sk+1 into two low-dimensional continuous vectors the two encoders ( ) and ( ) share the same architecture but do not share parameters i.e. θu cid:54 = θv and thus the dcohesion sk sk+1 is not symmetric finally dcohesion sk sk+1 is computed as the cosine similarity of the two vectors note that we use cnns or rnns to embed sentences in dcohesion which takes the word order in a sentence into consideration this is different from the bow embedding in the dcoherence where the word order does not matter because the word order indeed matters when determining the cohesion of two consecutive sentences as an example from table , for the source sentence once you get there you are greeted by the staff. , they explain everything to you. is a cohesive follow-up while you explain everything to them. is not the parameters of dcohesion θu and θv are optimized using the same pairwise ranking loss the positive pairs a training minibatch for dcohesion is obtained from 1 decomposing each paragraph s t in ( si ti } i=1 into pairs of consecutive sentences and 2 randomly selecting pairs as the positive cohesive pairs ( sk sk+1 i b i=1 we construct negative incohesive pairs using the same methods as in the coherence discriminator negative incohesive constructing we ( sk ( ) sk+1 j i 2b−1 pairs construct ∗ − negative pairs every positive pair sk sk+1 i using three different methods and omit the minibatch index hereafter for each positive for j=1 j=1 to obtain pair sk sk+1 in the minibatch • we mismatch sentence pairs ( sk ( ) sk+1 j } b−1 we shufﬂe words in sk+1 to obtain cid:101 sk+1 obtain additional pairs ( sk ( ) sk+1 j } b−1 we combine the previous two methods and in total we obtain 2b 1 negative pairs for each positive pair in the minibatch j=1 training using a pairwise ranking loss the parameters of ( ) and ( ) are optimized such that dcohesion sk sk+1 > dcohesion sk ( ) sk+1 j for any j. to achieve this we propose to minimize the following pairwise ranking loss with margin : lcohesion θu θv : max avgλ cid:16 { dcohesion sk ( ) sk+1 j } 2b−1 j=1 cid:17 ( ) , − dcohesion sk sk+1 . 2 ( ) we leave the training details and hyper parameter conﬁgurations to section 5.2 4 negative-critical sequence training for long-form text generation 4.1 long-form text generator g the generator is an attention-based bidirectional sequence-to-sequence model bahdanau et al. 2014 and is pre-trained by maximizing the log likelihood on training data which we denote as gmle however long-form texts generated using gmle often do not meet our high coherence and cohesion standards we propose to use the two pre-trained discriminators dcoherence and dcohesion to modify the text generation behavior of gmle the scores from the discriminators are used as reward or penalty signals to adjust the parameters of gmle using a variant of policy gradient called negative-critical sequence training which we propose for our task and describe in details in the next subsection 4.2 negative-critical sequence training for an arbitrary pair of and tgen where tgen is the generator s output conditioned on , we compute the coherence and cohesion scores by calling dcoherence and dcohesion since each generated text consists of multiple sentences the overall cohesion score is computed as the mean of all the consecutive sentence pairs ( sk sk+1 ⊂ s−1 tgen , where s−1 is the last sentence from the source these scalar scores however are not interpretable since the discriminators are trained by op timizing a pairwise ranking loss instead the differences between positive pair scores and the maximal or average negative pair scores provide insights of how well the models distinguish between the positive and the negative pairs this difference relates to reward with baseline in actor-critic methods barto et al. 1983 witten 1977 williams 1992 sutton et al. 1999 that typically require a separate critic function as a baseline in nlp we have observed similar practices by ranzato et al ( 2015 , bahdanau et al ( 2016 , and nguyen et al ( 2017 . rennie et al ( 2017 proposed a method that avoids learning a separate critic similarly our method does not require learning a separate critic since this margin is a form of reward minus baseline speciﬁcally we deﬁne the reward functions with baselines as rcoherence s tgen : dcoherence s tgen ( ) dcoherence s ( ) ) cid:105 ( ) e cid:101 t rcohesion [ s−1 tgen ) = cid:88 ( ) cid:12 ( ) cid:12 ( sk sk+1 ⊂ s t ( sk sk+1 ⊂ s−1 tgen 1 |tgen| e cid:101 sk+1 dcohesion sk sk+1 ( ) cid:105 dcohesion sk ( ) sk+1 ( ) where |tgen| denotes the number of sentences in tgen and ( ) ( and ( ) sk+1 are computed by aver aging over an ensemble of negative pairs notice that this reward resembles the ranking loss we use to train our discriminators except that our baseline is the mean score instead of the weighted mean over negative pairs the rationale for this difference is that because the best artiﬁcially constructed negative sample may be a formidably good sample the maximal or the weighted mean can in fact be noisy as a baseline and thus introduce noise in rewards to alleviate such noise we use the mean discriminator score of negative pairs as the baseline and this turns out to be an empirically better alternative then we use policy gradient to maximize a weighted sum of the coherence and cohesion rewards 5 experiments in this section we detail the training and evaluation of dcoherence dcohesion the baseline generator gmle and the rl-tuned generators gmle+rl cohesion , gmle+rl coherence , and cohesion coherence 0.0002 +0.3735 -0.2001 source this hotel was unbelievably overpriced we were looking for something cheaper but thought we would at least be staying in a decent hotel having paid that much when booking it wasn clear when booking that we would have to share a bathroom there was one shower for the whole ﬂoor which was tiny and unclean the room was old and lacking in facilities target the beds were very uncomfortable and the linen was very old breakfast was ok but the staff were incompetent on our last day they were too lazy to clean our table and never bothered taking our order we had to leave having had no breakfast as we ran out of time they saw us get up and leave and didn even apologise for the appalling lack of service negative target the staff recommended great restaurants with very reasonable prices within walking distance the paris hop on bus stops nearby the gare est is within blocks we paid 75 euro per nite excluding breakfast but paid for breakfast one day and found it very good and reasonably priced the rooms are clean and bathrooms ensuite more examples of cohesion once you get there you are greeted by the staff they explain everything to you and in english not the best but good enough the coffee was even good for a coffee snob like myself the hotel is much smaller than thought and only has six ﬂoors the only negative was the curtain in the bathroom it was very shear and we felt that people in the building across the street could look right in at night the beer at the lobby bar was stale there are many friendly cats on the grounds 0.0411 0.0084 0.0054 0.0768 0.0591 -0.0097 0.0457 0.0514 0.0798 -0.0156 0.0082 0.1004 -0.1103 0.0787 -0.0830 table : coherence and cohesion rewards on test data the cohesion reward at the end of each line is computed with its next sentence this is an example of contradiction and inconsistent sentiment suggestive of incoherence we append more examples with extreme cohesion rewards tripadvisor discriminators encoding dcoherence dcohesion 1-layer bi-dir 2,3,4,5 conv512 gru1024 conv512 gru1024 3,4,5,6 1-layer bi-dir target sentences retrieval @ r 5 0.43 0.18 0.50 0.26 0.28 0.12 0.11 0.21 @ 10 0.60 0.65 0.43 0.33 yelp discriminators encoding dcoherence dcohesion 1-layer bi-dir 2,3,4,5 conv512 gru1024 conv512 gru1024 3,4,5,6 1-layer bi-dir target sentences retrieval @ r 5 0.61 0.33 0.68 0.39 0.33 0.14 0.11 0.26 @ 10 0.74 0.81 0.47 0.39 table : retrieval ratios for coherence and cohesion discriminators from a collection of 100 negative candidates from the test data the reported numbers are the averages over 20 evaluations notations conv512 2,3,4,5 is a convolutional input encoder with ﬁlter sizes , , , and , and there are 512 ﬁlters for each ﬁlter size gru1024 1-layer bi-dir is a 1-layered bi-directional gru input encoder with hidden size 1024. we experimented different conﬁgurations for both encoder types and selected the best performing models for the negative-critical sequence training step gmle+rl coherence cohesion . we show that by using feedback from the discriminators the quality of the generated texts is signiﬁcantly improved see table for a sample comparison 5.1 dataset we use the tripadvisor hotel english reviews dataset collected by wang et al ( 2010 and the yelp english reviews dataset3 we use only the : //www.yelp.com/dataset source sentences gmle gmle+rl coherence cohesion the hotel inglaterra delivered as promised the staff was welcoming and spoke good english the cleaning staff did a very good job every day the rooms were spotless and very modern the bathroom was large and had a very nice shower and there were two generously sized bath towels that were twice the size of normal towels the breakfast in the morning was delicious and very good it was the only hotel where slept very well the staff was very helpful in late afternoon or late times the breakfast was adequate with a decent range of cereals fruit and fruits there is also free use of the coffee in the reception area the breakfast was plentiful including fresh breads and cooked to order the location was fantastic it is in the north of the marina and in a very short distance the marina has a small swimming pool with sitting area and a small gym they are very popular and guests have an evening reception which is very nice table : sample generations from our mle-trained baseline model gmle and our discriminator-guided model gmle+rl coherence cohesion . the red texts highlight a common problem in gmle it exhibits a repetition and an inconsistent opinion as a review in contrast our discriminator-guided model is able to generate a more interesting and sentiment-consistent continuation model nll ppl bleu-3 bleu-4 bleu-5 intraunique-1 intraunique-2 interunique-2 interunique-3 tripadvisor gmle baseline gmle +rl cohesion gmle+rl coherence gmle+rl coherence cohesion 0.86 0.77 0.80 0.80 2.36 2.18 2.24 2.25 yelp gmle baseline gmle+rl cohesion gmle+rl coherence gmle+rl coherence cohesion 1.32 1.26 1.24 1.25 3.84 3.65 3.56 3.59 0.38 0.46 0.44 0.44 0.37 0.45 0.45 0.43 0.19 0.27 0.25 0.24 0.17 0.23 0.23 0.22 0.08 0.14 0.12 0.12 0.07 0.11 0.11 0.11 0.66 0.64 0.64 0.65 0.68 0.68 0.69 0.69 0.93 0.94 0.94 0.94 0.95 0.95 0.95 0.95 0.40 0.38 0.39 0.40 0.54 0.53 0.55 0.56 model nll ppl bleu-3 bleu-4 bleu-5 intraunique-1 intraunique-2 interunique-2 interunique-3 length ratio 1.08 0.97 1.06 1.02 length ratio 1.07 1.05 1.00 1.05 0.72 0.71 0.72 0.72 0.86 0.85 0.87 0.88 table : an ablation study with automated evaluation metric scores nll ppl bleu-n intra/inter-unique-n along with the length ratio with the length of corresponding true target sentences as 1. signiﬁcant numbers are highlighted in bold before rounding subsets of the two datasets that satisfy the following two conditions ( ) a review must have at least 10 sentences and 2 each sentence has from to 30 words this yields roughly 60,000 tripadvisor reviews and 220,000 yelp reviews split into 0.8 0.1 0.1 ratio for train/dev/test sets we merge the source and target vocabularies and limit it to the top 50,000 frequent words excluding special tokens for each review we use the ﬁrst ﬁve sentences as the input to , and the next ﬁve sentences as the target output from g. implementation details 5.2 baseline gmle gmle takes individual words as inputs and embeds into a pre-trained glove 300dimensional word vectors this embedding layer is ﬁxed throughout training gmle uses a twolayered gru and hidden size of 1024 for both encoder and decoder during optimization using adam kingma and ba 2014 , we set the learning rate to 2e-4 and clip the gradient s l2-norm to 1.0. we initially train gmle for 60 epochs on the tripadvisor data and 30 epochs on the yelp data discriminators for the cnn-based encoder the convolutional layer consists of ﬁlters of sizes , , , and for dcoherence 3 4 5 and for dcohesion , each with 512 ﬁlters each convolution ﬁlter is followed by a tanh activation then we max-pool in time and append a fully connected layer to generate a feature vector of dimension 512 followed by a batch normalization layer and a tanh activation for the rnn-based encoder we use a 1-layered bi-directional gru concatenate the ﬁnal hidden states at both ends and append the same remaining layers both discriminators use the pre-trained glove word embedding vectors4 which are ﬁxed during the training we use an adam optimizer with a learning rate of 1e-5 we ﬁx = and = 0.2 in equations 1 and 2 .5 we train both discriminators for 50 epochs and choose the models with the best @ scores on the validation dataset model gmle+rl in the ﬁne-tuning stage we use the negative-critical sequence training method 4the vector dimension can be different from that of g. the differences were marginal for sizes 50 100 and 300. for results shown in this paper we used the same dimension of size 300 5we performed a coarse grid search over the values of and and these values for the hyper-parameters pair resulted in fast convergence to high recall scores on the dev dataset 7 cohesion human judges preferred coherence human judges preferred our method neutral comparison 36.41 33.57 30.50 gmle 29.91 30.85 39.24 human gmle+rl gmle+rl our method neutral comparison 37.23 31.44 31.80 gmle 28.96 31.32 39.72 human gmle+rl gmle+rl table : results of human evaluation showing preferences % for our model gmle+rl coherence cohesion vis-a-vis the baseline gmle after adjustment for spamming gmle+rl coherence cohesion is preferred over gmle for simplicity the 5-point likert scale has been collapsed to a 3-point scale see the appendix for further details of distributions as described in section , up to epochs with a learning rate of 1e-5 we equally weight the coherence and cohesion rewards 1 rcoherence s tgen + rcohesion [ s−1 tgen ) we also continue the su1 pervised learning of to constrain the policy search within a space that represents the sentences that are likely to be grammatically plausible similar to paulus et al ( 2017 ; wu et al ( 2016 ; lewis et al ( 2017 . for all the generations from gmle and gmle+rl we use the simple greedy decoding method because we do not observe any signiﬁcant difference when switching to beam search 5.3 results evaluating dcoherence and dcohesion since the discriminators are implemented as pairwise rankers we employ the metrics commonly used in information retrieval for evaluation i.e. recall at ( @ ) which is deﬁned as the fraction of correctly identifying an item in the top-k retrieved list baeza-yates and ribeiro-neto 1999 . we present the retrieval results in table 2. to help readers understand the roles of dcoherence and dcohesion we present examples of positive and negative pairs and their rewards in table 1. automatic evaluation of g. it is widely known that there is no perfect automated metric to evaluate text generators nevertheless we report the scores of widely used metrics including negative log-likelihood nll , perplexity ppl , bleu and the proportion of unique n-grams within a single generation intra-unique-n , and across generations inter-unique-n , as in gu et al ( 2018 . results in table show that our discriminators signiﬁcantly improve bleu scores nll and ppl with marginal difference in diversity human evaluation of g. coherence and cohesion of a text can not be easily measured using standard automated metrics thus we perform crowd-sourced human evaluation we ran domly selected 200 samples from the tripadvisor dataset including corresponding generated output from the baseline gmle and our model gmle+rl for comparison we pair systems as human gmle+rl and gmle+rl gmle . the outputs of these system pairs are presented in random order and each is ranked in terms of coherence and cohesion using a ﬁve-point likert scale by human judges initially we hired judges to judge each pair we identiﬁed a group of poor judges probable spammers who choose gmle+rl over the human more than 40 of the time and eliminated them from the judge pool table reports the ﬁnal scores in terms of percentages of the total remaining judgments 6 conclusion this paper proposes a neural approach to explicitly modeling cross-sentence linguistic properties coherence and cohesion for long-form text generation the coherence discriminator dcoherence provides a macro-level view on structuring a paragraph the cohesion discriminator dcohesion provides a micro-level view on local connectivity between neighboring sentences the pre-trained discriminators are used to score the generated texts and artiﬁcially constructed negative pair scores are used to form baselines for the policy gradient which we call negative-critical sequence training to train neural language models on two long-form text generation tasks human evaluation results are consistent with automatic evaluation results which together demonstrate that our proposed method generates more locally and globally consistent texts with the help of the discriminators despite the encouraging initial results we only scratched the surface of the problem the proposed method is yet to be signiﬁcantly improved to meet the ultimate goal of generating meaningful and logical long-form texts 8 references sanjeev arora yingyu liang and tengyu ma 2017. a simple but tough-to-beat baseline for sentence embeddings in international conference on learning representations ricardo baeza-yates and berthier ribeiro-neto 1999. modern information retrieval volume 463. acm press books dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio 2016. an actor-critic algorithm for sequence prediction arxiv preprint arxiv:1607.07086. dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio 2014. jointly learning to align and translate abs/1409.0473 andrew barto richard sutton and charles anderson 1983. neuronlike adaptive elements that can solve difﬁcult learning control problems ieee transactions on systems man and cybernetics smc-13 5 :834–846 regina barzilay and mirella lapata 2008. modeling local coherence an entity-based approach computational linguistics 34 1 :1–34 antoine bosselut asli celikyilmaz xiaodong he jianfeng gao po-sen huang and yejin choi 2018. discourse-aware neural rewards for coherent text generation in proc of naacl pages 173–184 kyunghyun cho bart van merri¨enboer c¸ alar g¨ulc¸ehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio 2014. learning phrase representations using rnn encoder–decoder for statistical machine translation in emnlp philip cohen and hector levesque 1985. speech in proceedings of the 23rd acts and rationality annual meeting on association for computational linguistics pages 49–60 association for computational linguistics ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu and pavel kuksa 2011. natural language processing almost from scratch j. mach learn res. 12:2493–2537 dan cristea nancy ide and laurent romary 1998. veins theory a model of global discourse cohein proceedings of the 36th sion and coherence annual meeting of the association for computational linguistics and 17th international conference on computational linguistics-volume , pages 281– 285. association for computational linguistics hal daum´e john langford and daniel marcu 2009. search-based structured prediction corr abs/0907.0786 harold edmundson 1969. new methods in aujournal of the acm jacm , tomatic extracting 16 2 :264–285 william fedus ian goodfellow and andrew dai 2018. maskgan better text generation via ﬁlling in the ˙˙˙˙ in iclr jianfeng gao michel galley and lihong li 2018. arxiv neural approaches to conversational ai preprint arxiv:1809.08267. jianfeng gao patrick pantel michael gamon xiaodong he and li deng 2014. modeling interestingness with deep neural networks in proceedings of the 2014 conference on empirical methods in natural language processing emnlp , pages 2– 13. yunchao gong yangqing jia thomas leung alexander toshev and sergey ioffe 2013. deep convolutional ranking for multilabel image annotation arxiv preprint arxiv:1312.4894. ian goodfellow jean pouget-abadie mehdi mirza bing xu david warde-farley sherjil ozair aaron courville and yoshua bengio 2014. generative adin advances in neural information versarial nets processing systems 27 pages 2672–2680 alex graves 2013. recurrent neural networks arxiv:1308.0850. generating sequences with arxiv preprint xiaodong gu kyunghyun cho jungwoo ha and sunghun kim 2018. dialogwae multimodal response generation with conditional wasserstein auto-encoder corr abs/1805.12352 jiaxian guo sidi lu han cai weinan zhang yong yu and jun wang 2017. long text generation via adversarial training with leaked information arxiv preprint arxiv:1709.08624. halliday and ruqaiya hasan 1996. cohesion in jerry hobbs 1985. on the coherence and structure text of discourse ari holtzman jan buys maxwell forbes antoine bosselut david golub and yejin choi 2018. learning to write with cooperative discriminators in proceedings of the association for computational linguistics eduard hovy 1988. planning coherent multisentenin proceedings of the 26th annual meettial text ing on association for computational linguistics pages 163–169 association for computational linguistics eduard hovy 1991. approaches to the planning of in natural language generation in coherent text artiﬁcial intelligence and computational linguistics pages 83–102 springer 9 po-sen huang xiaodong he jianfeng gao li deng alex acero and larry p. heck 2013. learning deep structured semantic models for web search using clickthrough data in cikm yoon kim 2014. convolutional neural networks for sentence classiﬁcation in emnlp diederik p. kingma and jimmy ba 2014. adam corr a method for stochastic optimization abs/1412.6980 mike lewis denis yarats yann dauphin devi parikh and dhruv batra 2017. deal or no deal end-to-end learning for negotiation dialogues arxiv preprint arxiv:1706.05125. jiwei li michel galley chris brockett jianfeng gao and bill dolan 2015. a diversity-promoting objective function for neural conversation models arxiv preprint arxiv:1510.03055. jiwei li michel galley chris brockett georgios spithourakis jianfeng gao and bill dolan 2016a a persona-based neural conversation model arxiv preprint arxiv:1603.06155. jiwei li will monroe alan ritter michel galley jianfeng gao and dan jurafsky 2016b deep reinforcement learning for dialogue generation arxiv preprint arxiv:1606.01541. jiwei li will monroe tianlin shi s´ebastien jean alan ritter and dan jurafsky 2017. adversarial learning for neural dialogue generation arxiv preprint arxiv:1701.06547. elizabeth duross liddy 1991. the discourse-level structure of empirical abstracts an exploratory information processing management study 27 1 :55–81 chin-yew lin and eduard hovy 2003. automatic evaluation of summaries using n-gram coin proceedings of the 2003 occurrence statistics conference of the north american chapter of the association for computational linguistics on human language technology volume , naacl 03 pages 71–78 stroudsburg pa usa kevin lin dianqi li xiaodong he zhengyou zhang and ming-ting sun 2017. adversarial ranking for language generation in advances in neural information processing systems pages 3155–3165 inderjeet mani eric bloedorn and barbara gates 1998. using cohesion and coherence models for text in intelligent text summarization summarization symposium pages 69–76 william mann and sandra a thompson 1988. rhetorical structure theory toward a functional theory of text organization text-interdisciplinary journal for the study of discourse 8 3 :243–281 kathleen mckeown 1985. discourse strategies for generating natural-language text artiﬁcial intelligence 27 1 :1–41 khanh nguyen hal daum´e and jordan l. boydgraber 2017. reinforcement learning for bandit neural machine translation with simulated human feedback in emnlp h. palangi l. deng y. shen j. gao x. he j. chen x. song and r. ward 2016. deep sentence embedding using long short-term memory networks analysis and application to information retrieval ieee/acm transactions on audio speech and language processing 24 4 :694–707 kishore papineni salim roukos todd ward and weijing zhu 2002. bleu a method for automatic in proceedings evaluation of machine translation of the 40th annual meeting on association for computational linguistics acl 02 pages 311–318 romain paulus caiming xiong and richard socher 2017. a deep reinforced model for abstractive summarization corr abs/1705.04304 jeffrey pennington richard socher and christopher d. manning 2014. glove global vectors for word representation in empirical methods in natural language processing emnlp , pages 1532– 1543. marc aurelio ranzato sumit chopra michael auli and wojciech zaremba 2015. sequence level training with recurrent neural networks corr abs/1511.06732 steven j. rennie etienne marcheret youssef mroueh jarret ross and vaibhava goel 2017. self-critical sequence training for image captioning 2017 ieee conference on computer vision and pattern recognition cvpr , pages 1179–1195 lifeng shang zhengdong lu and hang li 2015. neural responding machine for short-text conversation arxiv preprint arxiv:1503.02364. alessandro sordoni michel galley michael auli chris brockett yangfeng ji margaret mitchell jian-yun nie jianfeng gao and bill dolan 2015. a neural network approach to context-sensitive genin naacleration of conversational responses hlt ilya sutskever oriol vinyals and quoc le 2014. sequence to sequence learning with neural networks in nips pages 3104–3112 richard s. sutton david mcallester satinder singh and yishay mansour 1999. policy gradient methods for reinforcement learning with function approximation in proceedings of the 12th international conference on neural information processing systems nips 99 pages 1057–1063 mit press teun a van dijk 2013. news as discourse routledge 10 oriol vinyals and quoc le 2015. a neural conversa tional model icml deep learning workshop hongning wang yue lu and chengxiang zhai 2010. latent aspect rating analysis on review text data a rating regression approach in kdd john wieting mohit bansal kevin gimpel and karen livescu 2016. towards universal paraphrastic sentence embeddings iclr j.m williams and g.g colomb 1995. style toward clarity and grace chicago guides to writing editing and publishing university of chicago press ronald j. williams 1992. simple statistical gradientfollowing algorithms for connectionist reinforcement learning mach learn. 8 3-4 :229–256 ian witten 1977. an adaptive optimal controller for discrete-time markov environments information and control 34 4 :286–295 yonghui wu mike schuster zhifeng chen quoc le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al 2016. google s neural machine translation system bridging the gap between arxiv preprint human and machine translation arxiv:1609.08144. tao xu pengchuan zhang qiuyuan huang han zhang zhe gan xiaolei huang and xiaodong he 2017. attngan fine-grained text to image generation with attentional generative adversarial networks arxiv preprint zichao yang zhiting hu chris dyer eric xing and taylor berg-kirkpatrick 2018. unsupervised text style transfer using language models as discriminators arxiv preprint arxiv:1805.11749. lantao yu weinan zhang jun wang and yong yu 2017. seqgan sequence generative adversarial nets with policy gradient in aaai yizhe zhang michel galley jianfeng gao zhe gan xiujun li chris brockett and bill dolan 2018. generating informative and diverse conversational responses via adversarial information maximization in nips yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen and lawrence carin 2017. adversarial feature matching for text generation in nips 11 a pre-training strategy for zero-resource response selection in knowledge-grounded conversations chongyang tao1∗ changyu chen2∗ jiazhan feng1 jirong wen2,3 and rui yan2,3† 2gaoling school of artiﬁcial intelligence renmin university of china 1peking university beijing china 3beijing academy of artiﬁcial intelligence { chongyangtao fengjiazhan @ pku.edu.cn { chen.changyu jrwen ruiyan @ ruc.edu.cn abstract response ) multi-turn recently many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge e.g. documents when conversing with humans however it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents which hinders the effective and adequate training of knowledge selection and response matching to overcome the , we consider decomposing the training of the knowledge-grounded selection into three tasks including 1 query-passage ) query-dialogue history matching task matching task response matching task and joint learning all these tasks in a uniﬁed pre-trained language model the former two tasks could help the model in knowledge selection and comprehension while the last task is designed for matching the proper response with the given query and background knowledge dialogue history . by this means the model can be learned to select relevant knowledge and distinguish proper response with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training introduction along with the very recent prosperity of artiﬁcial intelligence empowered conversation systems in the spotlight many studies have been focused on building human-computer dialogue systems wen et al. 2017 zhang et al. 2020 with either retrievalbased methods wang et al. 2013 wu et al. 2017 ∗equal contribution †corresponding author rui yan ruiyan ruc.edu.cn . whang et al. 2020 or generation-based methods li et al. 2016 serban et al. 2016 zhang et al. 2020 , which both predict the response with only the given context in fact unlike a person who may associate the conversation with the background knowledge in his or her mind the machine can only capture limited information from the query message itself as a result it is difﬁcult for a machine to properly comprehend the query and to predict a proper response to make it more engaging to bridge the gap of the knowledge between the human and the machine researchers have begun to simulating this motivation by grounding dialogue agents with background knowledge zhang et al. 2018 dinan et al. 2019 li et al. 2020 , and lots of impressive results have been obtained in this paper we consider the response selection problem in knowledge-grounded conversion and specify the background knowledge as unstructured documents that are common sources in practice the task is that given a conversation context and a set of knowledge entries one is required ) to select proper knowledge and grasp a good comprehension of the selected document materials knowledge selection ; ) to distinguish the true response from a candidate pool that is relevant and consistent with both the conversation context and the background documents knowledge matching . while there exists a number of knowledge documents on the web it is non-trivial to collect large-scale dialogues that are naturally grounded on the documents for training a neural response selection model which hinders the effective and adequate training of knowledge selection and response matching although some benchmarks built upon crowd-sourcing have been released by recent works zhang et al. 2018 dinan et al. 2019 , the relatively small training size makes it hard for the dialogue models to generalize on other domains or topics zhao et al. 2020 . thus in this work we 4446 proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing pages 4446–4457 august 1–6 2021 ©2021 association for computational linguistics focus on a more and practical scenario learning a knowledge-grounded conversation agent without any knowledge-grounded dialogue data which is known as zero-resource settings since knowledge-grounded dialogues are unavailable in training it raises greater for learning the grounded response selection model fortunately there exists a large number of unstructured knowledge e.g. web pages or wiki articles , passage search datasets e.g. query-passage pairs coming from ad-hoc retrieval tasks ( khattab and zaharia 2020 and multi-turn dialogues e.g. context-response pairs collected from reddit ( henderson et al. 2019 , which might be beneﬁcial to the learning of knowledge comprehension knowledge selection and response prediction respectively besides in multi-turn dialogues the background knowledge and conversation history excluding the latest query are symmetric in terms of the information they convey and we assume that the dialogue history can be regarded as another format of background knowledge for response prediction based on the above intuition in this paper we consider decomposing the training of the grounded response selection task into several sub-tasks and joint learning all those tasks in a uniﬁed model to take advantage of the recent breakthrough on pretraining for natural language tasks we build the grounded response matching models on the basis of a pre-trained language model plms ( devlin et al. 2019 yang et al. 2019 , which are trained with large-scale unstructured documents from the web on this basis we further train the plms with query-passage matching task query-dialogue history matching task and multi-turn response matching task jointly the former two tasks could help the model not only in knowledge selection but also in knowledge and dialogue history comprehension while the last task is designed for matching the proper response with the given query and background knowledge dialogue history . by this means the model can be learned to select relevant knowledge and distinguish proper responses with the help of a large number of ungrounded dialogues and ad-hoc retrieval corpora during the testing stage we ﬁrst utilize the trained model to select proper knowledge and then feed the query dialogue history selected knowledge and the response candidate into our model to calculate the ﬁnal matching degree particularly we design two strategies to compute the ﬁnal matching score in the ﬁrst strategy we directly concatenate the selected knowledge and dialogue history as a long sequence of background knowledge and feed into the model in the second strategy we ﬁrst compute the matching degree between each queryknowledge and the response candidates and then integrate all matching scores we conduct experiments with benchmarks of knowledge-grounded dialogue that are constructed by crowd-sourcing such as the wizard-ofwikipedia corpus dinan et al. 2019 and the cmu dog corpus zhou et al. 2018a . evaluation results indicate that our model achieves comparable performance on knowledge selection and response selection with several existing models trained on crowd-sourced benchmarks our contributions are summarized as follows • to the best of our knowledge this is the ﬁrst exploration of knowledge-grounded response selection under the zero-resource setting • we propose decomposing the training of the grounded response selection models into several sub-tasks so as to empower the model through these tasks in knowledge selection and response matching • we achieve a comparable performance of response selection with several existing models learned from crowd-sourced training sets 2 related work early studies of retrieval-based dialogue focus on single-turn response selection where the input of a matching model is a message-response pair wang et al. 2013 ji et al. 2014 wang et al. 2015 . recently researchers pay more attention to multiturn context-response matching and usually adopt the representation-matching-aggregation paradigm to build the model representative methods include the dual-lstm model lowe et al. 2015 , the sequential matching network smn ( wu et al. 2017 , the deep attention matching network dam ( zhou et al. 2018b , interaction-overinteraction network ioi ( tao et al. 2019 and multi-hop selector network msn ( yuan et al. 2019 . more recently pre-trained language models devlin et al. 2019 yang et al. 2019 have shown signiﬁcant beneﬁts for various nlp tasks and some researchers have tried to apply them on multi-turn response selection vig and ramea 2019 exploit bert to represent each utteranceresponse pair and fuse these representations to 4447 calculate the matching score whang et al ( 2020 and xu et al ( 2020 treat the context as a long sequence and conduct context-response matching with bert besides gu et al ( 2020a integrate speaker embeddings into bert to improve the utterance representation in multi-turn dialogue to bridge the gap of the knowledge between the human and the machine researchers have investigated into grounding dialogue agents with unstructured background knowledge ghazvininejad et al. 2018 zhang et al. 2018 dinan et al. 2019 . for example zhang et al ( 2018 build a persona-based conversation data set that employs the interlocutor s proﬁle as the background knowledge zhou et al ( 2018a publish a data where conversations are grounded in articles about popular movies dinan et al ( 2019 release another documentgrounded data with wiki articles covering a wide range of topics meanwhile several retrievalbased knowledge-grounded dialogue models are proposed such as document-grounded matching network dgmn ( zhao et al. 2019 and dually interactive matching network dim ( gu et al. 2019 which let the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention mechanism gu et al ( 2020b further propose to pre-ﬁlter the context and the knowledge and then use the ﬁltered context and knowledge to perform the matching with the response besides with the help of gold knowledge index annotated by human wizards dinan et al ( 2019 consider joint learning the knowledge selection and response matching in a multi-task manner or training a two-stage model 3 model in this section we ﬁrst formalize the knowledgegrounded response matching problem and then introduce our method from preliminary to response matching with plms to details of three pre-training tasks 3.1 problem formalization we ﬁrst describe a standard knowledge-grounded response selection task such as wizard-ofwikipedia suppose that we have a knowledgegrounded dialogue data set = ki ci ri yi n where ki { p1 p2 . . plk represents a collection of knowledge with pj the j-th knowledge entry a.k.a. passage and lk is the number of entries ci { u1 u2 . . ulc denotes i=1 multi-turn dialogue context with uj the j-th turn and lc is the number of dialogue turns it should be noted that in this paper we denote the latest turn ulc as dialogue query qi and dialogue context except for query is denoted as hi ci/ qi . ri stands for a candidate response yi 1 indicates that ri is a proper response for ci and ki otherwise yi 0. is the number of samples in data set the goal knowledge-grounded dialogue is to learn a matching model ( , , ) from , and thus for any new k c r , ( , , ) returns the matching degree between and k c . finally one can collect the matching scores of a series of candidate responses and conduct response ranking i=1 zero-resource grounded response selection then is formally deﬁned as follows there is a standard multi-turn dialogue dataset dc { qi hi ri n and an ad-hoc retrieval dataset dp { qi pi zi m i=1 where qi is a query and pi stands a candidate passage zi 1 indicates that pi is a relevant passage for qi otherwise zi 0. our goal is to learn a model ( , , , ) from dc and dp and thus for any new input k h q r , our model can select proper knowledge ˆk from and calculate the matching degree between and ˆk q h . 3.2 preliminary response matching with plms pre-trained language models have been widely used in many nlp tasks due to the strong ability of language representation and understanding in this work we consider building a knowledge-grounded response matching model with bert speciﬁcally given a query , a dialogue history = u1 u2 ... unh where ui is the i-th turn in the history a response candidate = r1 r2 ... rlr with lr words sequences as a single we concatenate all consecutive special sequence with tokens tokens which can be represented as = [ cls , u1 [ sep , . , sep , ulh [ sep , , sep , , sep } [ cls and sep are classiﬁcation segment symbol symbol respectively for each token in , bert uses a summation of three kinds of embeddings including wordpiece embedding wu et al. 2016 , segment embedding and position embedding separation and then the embedding sequence of is fed into bert giving us the contextualized embedding sequence e cls , e2 . . elx . [ cls is an aggregated representation vector that contains the 4448 figure : the overall architecture of our model semantic interaction information between the query history and response candidate finaly e cls is fed into a non-linear layer to calculate the ﬁnal matching score which is formulated as g h q r = ( w2 tanh w1e cls + b1 + b2 ( ) where { 1,2 and { 1,2 is training parameters for response selection task σ is a sigmoid function in knowledge-grounded dialogue each dialogue is associated with a large collection of knowledge entries = p1 p2 . . plk 1. the model is required to select ( ≥ ) knowledge entries based on semantic relevance between the query and each knowledge and then performs the response matching with the query dialogue history and the highly-relevant knowledge speciﬁcally we denote ˆk ( ˆp1 . . ˆpm as the selected knowledge entries and feed the input sequence = [ cls , ˆp1 [ sep , . , sep , ˆpm [ sep , u1 [ sep , . , sep , ulh [ sep , , sep , , sep } to bert the ﬁnal matching score ( ˆk h q r can be computed based on cls representation 3.3 pre-training strategies on the basis of bert we further jointly train it with three tasks including ) query-passage matching task 2 query-dialogue history matching task 3 multi-turn response matching task the former two tasks could help the model in knowledge selection and knowledge and dialogue history comprehension while the last task is designed for matching the proper response with the given query and background knowledge dialogue 1the scale of the knowledge referenced by each dialogue usually exceeds the limitation of input length in plms history . by this means the model can be learned to select relevant knowledge and distinguish the proper response with the help of a large number of ungrounded dialogues and ad-hoc retrieval corpora 3.3.1 query-passage matching although there exist a huge amount of conversation data on social media it is hard to collect sufﬁcient dialogues that are naturally grounded on knowledge documents existing studies dinan et al. 2019 usually extract the relevant knowledge before the response matching or jointly train the knowledge retrieval and response selection in a multi-task manner however both methods need in-domain knowledge-grounded dialogue data with gold knowledge label to train making the model hard to generalize to a new domain fortunately the ad-hoc retrieval task harman 2005 khattab and zaharia 2020 in the information retrieval area provides a potential solution to simulate the process of knowledge seeking to take advantage of the parallel data in the ad-hoc retrieval task we consider incorporating the query-passage matching task so as to help the knowledge selection and knowledge comprehension for our task given a query-passage pair q p , we ﬁrst concatenate the query and the passage as a single consecutive token sequence with special tokens separating them which is formulated as sqp { cls , wp , . , wp np [ sep , wq , . , wq nq ( ) , wq where wp denotes the i-th and j-th token of knowledge entry and query respectively for each token in sqp token segment and position 4449 𝑔𝑞 𝑘 𝑟 ! `` mlp output layer position embeddings segment embeddings token embeddings ! input cls [ sep query-dialogue history matching task query-passage matching task $ 𝐸 % 𝑬 '' # & $ 𝐸 % 𝑬 𝐸 % 𝐸 ! response matching task ( % # & ··· pre-trained language model bert ··· ··· ··· ··· ··· ··· ··· ··· '' ! '' ··· sep 𝑤 '' $ '' dialogue history or knowledge background knowledge [ query ··· response ··· ··· ··· $ 𝑟 [ sep 𝑢 𝑞 ··· sep 𝑤 , # sep 𝑤 , query response embeddings are summated and fed into bert it is worth noting that here we set the segment embedding of the knowledge to be the same as the dialogue history finally we feed the output representation of cls eqp cls into a mlp to obtain the ﬁnal query-passage matching score ( , ) the loss function of each training sample for query-passage matching task is deﬁned by lp q p+ p − log − , . , − np eg q p+ eg q p+ + cid:80 δp j=1 eg q p ) ( ) where p+ stands for the positive passage for , p− is the j-th negative passage and δp is the number of negative passage 3.3.2 query-dialogue history matching in multi-turn dialogues the conversation history excluding the latest query is a piece of supplementary information for the current query and can be regarded as another format of background knowledge during the response matching besides due to the natural sequential relationship between dialogue turns the dialogue query usually shows a strong semantic relevance with the previous turns in the dialogue history inspired by such characteristics we design a query-dialogue history matching task with the multi-turn dialogue context so as to enhance the capability of the model to comprehend the dialogue history with the given dialogue query and to rank relevant passages with these pseudo query-passage pairs we ﬁrst speciﬁcally the concatenate the dialogue history into a long sequence task requires the model to predict whether a nq and a dialogue history query = wq sequence = wh are consecutive and relevant we concatenate two sequences into a single consecutive sequence with sep tokens sqh { cls , wh , . , wh nh , . , wq nh [ sep , wq nq ( ) , . , wq , . , wh for each word in sqh token segment and position embeddings are summated and fed into bert finally we feed eqh cls into a mlp to obtain the ﬁnal query-history matching score ( , ) the loss function of each training sample for queryhistory matching task is deﬁned by lh q h+ h − log − , . , − nh eg q h+ eg q h+ + cid:80 δh j=1 eg q h ) where h+ stands for the true dialogue history for , h− is the j-th negative dialogue history randomly sampled from the training set and δh is the number of sampled dialogue history 3.3.3 multi-turn response matching the above two tasks are designed for empowering the model to knowledge or history comprehension and knowledge selection in this task we aim at training the model to match reasonable responses based on dialogue history and query since we treat the dialogue history as a special form of background knowledge and they share the same segment embeddings in the plms our model can acquire the ability to identify the proper response with either dialogue history or the background knowledge through the multi-turn response matching task speciﬁcally we format the multi-turn dialogues as query-history-response triples and requires the model to predict whether a response candidate = wr nr is appropriate for a given query nq and a concatenated dialogue = wq history sequence = wh . concretely we concatenate three input sequences into a single consecutive tokens sequence with sep tokens 1 . . wr , . , wq , . , wh nh shqr { cls , wh , . , wh nq [ sep , wr nh [ sep , nr 1 . . wr wq , . , wq 6 similarly we feed an embedding sequence of which each entry is a summation of token segment and position embeddings into bert finally we feed ehqr cls into a mlp to obtain the ﬁnal response matching score ( , , ) the loss function of each training sample for multi-turn response matching task is deﬁned by lr h q r+ r − log − , . , − δr eg h q r+ + cid:80 nr eg h q r+ i=j eg h q r − ) 7 j where r+ is the true response for a given and , r− is the j-th negative response candidate randomly sampled from the training set and δr is the number of negative response candidate 3.3.4 we adopt a multi-task learning manner and deﬁne the ﬁnal objective function as joint learning lfinal lp lh lr 8 ( ) in this way all tasks are jointly learned so that the model can effectively leverage two training 4450 corpus and learn to select relevant knowledge and distinguish the proper response 3.4 calculating matching score after learning model from dc and dp we ﬁrst rank pi nk i=1 according to ( , ki and then select top knowledge entries p1 . . pm for the subsequent response matching process here we design two strategies to compute the ﬁnal matching score ( , , , ) in the ﬁrst strategy we directly concatenate the selected knowledge and dialogue history as a long sequence of background knowledge and feed into the model to obtain the ﬁnal matching score which is formulated as g k h q r = ( p1 . . pm c q r where denotes the concatenation operation ( ) in the second strategy we treat each selected knowledge entry and the dialogue history equally as the background knowledge and compute the matching degree between each query background knowledge and the response candidates with the trained model consequently the matching score is deﬁned as an integration of a set of knowledgegrounded response matching scores formulated as g k h q r = ( , , ) max i∈ 0 m g pi q r ( 10 where is the number of selected knowledge entries we name our model with the two strategies as ptkgccat and ptkgcsep respectively we compare the two learning strategies through empirical studies as will be reported in the next section 4 experiments 4.1 datasets and evaluation metrics training set we adopt ms marco passage ranking dataset nguyen et al. 2016 built on bing s search for query-passage matching task the dataset contains 8.8m passages from web pages gathered from bing s results to real-world queries and each passage contains an average of 55 words each query is associated with sparse relevance judgments of one or very few passage marked as relevant the training set contains about 500k pairs of query and relevant passage and another 400m pairs of query and passages that have not been marked as relevant from which the negatives are sampled in our task for the query-dialogue history matching task and multi-turn response matching task we use the multi-turn dialogue corpus constructed from the reddit dziri et al. 2018 . the dataset contains more than 15 million dialogues and each dialogue has at least utterances after the pre-processing we randomly sample 2.28m/20k dialogues as the training/validation set for each dialogue session we regard the last turn as the response the last but one as the query and the rest as the positive dialogue history the negative dialogue histories are randomly sampled from the whole dialogue set on average each dialogue contains 4.3 utterances and the average length of the utterances is 42.5. test set we tested our proposed method on the wizard-of-wikipedia wow ( dinan et al. 2019 and cmu dog zhou et al. 2018a . both datasets contain multi-turn dialogues grounded on a set of background knowledge and are built with crowd-sourcing on amazon mechanical turk in wow the given knowledge collection is obtained from wikipedia and covers a wide range of topics or domains while in cmu dog the underlying knowledge focuses on the movie domain unlike cmu dog where the golden knowledge index for each turn is unknown the golden knowledge index for each turn is provided in wow two conﬁgurations e.g. test-seen and test-unseen are provided in wow following existing works dinan et al. 2019 zhao et al. 2019 , positive responses are true responses from humans and negative ones are randomly sampled the ratio between positive and negative responses is : 99 for wow and : 19 for cmu dog more details of the two benchmarks are shown in appendix a.1 evaluation metrics following previous works on knowledge-grounded response selection gu et al. 2020b zhao et al. 2019 , we also employ recall at rn k where = 100 for wow and = 20 for cmu dog and = 1 2 5 ) as the evaluation metrics implementation details 4.2 our model is implemented by pytorch paszke et al. 2019 . without loss of generality we select english uncased bertbase 110m as the matching model during the training the maximum lengths of the knowledge a.k.a. passage , the dialogue history the query and the response candidate were set to 128 120 60 and 40. intuitively the last tokens in the dialogue history and the previous 4451 models ir baseline bow memnet two-stage transformer transformer memnet dim gu et al. 2019 fire gu et al. 2020b ptkgccat ptkgcsep test seen test unseen @ r 2 @ r 1 @ r 5 17.8 71.3 84.2 87.4 83.1 88.3 85.7 89.5 14.2 33.1 63.1 69.8 60.3 68.3 65.5 69.6 77.8 84.5 82.0 85.8 91.1 95.3 94.6 96.7 95.7 97.7 98.2 98.9 92.3 95.1 94.7 96.3 models starspace wu et al. 2018 bow memnet zhang et al. 2018 kv proﬁle memory zhang et al. 2018 transformer memnet mazar´e et al. 2018 dgmn zhao et al. 2019 dim gu et al. 2019 fire gu et al. 2020b ptkgccat ptkgcsep @ r 2 @ 50.7 80.3 81.4 51.6 82.4 56.1 87.4 60.3 91.2 65.6 78.7 97.1 97.4 81.8 86.1 61.6 66.1 88.7 64.5 65.8 69.9 74.4 78.3 89.0 90.8 73.5 77.8 table : evaluation results on the test set of wow evaluation results on the test set of table : cmu dog tokens in the query and response candidate are more important so we cut off the previous tokens for the context but do the cut-off in the reverse direction for the query and response candidate if the sequences are longer than the maximum length we set a batch size of 32 for multi-turn response matching and query-dialogue history matching and for query-document matching in order to train these tasks jointly under the circumstance of training examples inequality we set δp 6 δh 1 and δr 12 for the query-passage matching the query-dialogue history matching and the multiturn response matching respectively particularly the negative dialogue histories are sampled from other training instances in a batch the model is optimized using adam optimizer with a learning rate set as 5e 6. the learning rate is scheduled by warmup and linear decay a dropout rate of 0.1 is applied for all linear transformation layers the gradient clipping threshold is set as 10.0. early stopping on the corresponding validation data is adopted as a regularization strategy during the testing we vary the number of selected knowledgeentries ∈ 1 . . 15 and set = for ptkgccat and set = 14 for ptkgcsep because they achieve the best performance 4.3 baselines since the characteristics of the two data sets are different only wow provides the golden knowledge label , we compare the proposed model with the baselines on both data sets individually baselines on wow 1 ir baseline dinan et al. 2019 uses simple word overlap for response selection 2 bow memnet dinan et al. 2019 is a memory network where knowledge entries are embedded via bag-of-words representation and the model learns the knowledge selection and response matching jointly 3 transformer memnet dinan et al. 2019 is an extension of bow memnet and the dialogue history response candidate and knowledge entries are encoded with transformer encoder vaswani et al. 2017 pre-trained on a large data set 4 two-stage transformer dinan et al. 2019 trains two separately models for knowledge selection and response retrieval respectively a best-performing model on the knowledge selection task is used for the dialogue retrieval task baselines on cmu dog ) starspace wu et al. 2018 selects the response by the cosine similarity between a concatenated sequence of dialogue context knowledge and the response candidate represented by starspace wu et al. 2018 ; ) bow memnet zhang et al. 2018 is a memory network with the bag-of-words representation of knowledge entries as the memory items 3 kv proﬁle memory zhang et al. 2018 is a key-value memory network grounded on knowledge proﬁles 4 transformer memnet mazar´e et al. 2018 is similar to bow memnet and all utterances are encoded with a pre-trained transformer 5 dgmn zhao et al. 2019 lets the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention 6 dim gu et al. 2019 is similar to dgmn and all utterance are encoded with bilstms 7 fire gu et al. 2020b ﬁrst ﬁlters the context and knowledge and then use the ﬁltered context and knowledge to perform the iterative response matching process 4.4 evaluation results performance of response selection table and table report the evaluation results of response selection on wow and cmu dog where ptkgccat and ptkgcsep represent the ﬁnal matching score computed with the ﬁrst strategy equation ) and the second strategy equation 10 respectively we can see that ptkgcsep is 4452 models ptkgcsep ptkgcsep q ptkgcsep q+h ptkgcsep q+k ptkgcsep m=1 ptkgcsep m=1 lp ptkgcsep m=1 lh wizard of wikipedia test seen test unseen cmu dog @ 89.5 70.6 84.9 89.5 85.6 84.7 84.9 @ 96.7 79.7 93.9 96.4 94.4 93.5 93.7 @ 98.9 86.8 97.8 98.6 97.9 97.5 97.6 @ 69.6 55.9 64.9 67.0 66.7 63.4 65.5 @ 85.8 70.8 81.7 84.0 82.8 80.5 81.7 @ 96.3 83.4 94.3 96.0 94.3 94.0 94.1 @ 66.1 47.3 59.5 62.7 60.4 58.7 59.4 @ 77.8 58.8 72.3 73.8 72.5 70.8 71.4 @ 88.7 75.0 86.1 84.8 86.0 85.6 85.3 table : ablation study models random ir baseline bow memnet transformer transformer w/ pretrain our model our model lp our model lh wizard seen wizard unseen @ r 2 @ r 1 @ r 5 2.7 5.8 23.0 22.5 25.5 22.0 12.8 21.2 2.3 7.6 8.9 12.2 22.9 23.1 13.3 22.7 32.1 23.3 31.2 50.7 45.5 49.2 48.8 45.2 47.6 31.2 22.6 29.9 table : the performance of knowledge selection on the test sets of wow data all baselines come from dinan et al ( 2019 . the details for all baselines are shown in appendix a.2 consistently better than ptkgccat over all metrics on two data sets demonstrating that individually representing each knowledge-query-response triple with bert can lead to a more optimal matching signal than representing a single long sequence our explanation to the phenomenon is that there is information loss when a long sequence composed of the knowledge and dialogue history passes through the deep architecture of bert thus the earlier different knowledge entries and dialogue history are fused together the more information of dialogue history or background knowledge will be lost in matching particularly on the wow in terms of @ , our ptkgcsep achieves a comparable performance with the existing stateof-the-art models that are learned from the crowdsourced training set indicating that the model can effectively learn how to leverage external knowledge feed for response selection through the proposed pre-training approach notably we can observe that our ptkgcsep performs worse than dim and fire on the cmu dog our explanation to the phenomenon is that the dialogue and knowledge in cmu dog focus on the movie domain while our train data including ad-hoc retrieval corpora and multi-turn dialogues come from the open domain thus our model may not select proper knowledge entries and can not well recognize the semantics clues for response matching due to the domain shift despite this ptkgcsep can still show better performance than several existing models such as transformer memnet and dgmn though ptkgcsep does not access any training examples in the benchmarks performance of knowledge selection we also assess the ability of models to predict the knowledge selected by human wizards in wow data the results are shown in table 4. we can ﬁnd that the performance of our method is comparable with various supervised methods trained on the gold knowledge index in particular on the testseen our model is slightly worse than transformer w/ pretrain , while on the test-unseen our model achieves slightly better results the results demonstrate the advantages of our pretraining tasks and the good generalization ability of our model 4.5 discussions ablation study we conduct a comprehensive ablation study to investigate the impact of different inputs and different tasks first we remove the dialogue history knowledge and both of them from the model which is denoted as ptkgcsep q+k , ptkgcsep q+h and ptkgcsep q respectively according to the results of the ﬁrst four rows in table , we can ﬁnd that both the dialogue history and knowledge are crucial for response selection as removing anyone will generally cause a performance drop on the two data besides the background knowledge is more critical for response selection as removing the background knowledge causes more signiﬁcant performance degradation than removing the dialogue history then we remove each training task individually from ptkgcsep and denote the models 4453 models ptkgcsep q+h ptkgcsep q+h -lh ptkgcsep q+h -lp ptkgcsep q+h -lh-lp wizard seen wizard unseen @ r 2 @ r 1 @ r 5 94.3 84.9 93.8 84.1 93.5 83.4 83.2 93.8 81.7 81.9 80.2 80.1 64.9 64.3 60.9 60.9 93.9 93.7 93.5 93.8 97.8 97.7 97.9 97.6 table : ablation study of our model without considering the grounded knowledge as ptkgcsep-x where ∈ lp lh meaning query-passage matching task and query-dialogue history matching task respectively table shows the ablation results of knowledge selection we can ﬁnd that both tasks are useful in the learning of knowledge selection and query-passage matching plays a dominant role since the performance of knowledge selection drops dramatically when the task is removed from the pre-training process the last two rows in table show the ablation results of response selection we report the ablation results when only knowledge is provided since the knowledge recalls for different ablated models and the full model are very close when is large m 14 . we can see that both tasks are helpful and the performance of response selection drops more when removing the query-passage matching task particularly lp plays a more important role and the performance on test-unseen of wow drops more obvious when removing each training task to further investigate the impact of our pretraining tasks on the performance of the multiturn response selection without considering the grounded knowledge , we conduct an ablation study and the results are shown in table 5. we can observe that the performance of the response matching model no grounded knowledge drops obviously when removing one of the pretraining tasks or both tasks particularly the query-passage matching task contributes more to the response selection the impact of the number of selected knowledge we further study how the number of selected knowledge m inﬂuences the performance of ptkgcsep figure shows how the performance of our model changes with respect to different numbers of selected knowledge we observe that the performance increases monotonically until the knowledge number reaches a certain value and then stable when the number keeps increasing the results are rational because more knowledge entries can provide more useful figure : the performance of response selection across different number of selected knowledge information for response matching but when the knowledge becomes enough the noise will be brought to matching 5 conclusion in this paper we study response matching in knowledge-grounded conversations under a zeroresource setting in particular we propose decomposing the training of the knowledge-grounded response selection into three tasks and joint train all tasks in a uniﬁed pre-trained language model our model can be learned to select relevant knowledge and distinguish proper response with the help of ad-hoc retrieval corpora and amount of multiturn dialogues experimental results on two benchmarks indicate that our model achieves a comparable performance with several existing methods trained on crowd-sourced data in the future we would like to explore the ability of our proposed method in retrieval-augmented dialogues acknowledgement we would like to thank the anonymous reviewers for their constructive comments this work was supported by the national key research and development program of china no 2020yfb1406702 , science foundation of china nsfc no 61876196 and beijing outstanding young scientist program no bjjwzyjh012019100020098 . rui yan is the corresponding author and is supported as a young fellow at beijing academy of artiﬁcial intelligence baai . the national 4454 0.885 0.887 0.889 0.891 0.892 0.893 0.894 0.895 0.895 seen unseen 0.869 0.864 0.882 0.875 0.877 0.685 0.687 0.688 0.690 0.692 0.696 0.696 0.682 0.682 0.681 0.682 0.682 0.90 0.89 0.88 0.87 0.85 0.70 0.69 0.68 0.66 0.65 0.86 0.856 @ 0 r 0.675 0.672 0.67 0.667 2 4 12 the number of selected knowledge m 10 11 7 9 13 14 15 references jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirectional transformers for language underin proceedings of the 2019 conference standing of the north american chapter of the association for computational linguistics human language technologies pages 4171–4186 association for computational linguistics emily dinan stephen roller kurt shuster angela fan michael auli and jason weston 2019. wizard of wikipedia knowledge-powered conversational in international conference on learning agents representations nouha dziri ehsan kamalloo kory mathewson and osmar zaiane 2018. augmenting neural response generation with context-aware topical attention arxiv preprint arxiv:1811.01063. marjan ghazvininejad chris brockett ming-wei chang bill dolan jianfeng gao wen-tau yih and michel galley 2018. a knowledge-grounded neural in the thirty-second aaai conversation model conference on artiﬁcial intelligence pages 5110– 5117. jia-chen gu tianda li quan liu zhen-hua ling zhiming su si wei and xiaodan zhu 2020a speaker-aware bert for multi-turn response selection in proceedings of the in retrieval-based chatbots 29th acm international conference on information and knowledge management cikm 20 pages 2041–2044 acm jia-chen gu zhen-hua ling xiaodan zhu and quan liu 2019. dually interactive matching network for personalized response selection in retrieval-based chatbots in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 1845–1854 hong kong china jia-chen gu zhenhua ling quan liu zhigang chen and xiaodan zhu 2020b filtering before iteratively referring for knowledge-grounded response selection in retrieval-based chatbots in findings of the association for computational linguistics emnlp 2020 pages 1412–1422 online association for computational linguistics donna harman 2005. the trec ad hoc experiments matthew henderson paweł budzianowski i˜nigo casanueva sam coope daniela gerz girish kumar nikola mrkˇsi´c georgios spithourakis pei-hao su ivan vuli´c and tsung-hsien wen 2019. a repository of conversational datasets in proceedings of the first workshop on nlp for conversational ai pages 1–10 florence italy zongcheng ji zhengdong lu and hang li 2014. text an information retrieval approach to short conversation arxiv preprint arxiv:1408.6988. omar khattab and matei zaharia 2020. colbert efﬁcient and effective passage search via contextualized late interaction over bert in proceedings of the 43rd international acm sigir conference on research and development in information retrieval pages 39–48 jiwei li michel galley chris brockett jianfeng gao and bill dolan 2016. a diversity-promoting objective function for neural conversation models in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies pages 110–119 san diego california association for computational linguistics linxiao li can xu wei wu yufan zhao xueliang zhao and chongyang tao 2020. zero-resource knowledge-grounded dialogue generation in proceedings of the 34th conference on neural information processing systems ryan lowe nissan pow iulian serban and joelle pineau 2015. the ubuntu dialogue corpus a large dataset for research in unstructured multiin proceedings of the 16th turn dialogue systems annual meeting of the special interest group on discourse and dialogue pages 285–294 prague czech republic association for computational linguistics pierre-emmanuel mazar´e samuel humeau martin training raison and antoine bordes 2018. millions of personalized dialogue agents in proceedings of the 2018 conference on empirical methods in natural language processing pages 2775–2779 brussels belgium association for computational linguistics tri nguyen mir rosenberg xia song jianfeng gao saurabh tiwary rangan majumder and li deng 2016. ms marco a human generated machine reading comprehension dataset in coco nips adam paszke sam gross francisco massa adam lerer james bradbury gregory chanan trevor killeen zeming lin natalia gimelshein luca pytorch an imperative antiga et al 2019. style high-performance deep learning library in advances in neural information processing systems volume 32. curran associates inc. iulian vlad serban alessandro sordoni yoshua bengio aaron courville and joelle pineau 2016. building end-to-end dialogue systems using generative hierarchical neural network models in proceedings of the thirtieth aaai conference on artiﬁcial intelligence volume 16 pages 3776–3784 chongyang tao wei wu can xu wenpeng hu dongyan zhao and rui yan 2019. one time of interaction may not be enough go deep with an interaction-over-interaction network for response in proceedings of the 57th selection in dialogues annual meeting of the association for computational linguistics pages 1–11 4455 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ł ukasz kaiser and illia polosukhin 2017. attention is in advances in neural information all you need processing systems volume 30. curran associates inc. zhilin yang zihang dai yiming yang jaime carbonell russ salakhutdinov and quoc le 2019. xlnet generalized autoregressive pretraining for language understanding in advances in neural information processing systems volume 32. curran associates inc. jesse vig and kalai ramea 2019. comparison of transfer-learning approaches for response selection in workshop on in multi-turn conversations dstc7 hao wang zhengdong lu hang li and enhong a dataset for research on shortchen 2013. in proceedings of the 2013 text conversations in natural conference on empirical methods language processing pages 935–945 association for computational linguistics mingxuan wang zhengdong lu hang li and qun liu 2015. syntax-based deep matching of short texts in ijcai pages 1354–1361 tsung-hsien wen david vandyke nikola mrkˇsi´c milica gaˇsi´c lina m. rojas-barahona pei-hao su stefan ultes and steve young 2017. a networkbased end-to-end trainable task-oriented dialogue in proceedings of the 15th conference of system the european chapter of the association for computational linguistics pages 438–449 association for computational linguistics taesun whang dongyub lee chanhee lee kisu yang dongsuk oh and heuiseok lim 2020. an effective domain adaptive post-training method for in proceedings of bert interspeech 2020 pages 1585–1589 in response selection ledell yu wu adam fisch sumit chopra keith adams antoine bordes and jason weston 2018. in thirty-second starspace embed all the things aaai conference on artiﬁcial intelligence pages 5569–5577 yonghui wu mike schuster zhifeng chen quoc v. le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner et al 2016. google s neural machine translation system bridging the gap between human and machine translation corr abs/1609.08144 yu wu wei wu chen xing ming zhou and zhoujun li 2017. sequential matching network a new architecture for multi-turn response selection in retrieval-based chatbots in proceedings of the 55th annual meeting of the association for computational linguistics pages 496–505 association for computational linguistics ruijian xu chongyang tao daxin jiang xueliang zhao dongyan zhao and rui yan 2020. learning an effective context-response matching model with self-supervised tasks for retrieval-based dialogues in proceedings of the thirty-fifth aaai conference on artiﬁcial intelligence chunyuan yuan wei zhou mingming li shangwen lv fuqing zhu jizhong han and songlin hu 2019. multi-hop selector network for multi-turn response selection in retrieval-based chatbots in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing pages 111–120 association for computational linguistics saizheng zhang emily dinan jack urbanek arthur szlam douwe kiela and jason weston 2018. personalizing dialogue agents i have a dog do in proceedings of the 56th you have pets too annual meeting of the association for computational linguistics pages 2204–2213 association for computational linguistics yizhe zhang siqi sun michel galley yen-chun chen chris brockett xiang gao jianfeng gao jingjing liu and bill dolan 2020. dialogpt largescale generative pre-training for conversational in proceedings of the 58th response generation annual meeting of the association for computational linguistics system demonstrations pages 270–278 online association for computational linguistics xueliang zhao chongyang tao wei wu can xu dongyan zhao and rui yan 2019. a documentgrounded matching network for response selection in proceedings of the in retrieval-based chatbots twenty-eighth international joint conference on artiﬁcial intelligence pages 5443–5449 xueliang zhao wei wu can xu chongyang tao dongyan zhao and rui yan 2020. knowledgegrounded dialogue generation with pre-trained the 2020 language models conference on empirical methods in natural language processing emnlp , pages 3377–3390 online association for computational linguistics in proceedings of kangyan zhou shrimai prabhumoye and alan black 2018a a dataset for document grounded in proceedings of the 2018 conferconversations ence on empirical methods in natural language processing pages 708–713 brussels belgium association for computational linguistics xiangyang zhou lu li daxiang dong yi liu ying chen wayne xin zhao dianhai yu and hua wu 2018b multi-turn response selection for chatbots with deep attention matching network in proceedings of the 56th annual meeting of the association for computational linguistics pages 1118–1127 association for computational linguistics 4456 a appendices a.1 details of test sets statistics wizard of wikipedia cmu dog test seen test unseen avg # turns avg # words per turn avg # knowledge entries avg # words per knowledge 9.0 16.4 60.8 36.9 9.1 16.1 61.0 37.0 test 12.4 18.1 31.8 27.0 table : the statistics of test sets of two benchmarks we tested our proposed method on the wizardof-wikipedia wow ( dinan et al. 2019 and cmu dog zhou et al. 2018a . both datasets contain multi-turn dialogues grounded on a set of background knowledge and are built with crowdsourcing on amazon mechanical turk in the wow dataset one of the paired speakers is asked to play the role of a knowledgeable expert with access to the given knowledge collection obtained from wikipedia while the other of a curious learner the dataset consists of 968 complete knowledge-grounded dialogues for testing it is worth noting that the golden knowledge index for each turn is available in the dataset response selection is performed at every turn of a complete dialogue which results in 7512 for testing in total following the setting of the original paper positive responses are true responses from humans and negative ones are randomly sampled the ratio between positive and negative responses is : 99 in testing sets besides the test set is divided into two subsets test seen and test unseen the former shares 533 common topics with the training set while the latter contains 58 new topics uncovered by the training or validation set the cmu dog data contains knowledgegrounded human-human conversations where the underlying knowledge comes from wiki articles and focuses on the movie domain similar to dinan et al ( 2019 , the dataset was also built in two scenarios in the ﬁrst scenario only one worker can access the provided knowledge collections and he/she is responsible for introducing the movie to the other worker while in the second scenario both workers know the knowledge and they are asked to discuss the content different from wow the golden knowledge index for each turn is unknown for both scenarios since the data size for an individual scenario is small we merge the data of the two scenarios following the setting with zhao et al ( 2019 . finally there are 537 dialogues for testing we evaluate the performance of the response selection at every turn of a dialogue which results in 6637 samples for testing we adopted the version shared in zhao et al ( 2019 , where 19 negative candidates were randomly sampled for each utterance from the same set more details about the two benchmarks can be seen in table 6. a.2 baselines for knowledge selection to compare the performance of knowledge selection we choose the following baselines from dinan et al ( 2019 including 1 random the model randomly selects a knowledge entry from a set of knowledge entries ( ) ir baseline the model uses simple word overlap between the dialogue context and the knowledge entry to select the relevant knowledge ( ) bow memnet the model is based on memory network where each memory item is a bag-of-words representation of a knowledge entry and the gold knowledge labels for each turn are used to train the model ( ) transformer the model trains a context-knowledge matching network based on transformer architecture ( ) transformer w/ pretrain : the model is similar to the former model but the transformer is pre-trained on reddit data and ﬁne-tuned for the knowledge selection task a.3 results of low-resource setting ration t 0 10 50 100 wizard seen wizard unseen @ r 2 @ r 1 @ r 5 96.3 89.5 90.8 96.8 96.9 91.5 92.2 97.1 69.6 73.2 73.9 74.3 96.7 97.1 97.1 97.6 98.9 99.4 99.3 99.4 85.8 86.9 87.9 88.1 table : evaluation results of our model in the lowresource setting on the wizard of wikipedia data as an additional experiment we also evaluate the proposed model for a low-resource setting we randomly sample ∈ 10 , 50 , 100 } portion of training data from wow and use the data to ﬁnetune our model the results are shown in table 7. we can ﬁnd that with only 10 training data our model can signiﬁcantly outperform existing models indicating the advantages of our pretraining tasks with 100 training data our model can achieve 2.7 improvement in terms of @ on the test-seen and 4.7 improvement on the testunseen 4457 using local knowledge graph construction to scale seq2seq models to multi-document inputs angela fan fair loria claire gardent cnrs loria chlo´e braud cnrs loria antoine bordes fair angelafan abordes fb.com claire.gardent chloe.braud loria.fr abstract query-based open-domain nlp tasks require information synthesis from long and diverse web results current approaches extractively select portions of web text as input to sequence-to-sequence models using methods such as tf-idf ranking we propose constructing a local graph structured knowledge base for each query which compresses the web search information and reduces redundancy we show that by linearizing the graph into a structured input sequence models can encode the graph representations within a standard sequence-to-sequence setting for two generative tasks with very long text input long-form question answering and multidocument summarization feeding graph representations as input can achieve better performance than using retrieved text portions introduction effective information synthesis is at the core of many natural language processing applications such as open-domain question answering and multi-document summarization in such tasks a fundamental is the ability to distill relevant knowledge from hundreds of thousands of tokens of noisy and redundant input such as webpages current approaches predominantly conduct tf-idf-based information extraction to identify key portions of the information and then provide this as sequential input to a sequence-to-sequence seq2seq model the sub-selected portions are limited to a few thousand words as models often struggle to encode much longer sequences in this work we propose a mechanism to restructure free text into local knowledge graphs that are then linearized into sequences creating a canonical form in which information is presented to models by constructing a graph intermediary redundant information can be merged or discarded producing substantially compressed input figure : multi-document input to linearized graph multi-document input resulting from web search queries are converted to a graph structured knowledge base using coreference resolution and information extraction then linearized into a sequence for seq2seq models color indicates coreference resolution node weight is indicated by circle radius and edge weight by line thickness — small enough to be fully encoded by seq2seq models such a method can be seen as merging previous work on symbolic knowledge bases for information extraction with newer approaches using deep neural networks to encode knowledge our approach shown in figure , takes a query and its corresponding multi-document web search results and builds for each query a speciﬁc local knowledge graph we present several modeling contributions to effectively encode the entire graph as a sequence and attend to the most relevant portions within this linearization we demonstrate the effectiveness of this approach on two large 4186 proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing pages 4186–4196 hong kong china november 3–7 2019. ( ) 2019 association for computational linguistics scale generative tasks with both long and noisy multi-document web input and paragraph length output long-form question answering on the eli5 dataset fan et al. 2019 and wikipedia lead paragraph generation as a multi-document summarization problem liu et al. 2018b . related work interest in generative sequence modeling has intensiﬁed due to recent improvements peters et al. 2018 devlin et al. 2018 radford et al. 2019 , making the of information synthesis more relevant in contrast to extractive tasks which only require models to identify spans and can do so effectively on long documents by looking at the paragraphs independently generative sequence models must combine multiple pieces of evidence from long and noisy multi-document input to generate correct and convincing responses 2.1 multi-document input previous work in multi-document summarization barzilay et al. 1999 applies various techniques to handle long input including clustering to ﬁnd similar information honarpisheh et al. 2008 , extractive methods to select relevant sentences daum´e iii and marcu 2002 gillick and favre 2009 berg-kirkpatrick et al. 2011 di fabbrizio et al. 2014 bing et al. 2015 cao et al. 2017 including maximal marginal relevance fabbri et al. 2019 , and incorporating queries baumel et al. 2018 and graphs ganesan et al. 2010 yasunaga et al. 2017 . however there are few large scale multi-document summarization datasets and many approaches have focused on extractive selection or hybrid extractive-abstractive models in this work we use graph construction to re-structure multidocument input for abstractive generation advancements in question answering have examined performance on datasets with multidocument input such as triviaqa joshi et al. 2017 . various approaches have been proposed including leveraging tf-idf and bigram hashing with an rnn to ﬁnd relevant information chen et al. 2017 , models that score individual paragraphs for sub-selection clark and gardner 2017 , and nearest neighbor search with paragraph re-ranking das et al. 2018a . however these approaches have been applied to extractive question answering tasks that require span identiﬁcation rather than abstractive text generation in an information synthesis setting 2.2 using knowledge bases previous work has explored various ways of representing information in knowledge bases bordes et al. 2011 and improving these representations chen et al. 2013 . knowledge bases have been leveraged to improve performance on various tasks from coreference resolution ng and cardie 2002 and question answering zheng 2003 bao et al. 2014 cui et al. 2017 sun et al. 2018 to signal processing b¨uckner et al. 2002 . various works convert text into abstract meaning representations liu et al. 2018a for domains such as news vossen et al. 2015 rospocher et al. 2016 and link nodes to large knowledge bases such as dbpedia auer et al. 2007 . wities et al ( 2017 combine open information extraction with coreference and lexical inference to build knowledge representations they apply this to tweets and analyze the accuracy on various aspects of graph construction das et al ( 2018b construct graphs from procedural text to track entity position to answer when and if entities are created destroyed or moved in contrast we build graphs from substantially longer multi-document input and use them for multi-sentence text generation recently many have explored neural architectures that can encode graph structured input bruna et al. 2013 kipf and welling 2016 beck et al. 2018 zhou et al. 2018 xu et al. 2018 lai et al. 2019 . these models often represent graphs as adjacency matrices to generalize architectures such as convolutional networks to graph inputs rather than encoding a static knowledge graph or leveraging external knowledge graphs we build a local graph for each query and model these using standard seq2seq models we leave the incorporation of graph networks for future work 3 graph construction we describe how symbolic graph representations of knowledge can be constructed from text our approach assumes a multi-document input such as web pages resulting from the execution of a query the graph construction process 1 compresses the web search input to a signiﬁcantly smaller size allowing models to encode the entirety of the compression and 2 reduces redundancy through merge operations allowing relevant information to be more easily identiﬁed 4187 tions . edges are merged similarly with existing edges between the same two nodes such merge operations allow strings such as the nobel prize and nobel prize to be represented as one node rather than separately similarly coreference resolution aids in merging by identifying that albert einstein and he refer to the same entity and thus merging them the construction of the graph reduces redundancy the size of the graph can be modiﬁed by controlling which triples are added using tf-idf overlap see figure , step ) tfidf overlap of the triple with the question can be used to determine if the triple contains relevant information this improves robustness to noisy web search input and helps ﬁlter entirely irrelevant portions such as scraped html tags 4 modeling graphs as sequences current models for text generation often use seq2seq architectures such as the transformer vaswani et al. 2017 . these models are designed to encode sequences rather than graphs we describe now how to convert a graph into a structured input sequence our complete model will take as input a linearized graph by encoding graph attributes such as node and edge weight as embeddings we add hierarchical and memory-compressed attention mechanisms to scale seq2seq models to encode the full graph and attend to the most relevant information within it figure ) and ﬁnally we integrate the beneﬁts of language modeling using multi-task training for example 4.1 graph to sequence linearization to represent the graph as a sequence for seq2seq it is linearized into a structured standard form of subject node object node and predicate edge separated by indicator tokens as shown in figure 1. the linearization sub albert einstein obj the nobel prize pred won would be created the linearization is accomplished through graph traversal in a breadth-ﬁrst manner following the directed edges formed by predicates and starting from the node with the largest weight as the root for two nodes that are connected by multiple predicates the predicates are concatenated shown in figure ) so a linearization such as pred won s received would indicate that albert einstein both won and received the nobel prize figure : steps of graph construction color relates the document sentence used to produce the graph output text to triples to graph graph construction proceeds in several steps outlined in figure 2. we apply coreference resolution clark and manning 2016a b 1 and open information extraction stanovsky et al. 2018 2 to convert sentences into a triple of the form subject predicate object . the sentence albert einstein a german theoretical physicist won the nobel prize would become albert einstein won the nobel prize . a graph is constructed using the triples by representing subjects and objects as nodes connected by predicates as directed edges for example the triple would become albert einstein −−→ the nowon bel prize nodes and edges have a name property that is the text they represent they also have a weight property that denotes the number of times that node or edge has appeared for example in figure , the node with name albert einstein has weight and edge with name won has weight 2. merging nodes and edges when subsequent triples are added to the graph they are merged with the existing graph if they already exist to reduce information replication to merge nodes the tf-idf overlap of the new node s name is calculated with the existing graph node names and the new node is merged into an existing node if the tf-idf is higher than some threshold see figure , steps and for example merge opera 1we use the implementation available here https // github.com/huggingface/neuralcoref available implementation 2we use the : //github.com/gabrielstanovsky/ supervised-oie here 4188 figure : graph attribute embeddings in addition to word and position embeddings models receive a graph weight embedding to encode node and edge weight and a query relevance embedding that encodes search result rank t mechanism for models to scale the graph embeddings we denote the embedding for position as et such that eword is the word embedding for the gw embedding models learn a gating function based on the word and gw embeddings such a mechanism provides capacity for the model to when the additional embeddings are useful based on the words in the input the gate is calculated by applying an mlp to the concatenation of the word and gw embeddings the learned gate is then applied to gw embeddings to create the output : ggw = [ egw hgw = ggw t egw t eword models learn a gating mechanism for the qr embedding in a similar manner the full embedding the model receives is as follows t epos eword + hgw ; hqr t 4.2 hierarchical attention one in modeling long sequences is that attention mechanisms struggle to make sharp selections when softmax-ing over long sequences fan et al. 2018 . when attention is blurry there lacks a strong distinction between noise and relevant information we assume that graphs are constructed from query-based web search input and thus leverage this query to learn a subselection operation using hierarchical top-k attention depicted in figure 4. the query is encoded with a transformer encoder and the linearized graph with another transformer encoder we model the interaction between the query and the input sequence e.g web search results or linearized graph by computing an attention distribution between the question and the input then selecting the top positions with the most attention weight such a mechanism can be thought of as building a query-dependent representation of the most relevant knowledge which is commonly done in question answering architectures like bidaf seo et al. 2017 . the top figure : model architecture gray indicates standard transformer elements green indicates modiﬁcation encoding graph information transformer seq2seq models have two embeddings a word embedding and a position embedding simply linearizing the graph however loses attribute information such as node and edge weight instead we encode these attributes as embeddings in addition to the word and position embeddings to represent graph weight gw , node and edge weight is provided as an embedding for each token the node weight and edge weight are equivalent to the number of merge operations 1. for example if albert einstein occurred times in the text the gw embedding for the tokens albert and einstein would be , as shown in figure 3. we encode a query relevance qr embedding to represent the relevance of the web search to the query as ranked by the information retrieval system e.g search engine . information from the top web search results is likely more relevant than information from the last web search results so providing this embedding allows the model to distinguish between these different information sources in figure , tokens representing sentences from the ﬁrst document have qr embedding , and tokens from the second document have value 2. models now have access to several different types of embeddings but all embedding information contributes equally as there is no mechanism to distinguish between them we introduce a 4189 operation limits the number of tokens making the attention mechanism sharper 4.3 scaling to encode the graph recent progress has improved the ability of language models to process longer sequences sukhbaatar et al. 2019 dai et al. 2019 , but models remain limited in their capacity to encode long documents the multi-document results of query-based web search have hundreds of thousands of tokens beyond the limit of current seq2seq models to handle for example the eli5 dataset provides an average of 200k tokens of web search input however by compressing the web search results into a knowledge graph we significantly reduce the number of tokens by an order of magnitude and make it possible for a model to access the entirety of the search information to represent the full graph models must scale to encode around 10k input tokens the attention mechanism in transformer architectures becomes computationally expensive for sequences of this length instead we experiment with the memorycompressed attention mca mechanism proposed for language models in liu et al. 2018b 3 and apply it to the encoder side of seq2seq models at each self-attention layer mca alternates between 1 local attention computed between smaller chunks of tokens and 2 strided convolutions to reduce the number of keys and values used in attention computation by adding the mca mechanism to the encoder e-mca , we are able to encode the complete linearized graph 4.4 multi-tasking with kb completion fan et al ( 2019 used multi-task training on language modeling and various seq2seq tasks to incorporate the beneﬁts of language modeling in seq2seq models we extend this by training additionally on knowledge graph completion models receive at training time sequences of a linearized graph with nodes edges or both selectively masked and must predict the missing content words for example models might receive as input sub albert einstein obj [ mask < pred won and need to predict the nobel prize this can be seen as both a multi-word extension of the masked language model training proposed in devlin et al. 2018 and as learning the task of 3in liu et al. 2018b , the mechanism is termed dmca as it is applied on the decoder side knowledge base completion lacroix et al. 2018 bordes et al. 2011 . at training time the tasks are distinguished using an indicator token in the input 5 experimental setup we evaluate our approach on two datasets with multi-document web input and multi-sentence abstractive output we use seq2seq models that leverage a query concatenated with web search results that have been processed into a supporting document e.g tf-idf subselection linearized graph etc — to generate long output 5.1 datasets and evaluation eli5 first we experiment with the explain like ’ five eli5 ( fan et al. 2019 question answering dataset of 270k complex questions paired with multi-sentence explanatory answers 130 words on average . to facilitate question answering the dataset provides the top 100 web search hits from querying the question which results in 200k words on average see appendix for examples previous work fan et al. 2019 used tf-idf to ﬁnd sentences in the web search that have the largest overlap with the question and created a tfidf extraction of about 850 words as input for seq2seq models instead we construct a local knowledge graph for each question from the 100 web search hits following the average length of the tf-idf support document constructed in fan et al. 2019 , we experiment with modeling the ﬁrst = 850 tokens of the linearized graph then scale to encode the entire graph using e-mca wikisum second we experiment on the wikisum commoncrawl liu et al. 2018b summarization dataset4 with 1.5 million examples this task formulates wikipedia lead paragraph generation as a multi-document summarization problem where the paragraph must be generated using the cited article references and other queried content from web search the query used is the title of the wikipedia article see appendix for examples previous work liu et al. 2018b applied tfidf ranking to order the paragraphs of web search given a query models receive the reordered paragraphs ranked by tf-idf as input liu et al ( 2018b model the ﬁrst = 500 words of this re-ranking and then = 11 000 using : //github.com/tensorflow/ tensor2tensor/tree/master/tensor2tensor/ data_generators/wikisum 4190 mca we construct the knowledge graph for each wikipedia article from the ﬁrst 200k words of the ranked web search results5 and experiment with encoding 500 and 11 000 tokens evaluation both tasks evaluate the multisentence generation output against the gold output using f1 rouge on wikisum we report only rouge-l following liu et al. 2018b . we conduct a comparative human evaluation on the eli5 dataset we use crowdworkers to examine the responses of two models on 300 different questions from the test set for each question 3 evaluators are shown two answers and asked to choose the one they prefer to reduce variance answers are standardized at 150 words each 5.2 training and generation to reduce the vocabulary size of varied web document content we apply byte-pair encoding sennrich et al. 2016 to generate 40k codes for each dataset we implement our models in fairseqpy ott et al. 2019 using the transformer big architecture and training schedule described in vaswani et al. 2017 . detailed parameters are listed in the appendix for generation we use beam search with beam size and tune a minimum and maximum length on the validation set 5.3 baselines we compare our results to the transformer sequence models presented in fan et al. 2019 for eli5 and liu et al. 2018b for wikisum we evaluate three additional baseline models • sentence selection with maximal marginal relevance ( fan et al. 2019 used tf-idf to identify relevant sentences in the web documents to form a support document of around 850 words however recent work fabbri et al. 2019 has shown that using maximal marginal relevance is an effective strategy for selecting relevant information while reducing redundancy we explore using mmr to select sentences from the web text to concatenate to form a support document • seq2seq multi-task triples to examine the impact of solely restructuring the input into open ie triples but not leveraging graph 5average length of provided web input is around 50k words and maximum length is around 900k words model + to a , tf-idf + to a mmr multi-task multi-task triples multi-task top20 trip q d to a graph multi-task graph top-100 attention e-mca input length avg 850 avg 850 avg 850 850 avg 570 850 850 850 11k rouge 5.1 5.0 5.4 5.2 5.3 5.3 5.6 5.7 5.8 22.8 22.9 23.1 23.2 23.2 23.3 23.6 23.8 24.0 28.3 28.1 28.9 29.0 28.8 28.8 29.5 29.7 30.0 table : answer generation on eli5 using seq2seq models receiving the question and a support document e.g tf-idf selection triples linearized graph to produce the answer * denotes results from fan et al. 2019 . model + to * lm d-mca t d to multi-task multi-task graph top-100 attention e-mca lm d-mca moe-256 inputlen rouge-l 500 11k 500 500 500 500 11k 7.5k 34.2 36.2 33.8 34.4 34.9 35.2 36.5 38.8 table : lead paragraph generation on wikisum commoncrawl using seq2seq models receiving the title and a support document e.g tf-idf ranking linearized graph to produce the lead paragraph * denotes results from liu et al. 2018b that use data scraped from unrestricted web search not the static commoncrawl version construction to reduce redundancy we experiment with a triples only baseline the triples are concatenated to form the input • seq2seq multi-task top 20 triples as an alternative to using graph construction to compress the full set of open ie triples we explore using tf-idf overlap with the query to ﬁnd the most relevant information we select the top 20 triples to concatenate as input 6 results we examine the performance of our proposed approach and the choices made in graph construction and modeling we analyze the quality of the compression created by graph construction and the robustness and interpretability of this process 6.1 linearized graph improves performance in table , we compare our methods to various baselines on the eli5 dataset using mmr to select the most relevant non-redundant input is similar to the tf-idf baseline from fan et al ( 2019 . 4191 the seq2seq multi-task triples baseline standardizes the input by forming triples but does not remove redundant triples it produces marginally better results compared to the baseline multi-task model sub-selecting to the top 20 triples is harmful as similar text has high tf-idf overlap with the query so redundant information is selected creating the graph structure brings an improvement of around 0.6 rouge-1 similar trends are seen for the wikisum dataset in table , where graph construction improves the multi-task model by 0.5 rouge-1 these improvements are statistically signiﬁcant at the 95 conﬁdence level for both datasets a further improvement is seen by using the hierarchical attention mechanism to attend to only the most relevant information in the linearized graph input for eli5 it brings an additional 0.2 rouge-1 improvement and on wikisum a 0.3 rouge-1 improvement by using mca to scale seq2seq models to encode the entire graph further gains can be seen particularly in information synthesis tasks prior work has shown the importance of reading more information liu et al ( 2018b achieved a 2-point rouge improvement by reading 11k tokens instead of 500. in our setting e-mca improves our results around 0.3 rouge on eli5 and 1.3 rouge on wikisum we display random generations from both datasets in the appendix we use human evaluation to compare the multitask baseline to the multi-task graph top-k attention model 57.4 of evaluations prefer the multi-task graph top-k attention model we conduct a two-tailed binomial test and ﬁnd this result is statistically signiﬁcant with = 0.003 6.2 analysis of modeling choices ablation on model components table ( a sequentially removes the graph embeddings the knowledge-base completion multi-tasking and the multi-tasking from fan et al. 2019 and reveals that each of these is important for performance graph attribute embeddings table ( ) displays the effect of removing the graph attribute embeddings and gating mechanism removing each is slightly harmful and the combination of all three together provide the best performance more web search documents figure ( right shows that graph construction with more web rouge-1 model a iterative removal of model components multi-task graph graph embeddings kb completion multi-tasking lm multi-tasking from fan et al. 2019 ( ) removing graph embedding components graph gated graph weight query relevance no graph weight embedding no query relevance embedding no gating c varying number of hits in graph multi-task graph top-k attention e-mca with graph on search hits with graph on 10 search hits with graph on 50 search hits with graph on 100 search hits d varying in hierarchical top-k atttention multi-task graph e-mca top-k 50 top-k 100 top-k 250 top-k 500 top-k 1000 29.4 29.1 28.9 28.4 28.6 28.4 28.3 28.4 28.8 29.3 29.6 29.9 29.1 29.5 29.4 29.3 29.2 table : ablations on the eli5 validation set model seq2seq + to a seq2seq + to a shufﬂe seq2seq + to a graph web shufﬂe input tf-idf web web shufﬂe web shufﬂe rouge-1 28.3 25.9 24.4 28.5 28.2 table : importance of web search relevance on validation for eli5 modeling 850 input words search information is important for answer token coverage the graph on the top search hit alone is missing 64 of the answer tokens but this decreases as more search hits are used table ( ) indicates that this lack of coverage of the answer tokens correlates with generation quality models receiving a graph built on the ﬁrst search hits alone are substantially worse than all 100 hits top-k attention table ( ) shows the effect of the top-k hierarchical attention mechanism for various values of k. attending to too many tokens lowers rouge for the eli5 task of writing approximately 130 word answers attending to 1000 input tokens likely means the model is focusing on irrelevant information and 50 tokens is too few 4192 figure : interpretable attention of seq2seq models on a subgraph when answering a question in eli5 figure : left distribution of number of nodes ( middle number of edges ( right weight of the largest node in graph construction on the eli5 training set search results is only missing 8.7 of tokens but has around 10k words when analyzing just the ﬁrst 850 tokens to match the average length of the tf-idf extraction the graph is better only missing 35 of tokens . further the merging and discarding operations done during graph construction do not have a large effect on answer token coverage the full set of triples marginally reduces the percentage of answer tokens missing to 7.3 instead of 8.7 . this indicates that much of the information in the full set triples is redundant and unnecessary for good answer token coverage 6.4 graph representation is more robust to poor search relevance ordering we analyze the robustness of our approach to the ordering of web search results in table 4. instead of constructing the graph from the ﬁrst web search result to the last we shufﬂe the web search results and construct the graph on this shufﬂed input we compare this to modeling the web search results directly no tf-idf retrieval and a model that receives this shufﬂed web search input the graph is more robust to shufﬂing as more information can be encoded in the graph due to its compression effect the search hit ordering is less critical figure : left graph construction drastically reduces input size by an order of magnitude ( right graph construction encodes more tokens present in the answer compared to tfidf extraction and building the graph from more search hits increases answer token coverage analysis on eli5 for both plots 6.3 graph improves answer token coverage despite compression figure displays the distribution of the number of nodes edges and the largest node weight for each local graph built on the eli5 dataset the 100 web search results are compressed to a few hundred nodes by merging redundancy and trimming irrelevant triples from the graph the input is reduced by an order of magnitude figure , left . despite compression the graph retains more answer tokens than tf-idf subselection figure ( right displays the percentage of answer tokens not present in the input the tf-idf extraction from fan et al. 2019 is missing 38 of tokens the graph constructed on all 100 web 4193 interpretable attention on subgraphs 6.5 figure shows an example of the nodes and edges the model focuses upon most when answering a question on eli5 to construct this visualization we calculate the top nodes the model attends to and then their top edges the model attention on a sub-portion of the linearized input can be visualized as an interpretable graph that corresponds well to the model s generated answer for example the relationship general relativity −→ is einstein s theory becomes the generated sentence general relativity is a theory of albert einstein 7 conclusion many open-domain nlp tasks rely upon multidocument input from the web to facilitate tasks such as answering questions or writing summaries but current approaches struggle to encode the entirely of this information we propose constructing one knowledge graph per query and show that this method compresses information and reduces redundancy we show on two abstractive generation tasks that using the linearized graph achieves better performance than tf-idf retrieval references s¨oren auer christian bizer georgi kobilarov jens lehmann richard cyganiak and zachary ives 2007. dbpedia a nucleus for a web of open data in the semantic web pages 722–735 springer junwei bao nan duan ming zhou and tiejun zhao 2014. knowledge-based question answering as main proceedings of the 52nd anchine translation nual meeting of the association for computational linguistics volume : long papers , volume , pages 967–976 regina barzilay kathleen mckeown and michael elhadad 1999. information fusion in the context of multi-document summarization in proceedings of the 37th annual meeting of the association for computational linguistics tal baumel matan eyal and michael elhadad 2018. query focused abstractive summarization incorporating query relevance multi-document coverage and summary length constraints into seq2seq models arxiv preprint arxiv:1801.07704. daniel beck gholamreza haffari and trevor cohn 2018. graph-to-sequence learning using arxiv preprint gated graph neural networks arxiv:1806.09835. taylor berg-kirkpatrick dan gillick and dan klein 2011. jointly learning to extract and compress in proceedings of the 49th annual meeting of the association for computational linguistics human language technologies-volume , pages 481–490 association for computational linguistics lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau 2015. abstractive multidocument summarization via phrase selection and merging arxiv preprint arxiv:1506.01597. antoine bordes jason weston ronan collobert and yoshua bengio 2011. learning structured embedin twenty-fifth aaai dings of knowledge bases conference on artiﬁcial intelligence joan bruna wojciech zaremba arthur szlam and yann lecun 2013. spectral networks and locally connected networks on graphs arxiv preprint arxiv:1312.6203. j¨urgen b¨uckner martin pahl o stahlhut and c-e liedtke 2002. a knowledge-based system for context dependent evaluation of remote sensing data in joint pattern recognition symposium pages 58–65 springer ziqiang cao wenjie li sujian li and furu wei 2017. improving multi-document summarization via text in thirty-first aaai conference on classiﬁcation artiﬁcial intelligence danqi chen adam fisch jason weston and antoine bordes 2017. reading wikipedia to answer opendomain questions in acl danqi chen richard socher christopher manning and andrew ng 2013. learning new facts from knowledge bases with neural tensor networks and semantic word vectors arxiv preprint arxiv:1301.3618. christopher clark and matt gardner 2017. simple and effective multi-paragraph reading comprehension arxiv preprint arxiv:1710.10723. kevin clark and christopher manning 2016a deep reinforcement learning for mention-ranking coreference models arxiv preprint arxiv:1609.08667. kevin clark and christopher manning 2016b improving coreference resolution by learning entityarxiv preprint level distributed representations arxiv:1606.01323. wanyun cui yanghua xiao haixun wang yangqiu song seung-won hwang and wei wang 2017. kbqa learning question answering over qa corpora and knowledge bases proceedings of the vldb endowment 10 5 :565–576 zihang dai zhilin yang yiming yang william cohen jaime carbonell quoc le and ruslan salakhutdinov 2019. transformer-xl attentive language models beyond a ﬁxed-length context arxiv preprint arxiv:1901.02860 4194 rajarshi das shehzaad dhuliawala manzil zaheer and andrew mccallum 2018a multistep retriever-reader interaction for scalable opendomain question answering rajarshi das tsendsuren munkhdalai xingdi yuan adam trischler and andrew mccallum 2018b building dynamic knowledge graphs from text using machine reading comprehension arxiv preprint arxiv:1810.05682. daum´e iii and daniel marcu 2002. a noisychannel model for document compression in proceedings of the 40th annual meeting on association for computational linguistics pages 449–456 association for computational linguistics jacob devlin ming-wei chang kenton lee and kristina toutanova 2018. bert pre-training of deep bidirectional transformers for language understanding corr abs/1810.04805 giuseppe di fabbrizio amanda stent and robert gaizauskas 2014. a hybrid approach to multidocument summarization of opinions in reviews in proceedings of the 8th international natural language generation conference inlg , pages 54–63 alexander fabbri irene li tianwei she suyi li and dragomir radev 2019. multi-news a large-scale multi-document summarization dataset and abstractive hierarchical model arxiv preprint arxiv:1906.01749. angela fan yacine jernite ethan perez david grangier jason weston and michael auli 2019. eli5 in proceedings of long form question answering acl 2019. angela fan mike lewis and yann dauphin 2018. hierarchical neural story generation arxiv preprint arxiv:1805.04833. kavita ganesan chengxiang zhai and jiawei han 2010. opinosis a graph based approach to abstractive summarization of highly redundant opinions in proceedings of the 23rd international conference on computational linguistics coling 2010 , pages 340–348 dan gillick and benoit favre 2009. a scalable global in proceedings of the model for summarization workshop on integer linear programming for natural langauge processing pages 10–18 association for computational linguistics mohamad ali honarpisheh gholamreza ghassemsani and ghassem mirroshandel 2008. a multidocument multi-lingual automatic summarization in proceedings of the third international system joint conference on natural language processing volume-ii mandar joshi eunsol choi daniel weld and luke zettlemoyer 2017. triviaqa a large scale distantly supervised dataset for reading comprehension in acl thomas kipf and max welling 2016. semisupervised classiﬁcation with graph convolutional networks arxiv preprint arxiv:1609.02907. timoth´ee lacroix nicolas usunier and guillaume obozinski 2018. canonical tensor decomposition arxiv preprint for knowledge base completion arxiv:1806.07297. yuxuan lai yansong feng xiaohan yu zheng wang kun xu and dongyan zhao 2019. lattice cnns for matching based chinese question answering arxiv preprint arxiv:1902.09087. fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith 2018a toward abstractive summarization using semantic representations arxiv preprint arxiv:1805.10399. peter j. liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer 2018b generating wikipedia by summarizing long sequences in iclr vincent ng and claire cardie 2002. improving machine learning approaches to coreference resolution in proceedings of the 40th annual meeting on association for computational linguistics pages 104– 111. association for computational linguistics myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and fairseq a fast extensible michael auli 2019. in proceedings of toolkit for sequence modeling naacl-hlt 2019 demonstrations matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer 2018. deep contextualized word representations arxiv preprint arxiv:1802.05365. alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever 2019. language models are unsupervised multitask learners marco rospocher marieke van erp piek vossen antske fokkens itziar aldabe german rigau aitor soroa thomas ploeger and tessel bogaard 2016. building event-centric knowledge graphs from news journal of web semantics 37:132–151 rico sennrich barry haddow and alexandra birch 2016. neural machine translation of rare words with subword units in acl minjoon seo aniruddha kembhavi ali farhadi and hannaneh hajishirzi 2017. bidirectional attention ﬂow for machine comprehension in iclr gabriel stanovsky julian michael luke zettlemoyer and ido dagan 2018. supervised open information extraction in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long papers , pages 885– 895 4195 sainbayar sukhbaatar edouard grave piotr bojanowski and armand joulin 2019. adaptive attention span in transformers haitian sun bhuwan dhingra manzil zaheer kathryn mazaitis ruslan salakhutdinov and william cohen 2018. open domain question answering using early fusion of knowledge bases and text arxiv preprint arxiv:1809.00782. ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser and illia polosukhin 2017. attention is all you need nips piek vossen tommaso caselli and yiota kontzopoulou 2015. storylines for structuring massive streams of news in proceedings of the first workshop on computing news storylines pages 40–49 rachel wities vered shwartz gabriel stanovsky meni adler ori shapira shyam upadhyay dan roth eugenio mart´ınez-c´amara iryna gurevych and ido dagan 2017. a consolidated open knowledge representation for multiple texts in proceedings of the 2nd workshop on linking models of lexical sentential and discourse-level semantics pages 12–24 kun xu lingfei wu zhiguo wang yansong feng michael witbrock and vadim sheinin 2018. graph2seq graph to sequence learning with arxiv preprint attention-based neural networks arxiv:1804.00823. michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev 2017. graph-based neural multi-document summarization arxiv preprint arxiv:1706.06681. zhiping zheng 2003. question answering using web in proceedings of the news as knowledge base tenth conference on european chapter of the association for computational linguistics-volume , pages 251–254 association for computational linguistics jie zhou ganqu cui zhengyan zhang cheng yang zhiyuan liu and maosong sun 2018. graph neural networks a review of methods and applications arxiv preprint arxiv:1812.08434 4196 published as a conference paper at iclr 2018 generating wikipedia by summarizing long sequences peter j. liu∗ mohammad saleh∗ etienne pot† ben goodrich ryan sepassi łukasz kaiser noam shazeer google brain mountain view ca peterjliu msaleh epot bgoodrich rsepassi lukaszkaiser noam @ google.com abstract we show that generating english wikipedia articles can be approached as a multidocument summarization of source documents we use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article for the abstractive model we introduce a decoder-only architecture that can scalably attend to very long sequences much longer than typical encoderdecoder architectures used in sequence transduction we show that this model can generate ﬂuent coherent multi-sentence paragraphs and even whole wikipedia articles when given reference documents we show it can extract relevant factual information as reﬂected in perplexity rouge scores and human evaluations 1 introduction the sequence-to-sequence framework has demonstrated success in natural-language sequence transduction tasks such as machine translation more recently neural techniques have been applied to do single-document abstractive paraphrasing text summarization of news articles rush et al ( 2015 , nallapati et al ( 2016 ) in this prior work the input to supervised models ranged from the ﬁrst sentence to the entire text of an article and they are trained end-to-end to predict reference summaries doing this end-to-end requires a signiﬁcant number of parallel article-summary pairs since language understanding is a pre-requisite to generate ﬂuent summaries in contrast we consider the task of multi-document summarization where the input is a collection of related documents from which a summary is distilled prior work has focused on extractive summarization which select sentences or phrases from the input to form the summaries rather than generating new text there has been limited application of abstractive neural methods and one possible reason is the paucity of large labeled datasets in this work we consider english wikipedia as a supervised machine learning task for multidocument summarization where the input is comprised of a wikipedia topic title of article and a collection of non-wikipedia reference documents and the target is the wikipedia article text we describe the ﬁrst attempt to abstractively generate the ﬁrst section or lead of wikipedia articles conditioned on reference text in addition to running strong baseline models on the task we modify the transformer architecture vaswani et al. 2017 to only consist of a decoder which performs better in the case of longer input sequences compared to recurrent neural network rnn and transformer encoder-decoder models finally we show our modeling improvements allow us to generate entire wikipedia articles ∗joint ﬁrst-authors ordered randomly †work done as a member of the google brain residency g.co/brainresidency 1 published as a conference paper at iclr 2018 related work 2.1 other datasets used in neural abstractive summarization neural abstractive summarization was pioneered in rush et al ( 2015 , where they train headline generation models using the english gigaword corpus graff cieri 2003 , consisting of news articles from number of publishers however the task is more akin to sentence paraphrasing than summarization as only the ﬁrst sentence of an article is used to predict the headline another sentence rnn-based encoder-decoder models with attention seq2seq perform very well on this task in both rouge lin 2004 , an automatic metric often used in summarization and human evaluation chopra et al. 2016 . in nallapati et al ( 2016 , an abstractive summarization dataset is proposed by modifying a questionanswering dataset of news articles paired with story highlights from daily mail and cnn this task is more difﬁcult than headline-generation because the information used in the highlights may come from many parts of the article and not only the ﬁrst sentence one downside of the dataset is that it has an order-of-magnitude fewer parallel examples 310k vs. 3.8m to learn from standard seq2seq models with attention do less well and a number of techniques are used to augment performance another downside is that it is unclear what the guidelines are for creating story highlights and it is obvious that there are signiﬁcant stylistic differences between the two news publishers in our work we also train neural abstractive models but in the multi-document regime with wikipedia as can be seen in table , the input and output text are generally much larger with signiﬁcant variance depending on the article the summaries wikipedia lead are multiple sentences and sometimes multiple paragraphs written in a fairly uniform style as encouraged by the wikipedia manual of style1 however the input documents may consist of documents of arbitrary style originating from arbitrary sources we also show in table the rouge-1 recall scores of the output given the input which is the proportion of unigrams/words in the output co-occuring in the input a higher score corresponds to a dataset more amenable to extractive summarization in particular if the output is completely embedded somewhere in the input e.g a wiki-clone , the score would be 100. given a score of only 59.2 compared to 76.1 and 78.7 for other summarization datasets shows that ours is the least amenable to purely extractive methods 2.2 tasks involving wikipedia there is a rich body of work incorporating wikipedia for machine learning tasks including questionanswering hewlett et al ( 2016 , rajpurkar et al ( 2016 ) and information extraction lehmann et al. 2015 , and text generation from structured data lebret et al. 2016 . the closest work to ours involving generating wikipedia is sauper barzilay 2009 , where articles are generated extractively instead of abstractively in our case from reference documents using learned templates the wikipedia articles are restricted to two categories whereas we use all article types the reference documents are obtained from a search engine with the wikipedia topic used as query similar to our search engine references however we also show results with documents only found in the references section of the wikipedia articles 2.3 transformer models previous work on neural abstractive summarization relies on rnns as fundamental modules mirroring techniques successful in machine translation mt . recently state-of-the-art mt results were obtained using a non-recurrent architecture called the transformer vaswani et al. 2017 . the lack of recurrence enables greater within-training-example parallelization at the cost of quadratic complexity in the input sequence length we ﬁnd the transformer transfers well to medium length input sequence summarization and describe modiﬁcations to better handle longer sequences 1https //en.wikipedia.org/wiki/wikipedia manual_of_style published as a conference paper at iclr 2018 table : order of magnitude input/output sizes and unigram recall for summarization datasets # examples rouge-1 dataset output input gigaword graff cieri 2003 cnn/dailymail nallapati et al. 2016 wikisum ours 101 102–103 102–106 101 101 101–103 106 105 106 78.7 76.1 59.2 table : percentiles for different aspects of wikisum dataset size is in number of words percentile 20 40 50 60 80 100 lead size num citations citations size num search results search results size 37 562 10 1,1691 62 1,467 20 33,989 78 2,296 26 49,222 98 3,592 31 68,681 166 10,320 46 135,533 10,034 1,029 6,159,463 2,095 5,355,671 english wikipedia as a multi-document summarization dataset wikipedia being an encyclopedia can be viewed as a collection of summaries on various topics given by their title e.g ” canada or machine learning . the source material to be summarized can be viewed as all reputable documents on the web or books however to make the problem more tractable we consider the following subsets of all documents d 1. cited sources a wikipedia article that conforms to the style guidelines should be wellsupported by citations found in the references section of wikipedia articles for each article ai we extract all text without markup from crawlable citation documents ci d to use as input to our method 2. web search results to expand the collection of reference documents we crawl the search results from the google search engine using the article section titles as queries for each query we collect 10 result pages from this collection we remove the wikipedia article itself which is often among the top results we also remove clones , which are detected when there is a high-level of unigram overlap with the article details provided in a.2.1 . we denote these reﬁned search results for an article ai as si d. similar to ci we extract only the text to use as input table describes overall properties of our wikisum dataset many articles have few citations motivating our supplementation of the source documents with web search results on the other hand citations when available tend to be of higher-quality when counting the total words in the entire dataset it is orders-of-magnitude larger than previous summarization datasets to have consistent train/development/test data across corpus-comparison experiments we restrict the articles to those with at least one crawlable citation we divide the articles roughly into 80/10/10 for train/development/test subsets resulting in 1865750 233252 and 232998 examples respectively 4 methods and models because the amount of text in input reference documents ci si can be very large see table ) it is infeasible to train an end-to-end abstractive model given the memory constraints of current hardware hence we ﬁrst coarsely select a subset of the input using extractive summarization the second stage involves training an abstractive model that generates the wikipedia text while conditioning on this extraction this two-stage process is inspired by by how humans might summarize multiple long documents first highlight pertinent information then conditionally generate the summary based on the highlights 3 published as a conference paper at iclr 2018 4.1 extractive stage we investigate three extractive methods from the summarization literature along with a trivial and cheating method to assess the importance of this stage for each article ai we create a ranked list of paragraphs { pi of ci si . from this we select the ﬁrst tokens as input to the second abstractive stage ri j } occurring in ci si where ri j is the rank of the jth paragraph pi 1. identity as a trivial baseline extractor we simply use the ﬁrst tokens of the input 2. tf-idf a non-trivial ranking is to consider ranking paragraphs as documents in a queryretrieval problem where the query is the title of the article t ai . we compute tf-idf ramos et al. 2003 for the query with respect to the documents { pi } that is we summate for each word in the query nw log nd ndw where nw nd and ndw are the count of the word in the document total number of documents and total number of documents containing the word respectively 3. textrank mihalcea tarau 2004 : a weighted graph is deﬁned where text units are nodes and edges are deﬁned by a similarity measure based on word overlap an algorithm similar to pagerank page et al. 1999 is then used to compute the ranking of text units we used paragraphs for the text units 4. sumbasic nenkova vanderwende 2005 : word frequencies in the input text are used to assign scores to words which are in turn used to score sentences after selecting the best scoring sentence words in it have their scores reduced and the process is repeated until the desired summary length is reached 5. cheating to further demonstrate the quality of extraction on the ﬁnal performance we } using recall of bigrams in the ground truth implement a cheating extractor that ranks pi text d pi , ai = bigrams pi ) bigrams ai bigrams ai ( ) 4.2 abstractive stage 4.2.1 data representation given the ordered paragraphs pi the paragraphs in order the most relevant at the beginning and preﬁxed with the title we then encode the text using sub-word tokenization similar to wu et al ( 2016 with a vocabulary size of 32,000 yielding tokenized input xi ri j } we derive the raw text input simply as the concatenation of for various values of in experiments we truncate the tokens to form the input sequence texti t ai ( ) pi ri j } tokenize texti = xi ( x1 , x2 , ... xni ) ml = x1 , ... xmin l ni i for the output we use the same vocabulary and tokenization for the wikipedia lead text but do not do any truncation across experiments next we describe the abstractive models w that learn to write articles ai w ml ) which we treat as a sequence transduction problem from very long input sequences up to = 11000 to medium output sequences typically less than 500 . published as a conference paper at iclr 2018 4.2.2 baseline models as a baseline we apply the standard lstm encoder-decoder with attention seq2seq-att as in bahdanau et al ( 2014 to this task as is typical we train to optimize the maximum-likelihood objective yi tokenize ai p yi|ml ) ( ) i=1 a stronger more recent baseline that we use is the non-recurrent transformer model described in 2.3 which also has symmetric encoder and decoder modules t-ed . 4.2.3 transformer decoder t-d we introduce a simple but effective modiﬁcation to t-ed for long sequences that drops the encoder module almost reducing model parameters by for a given hyper-parameter set , combines the input and output sequences into a single sentence and is trained as a standard language model that is we convert a sequence-transduction example m1 ... mn ( ) ( y1 ... yη into the sentence w1 ... wn+η+1 = m1 ... mn δ y1 ... yη , where is a special separator token and train a model to predict the next word given the previous ones p w1 ... wn+η = ( wi|w1 ... wj−1 n+η cid:89 j=1 since the model is forced to predict the next token in the input m as well as , error signals are propagated from both input and output time-steps during training we also suspect that for monolingual text-to-text tasks redundant information is re-learned about language in the encoder and decoder we believe this allows for easier optimization and empirically observe this with longer sequences see section 5.3 . note that because of the self-attention of the transformer when generating the next token attention from both and are considered at inference we provide the input sequence mi initially and auto-regressively generate the output yi as normal 4.2.4 transformer decoder with memory-compressed attention t-dmca to re-use the terminology used to describe the transformer the attention is a function of a query q and set of key k and value v pairs to handle longer sequences we modify the multi-head self-attention of the transformer to reduce memory usage by limiting the dot products between and in attention q k v = sof tmax qk t√ dk v local attention sequence tokens are divided into blocks of similar length and attention is performed in each block independently as the attention memory cost per block becomes constant this modiﬁcation allow us to keep the number of activations linear with respect to the sequence length in our experiments we choose to have blocks of 256 tokens memory-compressed attention after projecting the tokens into the query key and value embeddings we reduce the number of keys and values by using a strided convolution the number of queries remains unchanged this modiﬁcation allows us to divide the number of activations by a compression factor in our experiments we use convolution kernels of size with stride 3. in contrast to local attention layers which only capture the local information within a block the memorycompressed attention layers are able to exchange information globally on the entire sequence these modiﬁcations see figure ) allow us in practice to process sequences 3x in length over the t-d model for both local and memory-compressed attention masking is added to prevent the queries from attending to future keys and values our ﬁnal architecture is a 5-layer network lmlml alternating between local-attention l layers and memory-compressed attention m layers in vaswani et al ( 2017 it is identical layers . we also added in some experiments one mixture of experts moe layer shazeer et al. 2017 to increase the network s capacity 5 published as a conference paper at iclr 2018 figure : the architecture of the self-attention layers used in the t-dmca model every attention layer takes a sequence of tokens as input and produces a sequence of similar length as the output left original self-attention as used in the transformer-decoder middle memory-compressed attention which reduce the number of keys/values right local attention which splits the sequence into individual smaller sub-sequences the sub-sequences are then merged together to get the ﬁnal output sequence 5 experiments 5.1 evaluation in experiments we evaluate based on perplexity per-wordpiece , a common language modeling metric and rouge-l f1 version rouge-1.5.5 , a common metric used in comparing candidate and reference summaries note the f1 ﬂavor of rouge is more appropriate in this setting as we do not explicitly constrain the output length in abstractive models it is the harmonic mean of rougerecall which favors long summaries and rouge-precision which favors short summaries . although optimizing rouge directly has been shown to not always yield the best summaries as evaluated by human judgment paulus et al. 2017 , we found that for our task optimizing for perplexity correlates with increased rouge and human judgment we suspect that the relatively uniform style of wikipedia articles makes rouge more appropriate here than in general abstractive summarization tasks 5.2 model training details and decoding the transformer for all abstractive model training we use the open-source tensor2tensor2 library the seq2seq baseline had a hidden size of 128 with layers we use the hyper-parameter set deﬁned in the library as lstm attention . for set transfomer base v1 and train for million steps models exhibited very little overﬁtting and did not require early-stopping the transformer decoder t-d was identical to the decoder part of t-ed the t-dmca model is similar to t-d but with the enhancements described in section 4.2.4. unless otherwise stated during decoding we use a beam search of size and length penalty = 0.6 wu et al. 2016 and decode until an end-of-sequence token is reached encoder-decoder hyper-parameter t-ed , we use the : //github.com/tensorflow/tensor2tensor published as a conference paper at iclr 2018 table : comparison of extractive method and corpus with = 500 and the transformer e-d model extractor corpus test log-perplexity rouge-l cheating tf-idf tf-idf tf-idf identity combined combined citations-only search-only combined 1.72975 2.46645 3.04299 3.56593 4.80215 59.3 34.2 22.6 2.8 4.0 5.3 results and discussion there are four main dimensions we vary in experiments in generating wikipedia lead sections 1. extractive method sumbasic textrank tf-idf identity cheating extractor 2. input corpus citations search results combined 3. abstractive model input length l we try values between 100 and 11000 4. abstractive model architecture seq2seq-att t-ed t-d t-dmca figure : rouge-l f1 for various extractive methods the abstractive model contribution is shown for the best combined tf-idf -t-dmca model extractive-only is not enough we investigate performance of extractive methods without the abstractive model by looking at the rouge-l f1 scores after running tf-idf sumbasic and textrank in figure , without any abstractive model in the case of textrank and sumbasic we matched the output length to the target length and observe the extractive methods perform roughly in-line with each other in terms of rouge-l f1 our best abstractive model more than doubled this metric further this model yields large improvements in perceived linguistic quality elaborated below . extractive method from table we observe that smart extraction is critical for ﬁnal abstractive performance there is a signiﬁcant gap between doing nothing identity and extractive summarization tf-idf further there is a signiﬁcant gap between tf-idf and the cheating extractor suggesting future work in improving the extraction step could result in signiﬁcant improvements one possibility is to train a supervised model to predict relevance eq 1 , which we leave as future work for subsequent experiments we ﬁx the extractive method to tf-idf input corpus from table we also observe that unsurprisingly the combined dataset performs best but the gaps between it and using only one of citations or search results are both signiﬁcant and their contributions are complementary in subsequent experiments we report only the combined results 7 published as a conference paper at iclr 2018 table : performance of best models of each model architecture using the combined corpus and tf-idf extractor model test perplexity rouge-l seq2seq-attention l 500 transformer-ed l 500 transformer-d l 4000 transformer-dmca no moe-layer l 11000 transformer-dmca moe-128 l 11000 transformer-dmca moe-256 l 7500 5.04952 2.46645 2.22216 2.05159 1.92871 1.90325 12.7 34.2 33.6 36.2 37.9 38.8 figure : shows perplexity versus for tf-idf extraction on combined corpus for different model architectures for t-dmca e denotes the size of the mixture-of-experts layer abstractive model architecture and input length as we see from table , seq2seq-attention as a baseline does quite poorly on this task compared to the transformer architectures as seen in figure , we observe that the transformer encoder-decoder t-ed architecture consistently improves in performance until a best of around = 500 1000 and is unable to learn at = 2000. this motivated the transformer-decoder which we found could learn and improve up to = 4000 before running out of memory on our machines equipped with 16gb of gpu ram nvidia p100 . by using the t-dmca modiﬁcations we were able to train up to = 11000 and continued to see improvements in performance we also found the moe-layer helped performance by adding model capacity at high , for example dropping log-perplexity from 2.05 to 1.93 at = 11000 with 128 experts our best model attempted uses 256 experts at = 7500 we were unable to use 256 experts with = 11000 due to memory constraints and achieves a perplexity of 1.90 human evaluation linguistic quality we conducted a duc-style human evaluation of linguistic quality3 of samples from a baseline abstractive seq2seq , the best extractive tf-idf , and our best tdmca models five different dimensions are assessed grammaticality non-redundancy referential clarity focus and structure/coherence as seen in table , the t-dmca model does statistically signiﬁcantly better on all dimensions except on non-redundancy where tf-idf does about as well overall we observed high ﬂuency and coherence from our best abstractive model occasionally we observed some repetition of phrases which hurt the non-redundancy and structure but it was much rarer compared with the other abstractive method seq2seq the biggest weakness of the extractive 3http //duc.nist.gov/duc2007/quality-questions.txt published as a conference paper at iclr 2018 table : linguistic quality human evaluation scores scale 1-5 higher is better . a score significantly different according to the welch two sample t-test with = 0.001 than the t-dmca model is denoted by . model focus grammar non redundancy referential clarity structure and coherence t-dmca best tf-idf -only seq2seq-attention 4.5 3.0 3.0 4.6 3.6 3.4 4.2 3.9 2.1 4.5 3.2 3.4 4.2 2.7 2.3 table : side-by-side for two models pair with large automatic metric gaps model a model rouge-l a rouge-l t-ed l 100 t-ed l 500 t-ed l 500 t-dmca-moe-256 l 7500 30.9 34.2 34.2 38.8 prefer # prefer a 4.25 1.5 method compared with our best abstractive model was the lack of structure and coherence in the summaries human evaluation side-by-side preference we validated our chosen metrics correlate with human preference by conducting two side-by-side human evaluation experiments comparing models with large gaps in perplexity/rouge we observe in table that human judgment correlates with our automatic metrics but it becomes more difﬁcult to distinguish at the higher-end of model performance details of the human evaluation experimental designs can be found in appendix a.3 to summarize the quantitative results we believe the highest impact future work will be from improving the extractive stage and extending the decoder-only architectures to learn from larger while maintaining sufﬁcient model capacity comparison with sauper barzilay 2009 : a direct comparison with sauper barzilay 2009 is difﬁcult for three reasons ( a they report results only for two small subsets of wikipedia diseases and american actors ( ) we report on lead generation instead of full-articles ( ) we were unable to obtain the exact articles they used as input and output in particular they make no claim of wikiclone detection . however we make a best-effort comparison by ﬁnding the subset of articles of our test set that correspond to diseases and american actors the two categories reported on by sauper barzilay and reporting our rouge-1 scores table ) we observe that we perform better on american actors than diseases probably because of the prevalence of the former and biographies in wikipedia compared to the latter in our training set for our single global model whereas sauper barzilay likely beneﬁt from the category-speciﬁc templates on average our rouge-1 scores are higher but do worse on the less common and somewhat speciﬁc disease category 5.4 qualitative discussion in figure , we show the predictions from three different models using tf-idf extraction and the combined corpus along with the wikipedia ground truth as the perplexity decreases we see improvements in the model outputs in terms of ﬂuency factual accuracy and narrative complexity in particular the t-dmca model offers a respectable alternative to the wikipedia version and is more succinct while mentioning key facts such as where the law ﬁrm was located when and how it was formed and the rise and fall of the ﬁrm in manual inspection of model outputs we noticed an unexpected side-effect models learn to translate names from english into multiple languages e.g rohit viswanath into hindi see figure ) although we did not do a systematic evaluation of the translations we found they are often correct and often they are not found in the wikipedia article itself we also veriﬁed that in general the published as a conference paper at iclr 2018 table : comparison of results with sauper barzilay 2009 . note our results are reported for lead section whereas sauper barzilay report for articles rouge-1 rouge-1 rouge-1 f1 all wikipedia t-dmca ours diseases 46 25 36 t-dmca ours , = 161 sauper barzilay american actors t-dmca ours , = 1322 sauper barzilay 52 46 53 48 39 72 40 43 29 37 54 41 figure : shows predictions for the same example from different models example model input can be found in the appendix a.4 translation is not merely copied from the source such as example cases where the target language is the incorrect one e.g translation of an english name into ukrainian . 5.5 generating full-wikipedia articles given that we have shown it is possible to learn sequence transduction models on combined inputoutput sequence lengths of approximately 12000 using the t-d architecture we show that it is possible to train a model to generate entire wikipedia articles as a preliminary result we trained two t-dmca models one is trained to use = 6000 reference tokens to predict at most 2192 article tokens longer examples are ignored and another is conditioned only on the title and generates articles up to 4000 tokens long we show samples from both models in appendix a.1 although the generated articles are not as good as the real wikipedia or our lead section samples the models can be seen to organize the 10 published as a conference paper at iclr 2018 figure : translation examples from the transformer-ed l 500. article into plausible sections and exhibit global coherence over multi-paragraph text the model with access to reference documents inserts factual information in the generated article although we did not focus or tune on the full-article task we see this as an interesting future work for abstractive summarization 6 conclusion we have shown that generating wikipedia can be approached as a multi-document summarization problem with a large parallel dataset and demonstrated a two-stage extractive-abstractive framework for carrying it out the coarse extraction method used in the ﬁrst stage appears to have a signiﬁcant effect on ﬁnal performance suggesting further research on improving it would be fruitful we introduce a new decoder-only sequence transduction model for the abstractive stage capable of handling very long input-output examples this model signiﬁcantly outperforms traditional encoderdecoder architectures on long sequences allowing us to condition on many reference documents and to generate coherent and informative wikipedia articles 7 public release of dataset and code to encourage further research on large-scale summarization we will release the urls used in our experiments the wikipedia url as well as the urls of its references . we also provide code that that extracts content from the commoncrawl dataset4 which is freely available for download we use the open-source tensor2tensor5 library for training abstractive models and will be releasing our abstractive modeling code extensions further details are available at : // goo.gl/wsuus9 4http //commoncrawl.org : //github.com/tensorflow/tensor2tensor 11 published as a conference paper at iclr 2018 acknowledgments we thank samy bengio jeff dean claire cui fred bertsch chad whipkey anurag rana ashish vaswani llion jones and the tensorflow/tensor2tensor contributors for help with the project references dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate arxiv preprint arxiv:1409.0473 2014. sumit chopra michael auli and alexander rush abstractive sentence summarization with attentive recurrent neural networks in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies pp 93–98 2016. hoa trang dang overview of duc 2005. in proceedings of the document understanding conference volume 2005 pp 1–12 2005. david graff and christopher cieri english gigaword 2003. linguistic data consortium philade plhia 2003. daniel hewlett alexandre lacoste llion jones illia polosukhin andrew fandrianto jay han matthew kelcey and david berthelot wikireading a novel large-scale language understanding task over wikipedia arxiv preprint arxiv:1608.03542 2016. r´emi lebret david grangier and michael auli neural text generation from structured data with application to the biography domain in proceedings of the 2016 conference on empirical methods in natural language processing emnlp 2016 austin texas usa november 1-4 2016 pp 1203–1213 2016. url http //aclweb.org/anthology/d/d16/d16-1128.pdf jens lehmann robert isele max jakob anja jentzsch dimitris kontokostas pablo mendes sebastian hellmann mohamed morsey patrick van kleef s¨oren auer et al dbpedia–a largescale multilingual knowledge base extracted from wikipedia semantic web 6 2 :167–195 2015. chin-yew lin rouge a package for automatic evaluation of summaries in text summarization branches out proceedings of the acl-04 workshop volume 8. barcelona spain 2004. rada and paul tarau textrank bringing order into text conference on empirical methods in natural language processing 2004. in proceedings of the 2004 ramesh nallapati bowen zhou cicero dos santos c¸ a glar gulc¸ehre and bing xiang abstractive text summarization using sequence-to-sequence rnns and beyond conll 2016 pp 280 2016. ani nenkova and lucy vanderwende the impact of frequency on summarization microsoft re search redmond washington tech rep. msr-tr-2005 101 2005. lawrence page sergey brin rajeev motwani and terry winograd the pagerank citation ranking bringing order to the web technical report stanford infolab 1999. romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization arxiv preprint arxiv:1705.04304 2017. pranav rajpurkar jian zhang konstantin lopyrev and percy liang squad 100,000+ questions for machine comprehension of text arxiv preprint arxiv:1606.05250 2016. juan ramos et al using tf-idf to determine word relevance in document queries in proceedings of the ﬁrst instructional conference on machine learning volume 242 pp 133–142 2003. alexander m. rush sumit chopra and jason weston a neural attention model for abstractive sentence summarization in proceedings of the 2015 conference on empirical methods in natural language processing emnlp 2015 lisbon portugal september 17-21 2015 pp 379–389 2015. url http //aclweb.org/anthology/d/d15/d15-1044.pdf 12 published as a conference paper at iclr 2018 christina sauper and regina barzilay automatically generating wikipedia articles a structureaware approach in proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp volume volume , acl 09 pp 208–216 stroudsburg pa usa 2009. association for computational linguistics isbn 978-1-932432-45-9. url http //dl.acm.org/citation cfm id=1687878.1687909 noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoffrey hinton and jeff dean outrageously large neural networks the sparsely-gated mixture-of-experts layer arxiv preprint arxiv:1701.06538 2017. ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser and illia polosukhin attention is all you need arxiv preprint arxiv:1706.03762 2017. yonghui wu mike schuster zhifeng chen quoc le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine transarxiv preprint lation system bridging the gap between human and machine translation arxiv:1609.08144 2016 13 published as a conference paper at iclr 2018 a appendix a.1 examples of full wikipedia generated samples figure : an example decoded from a t-dmca model trained to produce an entire wikipedia article conditioned on 8192 reference document tokens 14 published as a conference paper at iclr 2018 figure : three different samples a t-dmca model trained to produce an entire wikipedia article conditioned only on the title samples and are truncated due to space constraints 15 published as a conference paper at iclr 2018 a.2 implementation details a.2.1 wikipedia clone detection for detecting whether a reference document d is a wikipedia article clone we compute the maximum recall of unigrams for each section of the wikipedia article a r d a = max s∈sections a |unigrams d ∩ unigrams s | |unigrams s | and detect a clone if > 0.5. a.3 human evaluation experiment figure : screenshot of duc-style linguistic quality human evaluation tool to assess linguistic quality we randomly selected samples generated by models from the test set and ask raters to choose a score from to ( higher is better for ﬁve dimensions grammaticality non-redundancy referential clarity focus and structure and coherence these were used in the past at duc for evaluating summaries dang 2005 . for each model we selected 25 examples and averaged the scores for each question across raters out of pool of ) to compare two models by human evaluation we randomly select examples from the test set and show model outputs side-by-side in the interface shown in figure 9. which side a model appears on is randomized per example and rater for the experiments in table we had raters score 25 examples each and computed the ratio of ratings preferring one model over the other a.4 example abstractive model input 16 published as a conference paper at iclr 2018 figure : screenshot of side-by-side human evaluation tool raters are asked whether they prefer model output on the left or right given a ground truth wikipedia text 17 published as a conference paper at iclr 2018 figure 10 example extractive-output/abstractive-input for models in dewey lebeouf example the extractive method used is tf-idf 18 the thirty-third aaai conference on artiﬁcial intelligence aaai-19 hierarchical encoder with auxiliary supervision for neural table-to-text generation learning better representation for tables tianyu liu fuli luo qiaolin xia shuming ma baobao chang zhifang sui key laboratory of computational linguistics ministry of education school of electronics engineering and computer science peking university beijing china tianyu0421 luofuli xql shumingma chbb szf @ pku.edu.cn abstract infobox generating natural language descriptions for the structured tables which consist of multiple attribute-value tuples is a convenient way to help people to understand the tables most neural table-to-text models are based on the encoder-decoder framework however it is hard for a vanilla encoder to learn the accurate semantic representation of a complex table the are two-fold ﬁrstly the table-to-text datasets often contain large number of attributes across different domains thus it is hard for the encoder to incorporate these heterogeneous resources secondly the single encoder also has difﬁculties in modeling the complex attribute-value structure of the tables to this end we ﬁrst propose a two-level hierarchical encoder with coarse-to-ﬁne attention to handle the attribute-value structure of the tables furthermore to capture the accurate semantic representations of the tables we propose joint tasks apart from the prime encoder-decoder learning namely auxiliary sequence labeling task text autoencoder and multi-labeling classiﬁcation as the auxiliary supervisions for the table encoder we test our models on the widely used dataset wikibio which contains wikipedia infoboxes and related descriptions the dataset contains complex tables as well as large number of attributes across different domains we achieve the state-of-the-art performance on both automatic and human evaluation metrics introduction data-to-text generation produces understandable texts from some underlying non-linguistic representation of information reiter and dale 1997 2000 . table-to-text generation which belongs to the data-to-text generation aims at generating natural language descriptions for the structured tables to help people to get the key points of the tables different from text-to-text generation tasks like machine translation or abstractive summarization the sources for table-to-text generation are the tables with hierarchical attribute-value structure open-domain tables like wikipedia infoboxes table ) often have large number of attributes across different domains although previous researchers proposed some taskspeciﬁc encoder-decoder models for table-to-text generation liu et al 2017a sha et al 2017 wiseman shieber and rush 2017 perez-beltrachini and lapata 2018 bao et al copyright ( ) 2019 association for the advancement of artiﬁcial intelligence www.aaai.org . all rights reserved content edward merrill root 04 january 1895 baltimore maryland usa 26 october 1973 kennebunkport maine american anti-communist activities educator and poet attribute name birthdate birthplace deathdate deathplace nationality known for occupation alma mater amherst college residence article title richmond indiana e. merrill root description edward merrill root known as e. merrill root january , 1895 october 26 1973 , was an american educator and poet devoted to anti-communist causes table : example of a wikipedia infobox for edward merrill root and the associated description 2018 , most of them are dedicated to improving the decoding phase while generating sha et al 2017 wiseman shieber and rush 2017 perez-beltrachini and lapata 2018 bao et al 2018 . we argue that a single encoder without any auxiliary assistant may not be effective to capture the accurate semantic representation due to the hierarchical structure and vast heterogeneous attributes of the tables to this end we ﬁrst propose a two-level hierarchical table encoder which encodes both the word-level and attributelevel semantics the coarse-to-ﬁne attention is proposed to cooperate with the two-level hierarchical table encoder although the hierarchical table encoder has greatly improved the table-to-text generation we believe the performance can be further enhanced by the external assistance to the table encoder as a complex table may not be well represented by a single encoder so we also propose auxiliary tasks including auxiliary sequence labeling task text autoencoder and multi-labeling classiﬁcation to help the table encoder to better represent the source tables and then improve the encoder-decoder style table-to-text generation auxiliary sequence labeling task is a new approach to incorporate the attribute information into the table encoder we view the attribute names such as name , birthdate in table ) as the labels for the related table content and use 6786 the sequence labeling as a multi-task to better represent a structured table this is a way to guide the encoder to reproduce thus remember the attribute names the experiments show that this is a better way to integrate attribute information than previous works lebret grangier and auli 2016 sha et al 2017 liu et al 2017a . text auto-encoder used the related descriptions of the source tables to supervise the table encoder compared with the complex structure of the source tables the associated descriptions are well-written and straightforward so it is easier to encode their semantic representation since the descriptions share the similar meanings of the associated source tables it is possible to supervise the learning of the semantic representation of the source tables with that of the related descriptions we use the internal representation of the auto-encoder to supervise that of the hierarchical table encoder by minimizing their distance multi-labeling classiﬁcation is also operated on the internal representation of the table encoder we view all the attribute names which appear in the speciﬁc table as the targets for the multi-label classiﬁcation on the internal representation of the table encoder we encourage the semantic representation of the source tables to carry as much attribute-level information as possible the auxiliary supervisions from text auto-encoder and multi-labeling classiﬁcation can be widely applied into all the encoder-decoder models for table-to-text generation the auxiliary sequence labeling supervision can only be used in the proposed hierarchical table encoder furthermore we witness a sharp decrease on the performance of the vanilla encoder-decoder models when we randomly shufﬂe the order of the attributes in the source tables in both training and testing set . the proposed auto-encoder and multilabeling supervisions can make our models more robust to the disordered tables we use wikibio lebret grangier and auli 2016 , which contains wikipedia infoboxes and related biographies as our benchmark dataset the dataset owns over three thousands attributes which describe people in the different areas domains , including sportsmen politicians artists soldiers etc each table contains about 20 attributes on average experiments show our model achieves the state-of-the-art results on both automatic and human evaluation metrics two-level hierarchical table encoder notations given a table-to-text dataset with data samples the tth data sample tt yt contains a source table tt { x1 x2 ··· xm with words and a description yt { y1 y2 ··· yl with words the source table tt also has attributes at { a1 a2 ··· an . each word xi belongs to a speciﬁc attribute aj in the following sections we use xaj to represent the i-th word xi in the table tt which belongs to aj xi aj . two-level hierarchical encoder most previous work liu et al 2017a sha et al 2017 wiseman shieber and rush 2017 viewed a structured figure : the proposed two-level hierarchical lstm and coarse-to-ﬁne attention suppose we have attributes marked by different colors and words in the table at the t-th decoding step the decoder state st attends to the hidden states of attribute-level and word-level lstms for coarse-grained and ﬁne-grained attention respectively table as a sequence of words and only used word-level lstm hochreiter and schmidhuber 1997 to encode the tables however a factual table is inherently organized in the attribute-value hierarchical structure thus we propose attribute-level lstm apart from word-level lstm to capture the attribute-level semantics of the table as shown in fig , the proposed table encoder contains two separate lstms the word-level lstm encodes each word xaj in the table sequentially while the attribute-level lstm encodes each attribute-value tuple by taking the last state haj last for the attribute aj in the word-level lstm as input haj = lstmword hak i−1 xaj ) aj lstmattribute h aj−1 haj last ( ) 2 we have ak aj if xi−1 aj otherwise ak aj−1 coarse-to-ﬁne attention to cooperate with the proposed two-level lstm we also modify the attention mechanism to incorporate both wordlevel and attribute-level semantics for convenience we only focus on the one step of decoding in the following illustration given the decoder state st at the t-th decoding step βaj is the ﬁne-grained attention for the i-th word xaj in the table which belongs to the attribute aj γaj is the coarse-grained attention for the attribute aj i ∝ ( haj βaj 3 in which ( ) is the bahdanau-style attention calculation function bahdanau cho and bengio 2014 . st ; γaj g h aj st the proposed coarse-to-ﬁne attention αi for the i-th word in the table is the element-wise product of the ﬁne-grained attention βaj and the coarse-grained attention γaj αi βaj × γaj xi aj ( ) auxiliary supervision for table encoder why we need auxiliary supervision 1 many open-domain table-to-text datasets such as wikibio have large numbers of attributes across different domains even an individual table in wikibio has about 20 6787 < % ( ) cid:25 ( ) cid:23 ( ) cid:13 ( ) cid:25 ( ) cid:1 - cid:16 ( ) cid:16 ( ) cid:9 ( ) cid:8 ℎ < ℎ < ℎ < ℎ < ℎ < ℎ+ ? , ? & ' , + cid:11 ( ) cid:23 ( ) cid:1 - cid:16 ( ) cid:16 ( ) cid:9 ( ) cid:8 % ; > * cid:3 ( ) cid:4 ( ) cid:11 ( ) cid:1 ( ) cid:9 ( ) cid:6 ( ) cid:8 ( ) cid:2 ( ) cid:12 ( ) cid:8 ( ) cid:7 ( ) cid:8 c+ , * & c' ( ) # ; ? ⊙⊙ a < a < a < a < a < a+ ( a < b = < b ? cid:3 ( ) cid:12 ( ) cid:24 ( ) cid:1 ( ) cid:23 ( ) cid:18 ( ) cid:16 ( ) cid:2 ( ) cid:25 ( ) cid:20 ( ) cid:18 ( ) cid:20 ( ) cid:18 ( ) cid:16 ( ) cid:17 ( ) cid:12 ( ) cid:20 ( ) cid:15 ( ) cid:25 ( ) cid:16 ( ) cid:25 ( ) cid:21 ( ) ⋯ ( ) cid:16 ( ) cid:16 ( ) cid:12 ( ) cid:16 ( ) cid:4 ( ) cid:24 ( ) cid:23 ( ) cid:22 ( ) cid:18 ( ) cid:20 - ∗ & a word-attribute concatenation a training stage b testing stage b auxiliary sequence labeling supervision figure : two ways to incorporate attribute information into the encoder most previous work utilized word-attribute concatenation to represent the tables fig a . we ﬁnd a more efﬁcient way to guide the encoder to remember the attributes fig ) we treat the attribute names as the labels dashed box for a sequence labeling auxiliary task and train it jointly with the table-to-text generation task attributes on average so it is very for a single encoder to incorporate such heterogeneous resources and learn the accurate semantic representation of the source tables 2 due to the extreme data hungry of the neural network models researches often use large-scale crawled datasets from the internet however some informal expressions or outdated information may appear as noisy cases in the structured tables 3 as studied in sha et al 2017 , the order of different key-value pairs does inﬂuence the generation quality the attributes feed earlier to the encoder may be ignored due to the gradient vanishing and exploding problem in the encoderdecoder framework auxiliary sequence labeling task although it is quite straightforward to incorporate the attribute names as the additional inputs to the table encoder hachey radford and chisholm 2017 liu et al 2017a sha et al 2017 , there is no explicit evidence showing that the encoder can remember the attributes to this end we treat the attribute information as an auxiliary supervision to explicitly guide the table encoder to reproduce thus remember ) the attribute names in the tables by a sequencelabeling multi-task as shown in fig , for a speciﬁc table tk with attributes the sequence labeling training is based on the hidden states figure : the overview of auto-encoder and multi-labeling supervision we use text auto-encoder to supervise the table encoder as the text has similar meaning with the source tables we also use the multi-label classiﬁcation as auxiliary assistant task at the training stage we use the auxiliary tasks to supervise the table-to-text model at the testing stage we only use table-to-text model to generate texts [ ai i=1 n of the attribute-level lstm a∗ : = label [ ai i=1 n ( ) in which label function can be multi-layer perceptron mlp or crf lafferty mccallum and pereira 2001 layer a∗ : are the predicted attribute names the auxiliary sequence labeling task is trained jointly with the sequence-to-sequence learning we use crossentropy loss for the sequence labeling task lsl −λ1 pθsl ak|hk ( ) ( ) k=1 λ1 is a tunable hype-parameter which is set to 0.3 according to the validation set ak and hk are the set of attribute names and the attribute-level hidden states for table tk auto-encoder supervision compared with the complex structured tables their associated descriptions are also strong guidances to the representation of the source tables as they are well-written and share the similar meanings as the source tables therefore we propose a text auto-encoder to reconstruct the descriptions at the training stage the table encoder compresses the source table ti into an internal representation zt at the same time the text encoder compresses the reference text yi into the representation zb then both zt and zb are fed into the same decoder to generate the related description the loss 6788 cid:3 ( ) cid:5 ( ) cid:8 ( ) cid:10 ( ) cid:11 ( ) cid:8 ( ) $ ℎ $ ℎ $ ℎ $ ℎ $ ℎ $ ℎ $ ℎ $ * $ * $ * $ * $ * $ * $ * $ , , , - % # $ , , , ( ) cid:12 ( ) cid:8 ( ) cid:2 ( ) cid:14 ( ) cid:10 ( ) cid:15 ( ) cid:9 ( ) cid:12 ( ) cid:7 ( ) cid:14 ( ) cid:11 ( ) cid:14 ( ) cid:12 ( ) # cid:1 ( ) cid:6 ( ) cid:7 ( ) cid:12 + + * * * ( ) cid:8 ( ) cid:11 ( ) cid:8 ( ) & $ + - % cid:3 ( ) cid:14 ( ) cid:7 ( ) cid:6 ( ) cid:2 ( ) cid:5 ( ) cid:10 ( ) cid:11 ( ) cid:1 ( ) cid:19 ( ) cid:10 ( ) cid:4 ( ) cid:20 ( ) cid:17 ( ) cid:7 ( ) cid:18 ( ) cid:16 ( ) cid:12 ( ) ' $ ℎ $ ℎ $ ℎ $ ℎ $ ℎ $ ℎ $ ℎ $ '' $ '' $ '' $ '' $ '' $ '' $ '' $ ( ) cid:4 ( ) cid:9 ( ) cid:2 ( ) cid:6 ( ) cid:7 ( ) cid:12 . ( ) * cid:16 ( ) cid:3 - cid:12 ( ) cid:11 ( ) cid:12 ( ) cid:6 ( ) cid:12 ( ) cid:13 ( ) cid:4 - cid:15 ( ) cid:16 ( ) cid:11 ( ) cid:11 ( ) cid:13 ( ) ( ) cid:14 ( ) cid:9 ( ) cid:7 ( ) cid:8 ( ) cid:16 ( ) ( ) cid:9 ( ) cid:20 ( ) cid:17 ( ) cid:14 ( ) ( cid:4 ( ) cid:15 ( ) cid:2 ( ) cid:7 ( ) cid:8 ( ) cid:13 ' ' ( ) cid:5 ( ) cid:10 ( ) cid:1 ( ) cid:11 ( ) cid:12 ( ) cid:9 ( ) '' $ '' % '' & ( cid:4 ( ) cid:15 ( ) cid:3 ( ) cid:7 ( ) cid:8 ( ) cid:13 ' ' . ( ) + + , ' ' ' ' ( ) cid:9 ( ) cid:14 ( ) cid:9 ( ) cid:12 ( ) cid:9 ( ) ) * ( '' ( ) cid:9 ( ) cid:14 ( ) cid:11 ( ) cid:12 ( ) cid:9 ( ) ) '' ' '' ( ) cid:5 ( ) cid:10 ( ) cid:1 ( ) cid:11 ( ) cid:12 ( ) cid:9 ( ) of the text auto-encoder is also the cross-entropy losses lae −λ2 pθae yi|zb ( ) ( ) i=1 where λ2 is a also tunable hyper-parameter which is set to 0.5 according to the validation set we enhance the supervision of text auto-encoder by minimizing the distance between the semantic representation zt and zb we implement the supervision by adding ldis into the loss function ( ) ae lae ldis ldis l ( zt zb ( ) λ3 nh where ( zt zb is a function which measures the distance between zt and zb λ3 is a tunable hyper-parameter to balance the loss of the supervision and other parts of the overall loss nh is the number of the hidden unit to limit the magnitude of the distance function we set λ3 0.3 based on the model performance on the validation set the distance between two representations can be written as l2 constraint d zt zb = ||zt zb||2 9 multi-labeling supervision as shown in fig , to encourage the internal representation zt of the table to carry as much attribute-level information as possible we also use multi-label classiﬁcation task on zt to encode the attributes across different domains we use mlp to map zt to the labels cid:101 a∗ : and treat all the attributes ai a1 n in the associated table ti as the targets λ4 is also a hyper-parameter which is set to 0.5 according to the validation set lm = −λ4 pθm ( ai|zt ( 10 n cid:88 i=1 loss function and training the overall objective function consists of parts crossentropy loss of the table-to-text generation auxiliary sequence labeling task eq ) text auto-encoder supervision eq & ) and the multi-labeling supervision eq 10 . the cross-entropy loss of the table-to-text generation is written as pθs2s yi|zt ( 11 ls2s − ( ) i=1 the overall loss of our model is the sum of these individual losses ( ) = ls2s lsl l ae lm ( 12 experiments dataset we use wikibio dataset lebret grangier and auli 2016 as our benchmark dataset wikibio contains 728,321 articles from english wikipedia seq 2015 . the dataset uses the ﬁrst sentence of each article as the description of the related infobox which contains 26.1 words on average 9.5 words in the description also occur in the infoboxes the infobox contains 53.1 words and 19.7 attributes on average the dataset has been divided in to training 80 ) testing 10 ) and validation 10 ) sets evaluation metrics automatic metrics following the previous work lebret grangier and auli 2016 sha et al 2017 liu et al 2017a , we use bleu-4 papineni et al 2002 and rouge-4 f measure ( lin 2004 for automatic evaluation human evaluation since automatic evaluations like bleu may not always be reliable for nlg systems we use human evaluation which involves the generation ﬂuency and the generation quality how much false or irrelevant information is mentioned in the biography . we ﬁrstly sampled 300 generated items from the test set for human evaluation each item contains the generated descriptions by different systems given the same resource tables these items are distributed to third-party crowd-workers who have no knowledge about which system the biography is from they are asked to score the generated biographies according to their ﬂuency and quality the scores range from to ( higher scores are better . table shows the scores for the generated biographies whose source table is table 1. experimental details following previous work liu et al 2017a . we select the most frequent 20,000 words in the training set as the word vocabulary for attribute vocabulary we select the most frequent 1480 attributes we tune the hyper-parameters based on the model performance on the validation set since we have many hyperparameters λ1 λ4 in eq , eq , eq and eq 10. for convenience we only tune each independently from 0.3,0.5,0.7,1.0 . the dimensions of word embedding ﬁeld embedding hidden unit are set as 500 50 600 respectively the batch size learning rate and optimizer are 32 3e-4 and adam kingma and ba 2014 , respectively we use xavier initialization for all the parameters in our model we replace unk tokens with the most relevant token in the source table according to the attention matrix jean et al 2014 . the results in table come from the results of independent runs of the models baselines kn template kn the template-based kneser-ney kn language model reported in lebret grangier and auli 2016 . they used the kenlm tool to train a 5-gram models the extracted template for the biography in table is name name name ( birthdate ··· during inference the decoder is constrained to emit words from the vocabulary or the special tokens occurring in the table • nlm table nlm lebret grangier and auli 2016 proposed a neural language model nlm which ignores attribute information table nlm includes local and global conditioning on the tables by taking the attribute information into consideration 6789 models kn template kn nlm table nlm order-planning struct-aware our models seq2seq† two-level lstm† coarse-to-ﬁne attention† sequence labeling eq ) + auto-encoder eq & ) + multi labeling eq 10 ‡ bleu 2.21 19.80 4.17 34.70 43.91 rouge 0.38 10.70 1.48 25.80 37.15 44.89 0.33 41.21 0.25 43.37 0.32 44.14 0.24 44.42 0.21 44.63 0.29 44.84 0.27 45.14 0.34 39.78 0.21 40.25 0.15 40.37 0.26 40.64 0.32 40.91 0.19 41.26 0.37 table : automatic evaluation on the wikibio dataset ‘ a means this model adds module a to the last model all the baselines are reported by their authors models marked by use word-attribute concatenation fig ( a ) to incorporate attribute information while models marked by use auxiliary sequence labeling supervision fig ( ) . models seq2seq struct-aware liu et al 2017a our best fluency quality 4.23 4.42 4.56 2.89 3.64 3.85 table : human evaluation larger values signiﬁes better performances for systems the generated cases of the stateof-the-art system liu et al 2017a are provided by the authors the pearson coefﬁcients of the annotators scores on the generation ﬂuency and quality are 0.77 and 0.68 respectively both p-values less than 0.001 . order-planning sha et al 2017 proposed a link matrix to model the order for the attribute-value tuples while generating the related biography • struct-aware liu et al 2017a proposed a structureaware learning which include the ﬁeld-gating mechanism which input the attribute name embedding into the lstm and the dual attention mechanism to incorporate the attribute information analysis for human automatic evaluation the automatic evaluation table ) shows that our proposed model outperform the seq2seq baseline by about 1.8 bleu and 1.5 rouge and also beat the state-of-the-art system liu et al 2017a . the proposed two-level lstm encoder with coarse-to-ﬁne attention brings 1.1 bleu and 0.6 rouge increase compared with the vanilla seq2seq model table shows that the improvement of the two-level lstm structure comes from better representation of the attributevalue structure of the tables rather then increased parameters the auxiliary supervisions from different resources further enhance the model performance human evaluations in encoder two-level lstm ours one-layer lstm two-layer lstm bi-directional lstm bleu rouge 44.14 43.37 43.25 43.41 40.25 39.78 39.53 39.64 table : the comparison of the proposed two-level lstm encoder with stacked lstm encoders it shows that the improvement of the two-level hierarchical encoder does not come from the increased model parameters s2s edward merrill root january , 1895 october 26 1973 was an american educator and poet liu et al 2017a edward merrill root january , 1895 october 26 1973 was an american educator and poet from richmond indiana our best edward e. merrill root january , 1895 in baltimore october 26 1973 in kennebunkpot was an american educator and poet best known for his anti-communist activities table : the generated biographies and associated human evaluation scores for table from systems all the biographies are scored 5.0 for the ﬂuency evaluation the average scores of generation quality for the systems are 2.67 3.67 and 4.0 respectively table also shows that the generated cases from our model have higher quality than the seq2seq baseline and the stateof-the-art system ablation studies we ﬁrst introduce the baselines in table and : hs2s concat the proposed hierarchical encoder with two-level lstm and coarse-to-ﬁne attention fig ) using word-attribute concatenation fig ( a ) • hs2s gating the proposed hierarchical encoder with ﬁeld-gating mechanism liu et al 2017a . we feed the attribute name embedding into both the word-level and bleu models 44.42 hs2s concat 44.57 hs2s gating 44.63 hs2s sl-mlp 44.54 hs2s sl-crf a different ways to incorporate attribute information acc % models hlstm mlp hlstm crf hs2s sl-mlp hs2s sl-crf b the accuracy of sequence labeling 94.01 94.30 93.84 94.07 table : the analysis of auxiliary sequence labeling supervision we show that the proposed sequence labeling auxiliary task performed better than word-attribute concatenation fig ( a ) and attribute-gating method liu et al 2017a ( left . the accuracies of the sequence labeling multi-task are comparable with competitive baselines right . 6790 models s2s co-training eq ) dis minimizing eq ) hs2s concat co-training eq ) dis minimizing eq ) hs2s sl-crf co-training eq ) dis minimizing eq ) bleu 43.37 rouge 39.78 43.69 +0.32 44.04 +0.67 40.02 +0.24 40.22 +0.44 44.42 40.37 44.64 +0.22 44.68 +0.26 40.68 +0.31 40.74 +0.37 44.63 40.64 44.76 +0.13 44.84 +0.21 40.86 +0.22 40.91+ 0.27 ( a ablation studies on auto-encoder supervision models s2s multi-labeling eq 10 hs2s concat multi-labeling eq 10 hs2s sl-crf multi-labeling eq 10 bleu 43.37 44.42 44.63 rouge 39.78 40.37 40.64 43.69 +0.32 40.02 +0.24 44.77 +0.35 40.63 +0.26 45.01 +0.38 41.06 +0.42 ( ) ablation studies on multi-labeling supervision table : ablation studies on the auto-encoder and multilabeling supervision attribute-level lstm units please refer to liu et al 2017a for more details • hs2s sl-mlp/crf the proposed hierarchical encoder which integrates attribute-level information by joint training the auxiliary sequence labeling task with the seq2seq learning the label function in eq can be mlp or crf • hlstm mlp/crf the independent sequence labeling model using the two-level hierarchical lstm we can also use mlp or crf as the label function table ( a shows that the proposed sequence labeling auxiliary task is a better way to incorporate attribute information than word-attribute concatenation fig ( a ) and the ﬁeld-gating mechanism liu et al 2017a . table ( ) on the other hand shows the accuracy of the sequence labeling auxiliary task on the test set of wikibio the joint training models hs2s sl-mlp/crf also achieve comparable sequence labeling accuracies with the independent sequence labeling models hlstm mlp/crf . the sequence labeling auxiliary task can only be operated on the proposed two-level lstm the auto-encoder and multi-labeling supervision on the other hand can be applied to all seq2seq-like models table shows that both auto-encoder and multi-labeling supervision can be helpful to encode the structured tables and improve the table-to-text generation we notice more increase on the automatic evaluations on the vanilla seq2seq model with the help of the auto-encoder value key bobby fenwick name inﬁelder position 10 december 1946 birthdate naha okinawa birthplace april 26 1972 debutdate debutteam houston astros finaldate finalteam may , 1973 louis cardinals gold robert richard fenwick december 10 1946 in naha okinawa , is a retired major league baseball player who played inﬁelder from 1972 1973 s2s robert joseph fenwick born december 10 1946 in naha okinawa is a former major league baseball inﬁelder + hierarchical encoder robert fenwick born december 10 1946 in naha okinawa is a former major league baseball inﬁelder who played for houston astros and louis cardinals + auxiliary supervision robert fenwick born december 10 1946 in naha okinawa is a former major league baseball player who played as inﬁelder for houston astros and louis cardinals from 1972 to 1973 table : the generated biographies for bobby fenwick in the wikibio supervision table ( a ) we believe this is because the less expressive encoder might beneﬁt more from the auxiliary auto-encoder supervision from the related text for the multi-labeling supervision all the baselines can get about 0.3 increase on both bleu and rouge metrics generation analysis we offer generated cases table and table ) for the infoboxes in the test set of wikibio table shows that the generated biography by our model get higher human evaluation scores than its counterpart provided by the state-ofthe-art system liu et al 2017a . in this case our system includes the information in the known for attribute similarly the generated biography by our system in table contains the teams where bobby fenwick played for as well as the time span in these teams disordered tables most previous work lebret grangier and auli 2016 liu et al 2017a sha et al 2017 viewed the attributes in the source tables as a ordered list and then feed them sequentially into the table encoder actually as proved by sha et al 2017 liu et al 2017a , the order of the attributes does inﬂuence the generation quality the conclusion makes perfect sense especially for wikipedia infoboxes and associated biographies as the human editors tend to describe a person in a relatively ﬁxed order for example most biographies accords with the name-birthdate-nationality-occupation-··· pattern the attributes in the infoboxes are usually arranged in the proper order however not every table is guaranteed to have the appropriate order a robust model should achieve constantly 6791 models s2s s2s auto-encoder eq & ) s2s multi-labeling eq 10 s2s ae ml eq , & 10 hs2s hs2s sl-crf eq ) hs2s auto-encoder eq & ) hs2s multi-labeling eq 10 hs2s ae ml eq , & 10 hs2s ae ml sl-crf full bleu 41.80 -1.57 43.19 -0.85 43.06 -0.63 43.89 -0.52 43.21 -1.21 43.46 -1.17 44.04 -0.64 44.30 -0.47 44.51 -0.43 44.58 -0.56 table : the performance of different models on the disordered tables which shows that the text auto-encoder and multi-labeling supervisions can make the models more robust to the disordered tables high performance even if the attributes are disordered so we test our model in the adversarial setting we randomly shufﬂe the attributes of the source tables in both training and testing set during the shufﬂing we only reorder the attributes of the source tables without changing the content in these attributes we shufﬂe for times and average the scores of the candidate models on the shufﬂed data sets during this process table shows that the shufﬂing does hurt the performance of our models however we ﬁnd that the proposed auto-encoder and multi-labeling auxiliary supervision can relieve the undesirable tendencies we believe this is because the two sources of external supervision can facilitate the table encoder to learn more accurate semantic representations for the source tables no matter how the attributes are ordered error analysis although our models have greatly improved the table-to-text generation we also ﬁnd some errors by case studies 1 the ﬁrst problem is the irrelevant information in the generated descriptions to the source tables it is a general problem in the seq2seq framework as we usually view the seq2seq models as the black boxes and can not easily debug the models according to the ill-formed generations 2 in some cases we need common sense knowledge to get the reasonable biographies for example when we say a retired basketball player , we should determine whether a man is retired or not according to the finaldate attribute 3 the information which needs some inference across several attributes like a time span may not be well represented by our model resentation a closely related work is the auto-encoder assistant proposed by ma et al 2018 which used the text autoencoder in the ﬁeld of abstractive summarization sequence labeling which labels the source sequences with pre-deﬁned labels hand-crafted domain-speciﬁc features were widely used in traditional methods like hmms and crfs lafferty mccallum and pereira 2001 mccallum and li 2003 . recently there are many attempts to build end-to-end systems for sequence labeling lample et al 2016 ma and hovy 2016 . our model is based on the success of lstm+mlp and lstm+crf models natural language generation tasks can be generally divided into two phases content selection ‘ what to say ) and surface realization ‘ how to say ) reiter and dale 1997 2000 . many previous work barzilay and lee 2004 barzilay and lapata 2005 2006 yu et al 2007 liang jordan and klein 2009 treated the task as a pipelined systems which viewed content selection and surface realization as two separate tasks duboue and mckeown 2002 proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base in a similar vein barzilay and lapata 2005 modeled the dependencies between the american football records and identiﬁed the bits of information to be verbalized ( liang jordan and klein 2009 angeli liang and klein 2010 extend the work of barzilay and lapata to soccer and weather domains by learning the alignment between data and text using hidden variable models most recent work treated natural language generation in an end-to-end fashion mei bansal and walter 2016 lebret grangier and auli 2016 wiseman shieber and rush 2017 xu et al 2018 lin et al 2018 luo et al 2018 liu et al 2017b wang et al 2017 with the help of attention mechanism bahdanau cho and bengio 2014 luong pham and manning 2015 luo et al 2018 wu et al 2018 . conclusion many tables have complex attribute-value hierarchical structure and large number of attributes across different domains so it is hard for a single encoder to learn the accurate semantic representation of the source tables to this end we ﬁrst propose a two-level hierarchical encoder with coarse-to-ﬁne attention to encode the tables furthermore we also propose auxiliary tasks to assist the table encoder namely auxiliary sequence labeling task text auto-encoder and multi-label classiﬁcation the experiments on the wikibio dataset show that our models achieve the state-of-the-art performance and are more robust in the adversarial setting related work acknowledgments auto-encoder has been shown to be effective to learn the internal representations vincent et al 2008 for the source data in various domains including natural language understanding ap et al 2014 , speech recognition deng et al 2010 lu et al 2013 and image representation krizhevsky and hinton 2011 . our model utilizes a biography autoencoder to supervise the learning for the related table rep we would like to thank xiaodong li wei wu and kexiang wang as well as the anonymous reviewers for the helpful discussions and suggestions our work is supported by the national science foundation of china under grant no 61876004 no 61751201 and no m1752013 the corresponding authors of this paper are baobao chang and zhifang sui 6792 references angeli g. liang p. and klein d. 2010. a simple domainindependent probabilistic approach to generation in emnlp 2010 502–512 ap s. c. lauly s. larochelle h. khapra m. ravindran b. raykar v. c. and saha a 2014. an autoencoder approach to learning bilingual word representations in nips 1853–1861 bahdanau d. cho k. and bengio y 2014. neural machine translation by jointly learning to align and translate corr abs/1409.0473 bao j. tang d. duan n. yan z. lv y. zhou m. and zhao t. 2018. table-to-text describing table region with natural language arxiv preprint arxiv:1805.11234. barzilay r. and lapata m. 2005. collective content selection for in proceedings of the conference on concept-to-text generation human language technology and empirical methods in natural language processing 331–338 barzilay r. and lapata m. 2006. aggregation via set partitioning for natural language generation in hlt-naacl 359–366 barzilay r. and lee l. 2004. catching the drift probabilistic content models with applications to generation and summarization arxiv preprint cs/0405039 deng l. seltzer m. l. yu d. acero a. mohamed a.-r. and hinton g. 2010. binary coding of speech spectrograms using a deep auto-encoder in eleventh annual conference of the international speech communication association duboue p. a. and mckeown k. r. 2002. content planner construction via evolutionary algorithms and a corpus-based ﬁtness function in proceedings of inlg 2002 89–96 hachey b. radford w. and chisholm a 2017. learning to generate one-sentence biographies from wikidata in eacl 2017 633–642 hochreiter s. and schmidhuber j 1997. long short-term memory neural computation ( ) :1735–1780 jean s. cho k. memisevic r. and bengio y 2014. on using very large target vocabulary for neural machine translation arxiv preprint arxiv:1412.2007. kingma d. p. and ba j 2014. adam a method for stochastic optimization arxiv preprint arxiv:1412.6980. krizhevsky a. and hinton g. e. 2011. using very deep autoencoders for content-based image retrieval in esann lafferty j. mccallum a. and pereira f. c. 2001. conditional random ﬁelds probabilistic models for segmenting and labeling sequence data lample g. ballesteros m. subramanian s. kawakami k. and dyer c. 2016. neural architectures for named entity recognition arxiv preprint arxiv:1603.01360. lebret r. grangier d. and auli m. 2016. neural text generation from structured data with application to the biography domain in emnlp 2016 1203–1213 liang p. jordan m. i. and klein d. 2009. learning semantic correspondences with less supervision in acl 91–99 lin j. sun x. ren x. li m. and su q 2018. learning when to concentrate or divert attention self-adaptive attention temperature for neural machine translation arxiv preprint arxiv:1808.07374. lin c.-y 2004. rouge a package for automatic evaluation of summaries text summarization branches out liu t. wang k. sha l. chang b. and sui z 2017a table-totext generation by structure-aware seq2seq learning arxiv preprint arxiv:1711.09724 6793 liu t. wei b. chang b. and sui z 2017b large-scale simple question generation by template-based seq2seq learning in national ccf conference on natural language processing and chinese computing 75–87 springer lu x. tsao y. matsuda s. and hori c. 2013. speech enhancement based on deep denoising autoencoder in interspeech 436–440 luo l. xu j. lin j. zeng q. and sun x 2018. an auto-encoder matching model for learning utterance-level searxiv preprint mantic dependency in dialogue generation arxiv:1808.08795. luong m.-t. pham h. and manning c. d. 2015. effective approaches to attention-based neural machine translation arxiv preprint arxiv:1508.04025. ma x. and hovy e. 2016. end-to-end sequence labeling via bi-directional lstm-cnns-crf arxiv preprint arxiv:1603.01354. ma s. sun x. lin j. and wang h. 2018. autoencoder as assistant supervisor improving text representation for chinese social media text summarization arxiv preprint arxiv:1805.04869. mccallum a. and li w. 2003. early results for named entity recognition with conditional random ﬁelds feature induction and web-enhanced lexicons in hlt-naacl 2003 188–191 mei h. bansal m. and walter m. r. 2016. what to talk about and how selective generation using lstms with coarse-toﬁne alignment in naacl hlt 2016 720–730 papineni k. roukos s. ward t. and zhu w.-j 2002. bleu a method for automatic evaluation of machine translation in acl2002 311–318 perez-beltrachini l. and lapata m. 2018. bootstrapping generators from noisy data arxiv preprint arxiv:1804.06385. reiter e. and dale r. 1997. building applied natural language generation systems natural language engineering ( ) :57–87 reiter e. and dale r. 2000. building natural language generation systems cambridge university press sha l. mou l. liu t. poupart p. li s. chang b. and sui z 2017. order-planning neural text generation from structured data arxiv preprint arxiv:1709.00155. vincent p. larochelle h. bengio y. and manzagol p.-a 2008. extracting and composing robust features with denoising autoencoders in icml 1096–1103 acm wang k. liu t. sui z. and chang b 2017. afﬁnity-preserving random walk for multi-document summarization in proceedings of the 2017 conference on empirical methods in natural language processing 210–220 wiseman s. shieber s. m. and rush a. m. 2017. in data-to-document generation arxiv preprint arxiv:1707.08052. wu w. wang h. liu t. and ma s. 2018. phrase-level selfattention networks for universal sentence encoding in proceedings of the 2018 conference on empirical methods in natural language processing 3729–3738 xu j. zhang y. zeng q. ren x. cai x. and sun x 2018. a skeleton-based model for promoting coherence among sentences in narrative story generation arxiv preprint arxiv:1808.06945. yu j. reiter e. hunter j. and mellish c. 2007. choosing the content of textual summaries of large time-series data sets natural language engineering 13 1 :25–49 augmenting transformers with knn-based composite memory for dialog angela fan facebook ai research universit´e de lorraine loria angelafan fb.com claire gardent cnrs/loria claire.gardent loria.fr chlo´e braud cnrs/irit chloe.braud irit.fr antoine bordes facebook ai research abordes fb.com abstract various machine learning tasks can benefit from access to external information of different modalities such as text and images recent work has focused on learning architectures with large memories capable of storing this knowledge we propose augmenting generative transformer neural networks with knnbased information fetching kif modules each kif module learns a read operation to access fixed external knowledge we apply these modules to generative dialog modeling a task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation we demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from wikipedia images and human-written dialog utterances and show that leveraging this retrieved information improves model performance measured by automatic and human evaluation 1 introduction machine learning approaches to various tasks such as game-playing or dialog are often dependent on external information this information can take multimodal forms including structured knowledge bases free text and images and also comes in overwhelmingly large quantities a pressing is to create models that can identify which specific elements of multiple information sources are relevant in a particular context and incorporate them into standard archi 82 tectures on each task in this work we focus on human–machine dialog and how to efficiently retrieve external knowledge that is relevant to the dialog we consider two scenarios and for each scenario retrieve two types of knowledge ( ) knowledge about similar dialog contexts and ii external knowledge used to ground the conversation into real world information knowledge about similar dialog contexts allows for a hybrid retrieval/generative approach to dialog where the system response is generated based not only on a representation of the current dialog context and of the relevant world knowledge but also based on a response retrieved from a similar dialog context the retrieved knowledge can be viewed as providing information about structure and dialog sentences or utterances which response is likely given a similar context external knowledge is also retrieved to improve the semantic content of the dialog model in one scenario wizard of wikipedia dinan et al 2018 , general topics are provided to crowdworkers who are asked to have in-depth and specific conversations about these topics by referencing specific wikipedia sentences as knowledge in this scenario external knowledge is retrieved from a pre-selected set of wikipedia sentences associated with the current dialog topic retrieval aims to select the sentence that is most relevant at each step of the dialog and thereby to ground system responses in relevant world knowledge e.g. by referring to star wars when talking about science fiction . in the other scenario engaging imagechat shuster et al. 2020 , crowdworkers are provided with images and asked to have a conversation transactions of the association for computational linguistics vol 9 pp 82–99 2021. : //doi.org/10.1162/tacl a 00356 action editor masaaki nagata submission batch 6/2020 revision batch 9/2020 published 3/2021 c cid:13 2021 association for computational linguistics distributed under a cc-by 4.0 license inspired by or about the image in this case the retrieved external knowledge is images and their associated dialogs by retrieving images that are similar to the image being talked about we aim to enrich system responses with knowledge about what is typically mentioned when describing similar images e.g. when talking about an image with dogs mentioning their breed . our work on incorporating different types and modalities of knowledge is related to methods that strive to add external memory such as knowledge bases to neural networks previous work has explored incorporating large external memories into neural network layers weston et al. 2015 sukhbaatar et al. 2015 2019 lample et al. 2019 . many existing approaches focus on using attention over the memory slots which is computationally intensive and becomes less effective as the the size of the memory grows in this work we propose representing multiple sources of external information as fixed encodings and using nearest neighbors knn search to fetch relevant information knn search is computationally efficient and scalable and libraries like faiss johnson et al. 2019 allow knn to be easily used on gpus and integrated into neural networks further the external memories are pre-encoded so the information encoding is only computed once as the external memories are kept fixed they do not require any training to learn the memories along with the model we can thus scale easily to larger memories by learning only the knn-based read operation to identify relevant information from the memory our core contribution proposes an efficient knn-based information fetching kif module that can access relevant external knowledge combine knowledge from different sources and integrate this information into standard sequence to sequence architectures we apply these flexible modules to two dialog datasets that generative models to leverage external information to write coherent on-topic responses both of our chosen tasks require models to leverage external information such as information from wikipedia or images to engage in the conversation we show that relevant information can be identified from hundreds of thousands of candidates in a multimodal multi-knowledge-source setting to improve the performance of generative dialog models further the output of the kif modules is interpretable as specific human-readable know ledge elements are selected allowing users to better understand the information the generative model conditions upon when writing the subsequent utterance on both datasets we achieve state-of-the-art results compared to generative models and find there is no statistically significant difference in the interestingness or human preference of our model output compared to stateof-the-art retrieval models 2 related work we discuss related work on learning to incorporate external knowledge into neural networks and efficiently access relevant information we then describe work in generative dialog that incorporates knowledge 2.1 incorporating external knowledge line of augmenting neural networks with memory or longer-term components that can be accessed with read and write operations has been explored in various proposed architectures for example memory networks weston et al. 2015 sukhbaatar et al. 2015 2019 introduce attention mechanisms over large external memories neural cache models grave et al. 2017b simplify these to access previous memories with a dot product previous work has also studied how to read and write into these memory architectures rae et al. 2016 graves et al. 2014 joulin and mikolov 2015 . in contrast we focus on how to read large memories another research has focused on computational scalability for larger external memories to allow efficient access of information for example chandar et al ( 2016 propose a hierarchical memory network rather than a flat one and rae et al ( 2016 learn sparse operations to read and write lample et al ( 2019 focus on learning memories of up to one million slots and how to efficiently access the slots using product keys khandelwal et al ( 2019 use nearest neighbor operations to augment language models by performing retrieval at the token level—in contrast we focus on multimodal retrieval of multiple pieces of knowledge based on an entire dialog context beyond explicit memory representations it may be possible to store information implicitly during training time by memorizing common patterns present in text petroni et al. 2019 . we focus on learning 83 to fetch relevant information from multiple explicit external multimodal knowledge sources and integrate them into one network further our work allows the retrieved information to be interpreted as each memory slot is an explicit fact that can be read as text rather than a learned vector such as in lample et al ( 2019 . work has also focused on computationally efficient softmax operations mnih and hinton 2009 grave et al. 2017a chen et al. 2016 . many approximate softmax techniques use knnlike operations to form clusters and the overall softmax operation is constrained by the slow calculation of the exponential our usage of knn benefits from efficient and scalable libraries such as faiss and nmslib 2.2 generative dialog we develop a general architecture for incorporating external information and apply it to the case of generative dialog models previous work in dialog has leveraged knowledge as necessary information to accomplish the task for example airline and restaurant booking tasks often use api calls to access information about reservation times and availability bordes et al. 2017 . in contrast our work focuses on how to incorporate unstructured knowledge such as free text found on the web previous work has used architectures that attend over the available knowledge and identify relevant pieces of information which scales poorly with large quantities of information dinan et al. 2018 qin et al. 2019 lian et al. 2019 . we replace the use of attention over external information with the output of a knn module other work has investigated incorporating information retrieval in language modeling and question answering chen et al. 2017 fan et al. 2019 seo et al. 2019 guu et al. 2020 , while we focus on dialog applications and flexibly incorporating knowledge from multiple multimodal sources on the modeling side work has explored both generative serban et al 2016a 2016b and retrieval based models zhang et al. 2018 , which identify the best utterance from the training set to return as the dialog response this often leverages self-attention or cross-attention mechanisms humeau et al. 2019 . further work has explored hybrid models for example using the output of a retrieval model as input for a generative model dinan et al. 2018 weston et al. 2018 cai et al. 2019 zhu et al. 2020 . some of this work has specialized to use both types of models to generate conversations in an ensemble song et al. 2016 or to specifically improve consistency song et al. 2020 . we extend these approaches by augmenting generative models with retrieval-like operations based on knn search allowing dialog models to flexibly incorporate various sources of external knowledge at the same time and scale to large quantities of retrieval candidates 3 knn-based information fetching modules the kif module assumes an enbroadly inputs = coder model can access x1 x2 . . xn . for example x can be a collection of sentences and xi represents an individual sentence in a setting without additional supporting information the encoder will process an input xi and produce the encoder output ( xi . if xi is a sequence such as a sentence then ( xi is a representation of the variable size of the sequence length by the fixed size encoder ’ hidden size however in many tasks additional information is present represented as = e1 e2 . . em . we encode each element of and into a vector representation using the encoder to identify the closest information in that is relevant to xi our general approach will be to use knn by comparing the representation of xi with the representation of each element in the set e. knn is a fully differentiable operation pl¨otz and roth 2018 , so can be incorporated in a straightforward way into neural models the most relevant information in will then be available in the model we display a kif-augmented model in figure and describe how the kif module operates the that embeddings of one to overcome is the representation of all elements of the knowledge source are pre-computed and kept fixed creating ( ) —we do not backpropagate to affect the pre-encoded knowledge in the early stages of training the model receives large amounts of loss which would affect the quality of the pre-encoded embeddings if we backpropagated to them further encoding the fixed external knowledge once and re-using it allows for greater scalability however this lack of backpropagation can introduce a mismatch 84 figure : kif modules fetch relevant information from multimodal external knowledge external knowledge sources e1 and e2 are pre-encoded by encoder ( green . in the model input xi is encoded by encoder ′ blue to produce ′ xi . kif modules orange operate on ′ xi and identify the nearest neighbors encoded in ( e1 and ( e2 using knn identified relevant elements from e1 and e2 are re-encoded by ′ in a gating mechanism with a weighted sum represented by ( ws1i · ws1i where ws stands for weighted sum , then concatenated to ′ xi . full description with notation can be found in section 3. between the encoding of and the encodings produced by a model that is training as the training model has constantly changing representations because the weights are being learned we use to represent the original encoder model used to encode and ′ to represent the constantly training model that is encoding x. the model must learn a function to align ′ xi to the pre-encoded elements of the external memory ( ) to circumvent this misalignment we learn a mapping operator fe m ( xi ) that trains to map elements of the model s representation of , or ′ x , into the additional information representation space ( ) concretely fe m ( xi ) is a multilayer perceptron with relu nonlinearities from the input elements of , fe m ( xi ) learns representations of an output close to the corresponding projection of into e. this can be interpreted as learning a read operation on a fixed external memory if there was no change to the encoding of the model compared to the pre-computed knowledge then the ideal mapping operator would be the identity function as ′ would equal ) however as the model changes significantly during the training process the nonlinear mapping capability of fe m ( xi ) is essential to be able to identify the correct knowledge from the input x. thus a model augmented with kif will incorporate external knowledge in the following manner first we find the nearest elements to fe m ( xi ) in ( ) based on knn search with inner product then the relevant elements identified by knn are re-encoded by ′ for example if element ej is retrieved by kif it would produce ′ ej . we use the optimized faiss library for knn search which can conduct billion-scale knn efficiently on gpus the knn output for an element xi is produced by using faiss to search for the nearest representations to fe m ( xi ) in ( ) note that as the encoders and ′ produce output representations of variable length for example in the case where xi is a variable length sequence such as a sentence , we average across the length dimension to produce a fixed-size representations to conduct the knn search rxi avg cid:0 fe m ( xi ) cid:1 re ( ) avg m e ) e e cid:9 knnxi knearest cid:0 k rxi re cid:1 ( ) 2 ( ) then the kif module output for an element xi is the set of all re-encoded representations of the knn-retrieved knowledge kifxi ( ) ′ e | ∈ knni cid:9 ( ) these elements are weighted by their normalized nearest neighbor scores and then summed this is subsequently concatenated to ′ xi to form the final encoder output [ ′ xi , weightedsum kifi ] 5 85 instance simultaneously for this can be easily extended to using multiple two modules sources of external information e1 and e2 can be combined by identifying the top candidates of each information source the weighted sum of the kif output on each information source is concatenated with the encoded input ′ xi . the kif output dimensionality is the same size as the hidden size of ′ xi , so they can be directly concatenated finally different sources of information may not be required for every prediction and some information sources can be more important than others to allow the model to make more finegrained decisions about what information to use from what source and how much of it we add a gating mechanism using a sigmoid function around each weighted sum of knn representations kif1i and kif2i denote the kif module from equation 4 applied to e1 and e2 respectively ws1i weightedsum kif1i ws2i weightedsum kif2i ( ) 7 which produces the final encoder output a concatenation of ′ xi with the output of multiple kif modules ( ) ′ xi , ( ws1i · ws1i σ ws2i · ws2i cid:3 ( ) this concatenation represents the output of the encoder ′ and can be used for various purposes such as providing the encoder output to a decoder in a sequence to sequence model 4 applying kif to dialog tasks we describe how to apply kif to the task of generative dialog a setting where models must generate engaging and on-topic responses we investigate dialog for two reasons first dialog agents must be able to consult relevant information to maintain the topic of the conversation second retrieval-based agents have strong performance compared to generative ones due to their ability to copy dialog utterances from the training set using kif we can incorporate the benefits of retrieval architectures into generative knowledge-based models 4.1 kif for generative dialog in dialog xi represents the text of the conversation i. a conversation consists of multiple backand-forth utterances or turns . for example a conversation could consist of turns xi [ xi,1 xi,2 xi,3 xi,4 where xi,4 the direct utterance the model should respond to and the earlier utterances are the conversation context is standard generative dialog models use a transformer neural network as the encoder and want to produce an output that is an appropriate response to the conversation however in many cases the conversation history alone does not include all of the information required to produce an appropriate response for example if a model needs to chat about a specific movie it can be helpful to provide the model with more information about that movie so a more interesting dialog response could be produced to incorporate knowledge models often concatenate a knowledge source such as wikipedia to xi and use attention modules to identify the most relevant knowledge however this approach is computationally intensive when handling large quantities of information further attention mechanisms have been found to operate poorly over long sequences as the mechanism becomes blurry due to the softmax and struggles to make fine-grained decisions fan et al. 2018b . the same is true for hierarchical approaches which lack scalability we augment transformer sequence to sequence seq2seq networks on the encoder side with kif to improve generative dialog models we experiment on two dialog tasks wizard of wikipedia dinan et al. 2018 and engaging imagechat shuster et al. 2020 . in both datasets models must leverage information external to the dialog history alone—in wizard of wikipedia the chat requires access to knowledgeable facts and in engaging imagechat discussion about a specific image as models must process multiple inputs and ground responses in the knowledgeable facts or images these tasks existing seq2seq approaches 4.2 wizard of wikipedia the goal of the wizard of wikipedia dataset is to train knowledgeable agents that can chat in any domain the dataset contains 1,365 various topics discussed in 18,430 dialogs in the training set 86 totalling 166,787 training utterances each topic is a general concept such as dogs or ice cream and is included as the first utterance of the conversation the conversation is meant to be in-depth and detailed so individual utterances must reference specific knowledge as a basis for the utterance the knowledge takes the form of wikipedia sentences for example the chat utterance love toy story it was released in 1995 would reference the wikipedia sentence toy story is a 1995 american computer-animated buddy comedy ... . for each utterance a set of sentences are identified by an information retrieval system and the crowdworker selected one knowledge sentence as the basis for their utterance knowledge sources our model for wizard of wikipedia has access to two sources of external information e1 and e2 • e1 is wikipedia knowledge provided by the dataset as evidence to support knowledgeable chitchat initially curated by the information retrieval system used in dinan et al [ 2018 ) the scale of this knn search is to filter through an average of 34 sentences the kif module uses dialog features to fetch relevant knowledge to condition upon to generate the subsequent utterance • e2 is training utterances to incorporate the benefits of retrieval-based dialog models to the generative setting we use kif to identify relevant utterances from the training set and take their responses as input if many conversations about dogs have already occurred models should be able to take advantage of these human-written examples to improve their generations for example likely conversation could occur about the breed of the dog daily routine with a pet and similar topics there are around 170k dialog utterances as inputs to knn search this can be interpreted as incorporating the benefits of retrieval models by identifying an utterance with similar structure as the text the model would like to generate we do not allow the module to fetch the correct response of the current conversation context access to these two sources of knowledge can be seen as learning a template and a topic separately sample templates can be identified from the training utterances and topic-specific information learned by accessing the wikipedia knowledge additional knn features to better identify relevant training utterances from the large quantity available we break down xi into conversation sub-features for a more fine-grained match in the knn search step by conducting knn on more features we can achieve higher quality retrieval we leverage the nature of dialog to these features we concatenate the encoding of the most recent dialog utterance e.g. xi last with the encoding of the dialog context from the current conversation and the turn number , such that ′ xi last , ′ xi −last , is the representation used for knn search concretely if the model is trying to produce the 5th turn of the conversation then xi last is the most recent utterance from the dialog partner xi −last would be the last turns of exchange and would be 4. note that the turn number is represented as a standalone number these are known to be salient conversation features the most recent dialog utterance is the direct turn the model is responding to and the dialog context may provide additional clues the turn number is important as earlier turns are often generic e.g. how are you doing today and later turns are more specific 4.3 engaging imagechat the goal of engaging imagechat is to create agents capable of chitchatting about images selected from the yffc100m dataset thomee et al. 2016 . the dataset contains 186,782 dialogs in the training set each about a unique image totalling 355,862 utterances agents are assigned one of 215 personalities e.g. sweet caring excited to increase engagingness previous work shuster et al. 2020 2019 identified that both crowdworkers and models when provided with personalities produced more diverse interesting responses as evaluated by humans we use a multimodal neural network designed to handle both image input and text input following shuster et al ( 2020 , the images are encoded using a pre-trained resnext network xie et al. 2017 . to extract the final image representation we project the 2048-dimensional output of the image encoder to 512-dimensions using a deep multilayer perceptron with relu activation units the conversation history which 87 includes the one-word personality is encoded with a transformer encoder network the image and conversation are integrated using the multimodalsum-combiner module proposed in shuster et al ( 2020 . then the turn number and personality are represented separately as the personality is a word we use the same transformer to encode it the concatenation of features used for knn search is m ( xi last , ′ xi −last , , p. knowledge sources our model for engaging imagechat has access to two sources of external information e1 and e2 5 experimental setup 5.1 implementation details e1 is chat on similar images although there are over 180k different images in this dataset many of the images are similar for example conversations associated with two pictures of dogs could be relevant to each other the model is able to use kif directly on the current image features to fetch from around 180k different images and return turns of related chat for each fetched image fetching from e1 consists of identifying related image chats or conversations on related topics • e2 is training utterances similar to the motivation for the previous dataset we allow the model to identify training utterances that could be useful for responding in the current conversation the scale of this fetching task is large 350k dialog utterances this could be interpreted as identifying utterances with similar structure to what the model would like to generate and is complementary to the topic-based related image chats additional knn features to identify relevant information from training utterances we use the same dialog features as wizard of wikipedia in the knn search step with one modification we add the personality provided by the dataset we represent the personality feature as the personality word such as caring and embed it with the encoder ′ as utterances from speakers with the same personality are more likely to be similar this feature improves the quality of the fetched information for example conversations with the sweet personality often include similar text such as aww that s wonderful we use two additional features for the knn search t the turn number and , the personality this feature is explicitly used in shuster et al ( 2020 to improve the engagingness and flow of the conversation similar to wizard of wikipedia we represent the conversation turn as a number the transformer model is used to encode text xi and produce a representation of the text parameter settings we use parl.ai miller et al. 2017 to implement our models the data for both datasets used is available for download from parl.ai as well we use byte-pair encoding sennrich et al. 2016 to represent the text to better handle the rare word problem dinan et al. 2018 fan et al. 2018a . our generative transformer models have encoder layers and decoder layers with ffn size 2048 embedding dimension 512 and attention heads we optimize using adam kingma and ba and the inverse square root learning schedule vaswani et al. 2017 with 10k warmup updates the initial learning rate is 0.0001 and we optimize for model perplexity we use a dropout of 0.5 and set gradient clipping to 0.1. we set = for all cases for both datasets we model a vocabulary size of 54,944 based on the bpe-based vocabulary from the reddit pretraining we tuned the learning rate and batchsize hyperparameters together pre-training we pre-train the transformer seq2seq model used for both datasets on 250m comments from reddit the reddit dataset was made available by pushshift.io the comments are parsed to maintain conversational threads of users responding to each other so the encoder network has been exposed to conversational context at the reddit dataset does not include aspects such as personality as those are unique to specific datasets such as engaging imagechat the context size in pre-training is set to 512 tokens the resnext encoder used to model images for the engaging imagechat dataset was pre-trained on 3.5 billion images mahajan et al. 2018 . training time note that 5.2 evaluation generation we generate with beam search setting the beam size to 4. we use 3-gram blocking this technique disallows repeated n-grams from being generated multiple times and reduces repetition 88 automatic metrics following dinan et al ( 2018 , we compute f1 a metric of unigram overlap between the generated utterance and the human-written reference utterance from the dataset for generative models utterances are generated using beam search for retrieval models the next utterance is predicted by ranking the entire set of training utterances and the highest scoring utterance is chosen in wizard of wikipedia there are two test sets a set of seen topics or topics that have been seen at training time with new test-time dialogs the second set is unseen or topics that have not been encountered at all during training time we evaluate on both subsets human evaluation we follow the setup and use the analysis questions proposed in the acute-eval dialog evaluation system li et al. 2019 . for reproducibility we adopt this existing evaluation setting that has been applied to several dialog datasets we use the question wording suggested by acute-eval and follow their self-chat procedure and interface as one of the original datasets assessed in this system was wizard of wikipedia their evaluation setting extends naturally to ours we collect 100 humanbot conversational dialogs on a crowdsourcing platform for both datasets the dialogs are eight turns long then we show pairs of the collected conversations side by side one conversation with a human and model a and the other conversation with a human and model b. we ask annotators the following questions • who would you prefer to talk to for a long conversation • if you had to say one of the speakers is interesting and one is boring who would you say is more interesting • which speaker sounds more human • which speaker has more coherent responses in the conversation • if you had to say that one speaker is more knowledgeable and one is more ignorant who is more knowledgeable ( wizard of wikipedia only we measure the percentage of time one model was chosen over the other taking the majority agreement between three evaluators to reduce variance dialogs paired in the evaluation were collected on the same topic for wizard of wikipedia and collected on the same image and personalities for engaging imagechat topic and images selected for evaluation are unique and taken randomly from the test set 5.3 baselines we compare transformers augmented with kif to other existing approaches on wizard of wikipedia and engaging imagechat the best approaches judged by human evaluation are retrieval models the retrieval transformer memory network from dinan et al ( 2018 and the retrieval transformer from shuster et al ( 2020 . these have been shown to be strong baselines compared with other retrieval techniques based on tf-idf chen et al. 2017 . thus we report the existing retrieval models for both datasets but focus on comparing to other generative baselines we compare to three additional generative baselines note that in wizard of wikipedia the construction of the dataset is that sentences of wikipedia knowledge are provided with the utterances in a concatenated form models must identify the relevant information in this provided knowledge or can access more wikipedia knowledge beyond the provided sentences the following baseline methods always have access to the information provided in the datas et already but no additional wikipedia knowledge beyond that • transformer memory networks to contrast the ability of kif to existing work we compare our models to published transformer memory networks dinan et al. 2018 . these models encode each piece of external information independently with a transformer encoder and these are stored as memory slots to access information in the memory slots a model performs dotproduct attention between the memory slots and the dialog context in dinan et al ( 2018 , the knowledge selection from wikipedia was supervised with either a a two-stage model where the first model was trained to predict the right knowledge and a second model conditions on the predicted knowledge to generate the next utterance or b an endto-end model with an auxiliary loss for knowledge prediction accuracy • retrieve and refine we implement a hybrid model weston et al. 2018 that incorporates 89 top retrieval candidates as additional input to generative transformer memnets in retrieve and refine a fixed number of candidates are retrieved and concatenated to the conversational history in the encoder making the input much longer for both datasets the retrieve and refine mechanism that fetches a fixed number of training utterances is added to the generative transformer memnet with reddit pre-training baseline unlike the kif-augmented transformer the retrieval is conducted with a separate model so there is no backpropagation to affect the retrieval with kif models can alter the retrieved candidates by learning the mapping operator further a fixed amount of information is always retrieved without the capability to easily rescale to focus on specific candidates kif modules have weighting mechanisms to focus more on certain information and the modules are combined with gating so models can learn which knowledge sources are more important and adjust flexibly lastly retrieve and refine is only used to retrieve one source of information training set utterances • response generation with mr. we implement the model proposed in qin et al ( 2019 , which encodes the conversation history and document contextually with a bilstm before generating the next dialog utterance the initial model was applied to a machine reading task where a knowledge document was provided along with the conversation history for wizard of wikipedia we replace the knowledge document with the wikipedia sentences provided in the dataset the model then uses the conversation to identify the most relevant information in the document using a cross-attention mechanism for the engaging imagechat dataset as there is no document provided with the dataset we replace the expected document with the conversation history and use the most recent utterance in the conversation to attend to the conversation history more effectively as they are trained for dialog thus we replace cove embeddings with domain-specific ones all of transformer generative baselines are initialized with the same pre-training on reddit that we use for our models for fair comparison on modeling quality 6 results we describe the results of incorporating kif modules into transformer networks we display an example conversation between a human and our model in figure , and show the top scoring wikipedia knowledge and training utterance fetched by kif modules we compare to various baselines using automatic and human evaluation and discuss our experiments we present various ablation settings to understand the key features that make our method function 6.1 kif is effective for incorporating knowledge automatic evaluation comparing kif augmented transformer networks to published baselines and retrieve and refine we find improved results for wizard of wikipedia the improvement in f1 score over the best baseline is around points see table ) a major contributing factor is the construction of the dataset—as each dialog turn is grounded in a specific knowledge sentence from wikipedia improving the ability to identify the relevant fact strongly improves performance contrasting the results from the seen and unseen test sets in table , the improvement on unseen is worse—it is harder to fetch training utterances for unseen topics while imagechat has no explicit dependency on knowledge we still see a point improvement compared to the generative transformer memnet with the additional reddit pre-training , indicating that kif can be generally useful see table ) compared to an even stronger baseline that we tune in this work retrieve and refine we see point improvement we make an additional improvement to this baseline in qin et al ( 2019 , the embeddings used pre-trained cove vectors mccann et al. 2017 . we found our reddit pretrained transformer embeddings to work human evaluation results are shown in figure 2. on both datasets we find there is large improvement over existing generative models green bars that is statistically significant for some of the evaluation questions evaluators agree that 90 model test f1 seen test f1 unseen retrieval baselines retrieval transformer memnet dinan et al. 2018 generative baselines 2-stage generative memnet dinan et al. 2018 generative transformer memnet dinan et al. 2018 + reddit pre-training retrieve and refine weston et al. 2018 response generation with mr qin et al. 2019 kif-augmented transformer 15.4 18.9 16.9 17.6 18.2 17.5 25.9 12.4 17.4 14.4 16.3 17.9 16.8 22.3 table : results on the wizard of wikipedia dataset we implement the retrieve and refine and response generation with mr approaches all with reddit pre-training and evaluate them on wizard of wikipedia the seen test set consists of conversations on topics seen at training time and the unseen test set consists of conversations about new topics that were not in the training set model retrieval baselines retrieval transformer shuster et al. 2020 generative baselines generative transformer memnet dinan et al. 2018 + reddit pre-training retrieve and refine weston et al. 2018 response generation with mr qin et al. 2019 kif-augmented transformer test f1 9.81 7.1 12.8 13.6 13.2 14.4 table : results on the engaging imagechat dataset we implement the generative transformer memory network retrieve and refine and response generation with mr approaches all with reddit pre-training and evaluate them on engaging imagechat kif-augmented transformers are generally more coherent and human-sounding compared to the generative memnet comparison with existing retrieval models shown in blue is more nuanced along the lines of existing work zhang et al. 2018 dinan et al. 2018 , we find that retrieval-based models score very well in human evaluations that ask how human or interesting a dialog sounds this is because retrieval models return human-written utterances from the training set and do not suffer from decoding mistakes present in generative 1in shuster et al ( 2020 , retrieval transformer models report hits n using a fixed candidate set of 99 distractor candidates and true candidate we compute f1 using their open-sourced model by scoring the entire training set of over 350k utterances with the model and taking the top scoring candidate as the response models for example on engaging imagechat while our model has significantly improved over the generative baseline see green bars in figure , right , it does not beat retrieval based methods in sounding more human or being more interesting see blue bars in figure , right . as the retrieval baseline returns human-written text for other humans to evaluate we hypothesize that humans score each other s writing quite well compared with generative models which we focus on improving retrieval models often produce longer text with more interesting nuanced vocabulary usage and do not make generation mistakes such as repetition these factors often lead to the stronger performance of retrieval models a surprising result is that kif-augmented transformers are more human sounding than 91 figure : human evaluation results on both datasets more than 50 indicates the knn model is preferred stars indicate statistical significance at < 0.05. retrieval models on wizard of wikipedia this is because the dataset s utterances are long and factual due to the tendency of crowdworkers to copy wikipedia sometimes humans chatting with the retrieval bot would respond uh . that s an interesting fact otherwise our model scores similarly to retrieval models with most evaluations not having statistically significant difference we conduct a second evaluation on the unseen test set of the wizard of wikipedia dataset results are shown in figure 3. trends are similar compared to the results on the seen test set though the preference for the kif-augmented transformer is greater over the retrieval baseline we hypothesize that because the unseen test set is on entirely held out topics the retrieval baseline can struggle to identify relevant utterances in contrast the kif-augmented transformer similar to the generative baseline from dinan et al ( 2018 , can use the generative capability to produce utterances lastly we conduct an additional study to examine the variance of the comparative dialog judgements the evaluation study for wizard of wikipedia is repeated three times on different days and evaluators who have answered on previous days are not allowed to evaluate again in any subsequent experiments overall we find reasonable interannotator agreement rates around 73 averaged across all evaluations which is similar to the agreement rates reported in li et al ( 2019 . we find there is greater variance on questions asking which dialog is more human and more interesting most likely as different evaluators can interpret these in different ways further we see that comparison with the retrieval model has less variance compared to the generative model possibly because the retrieval model s human written text is devoid of figure : human evaluation on the unseen test set of wizard of wikipedia more than 50 indicates the knn model is preferred stars indicate statistical significance at < 0.05. mistakes overall we find that the conclusions and statistical significance are stable across multiple evaluations 6.2 analysis of fetched knowledge example conversations from our kif-augmented generative model are shown in figure on wizard of wikipedia we find that relevant knowledge is identified that affects the content of the generated utterance for example the model finds knowledge sentences about disney movies as the human conversationalist starts the conversation discussing disney the model leverages the fetched knowledge to write the content of the generated utterance in a concrete example the fetched sentence disney announced intentions ... after the success of the incredibles leads the model to generate the utterance love the incredibles they are my favorite disney movie in contrast the model uses the form of the fetched training utterance often as a template for writing a response for example the model copies the training utterance ohhh . what do people with color blindness do to cope with the effects and starts the model generation with ohhh ... and continues with the question think toy story is a classic following the form of the selected training utterance 92 figure : conversation between human and kif-augmented transformer on wizard of wikipedia the top-scoring wikipedia knowledge and training utterances fetched by kif are displayed with model output figure displays the top-3 fetched training set utterances and knowledge sentences on the wizard of wikipedia dataset when responding to a human utterance kif modules can identify multiple relevant items in response to the human question about blue skies the 1946 movie the model identifies both the comedy film and the band finally the elements retrieved by kif modules provide a more interpretable understanding of what the model is conditioning upon to generate a dialog response in table , we display for the same dialog history changing the model s fetched training utterance and knowledge sentence for our own examples the model heavily incorporates our manual changes of the fetched information into the generated utterance for example changing the knowledge directly affects what the model generates as the favorite character—from buzz lightyear to mr potato head to slinky dog—while changing the fetched training utterance changes the form of the generated sentence 6.3 scaling kif to retrieval settings kif modules can be used in more realistic and settings for knowledge retrieval that test the scalability of the module in figure ( a , we compare the generative transformer memnet baseline with kif-augmented transformers in three settings the first is the standard wikipedia sentences provided by the dataset average 34 sentences . then we extend to providing the model with the full wikipedia article on average 57 sentences and finally to multiple wikipedia articles on average totaling 205 sentences , identified using the conversation s topic this increasing size of available knowledge could be realistic for settings where it is unclear what information is most relevant if filtering steps to preprocess the data remove potentially relevant information or if information synthesis from multiple knowledge sources is necessary to produce a high-quality generation as the wikipedia knowledge becomes more difficult to identify performance decreases but still outperforms the baseline that uses the dataset-provided set of 34 sentences comparing the scaling capability of kif to the standard generative transformer memnet baseline highlights the advantage of using knn the attention-based mechanism used in dinan et al. 2018 struggles to identify salient information 93 figure : examples of top-3 fetched training utterances and fetched knowledge when responding to a human chat from the dataset using a trained wizard of wikipedia model examples are taken from validation when given increasingly larger quantities of knowledge unlike the knn information fetch we hypothesize the attention mechanism is by softmax-ing over a larger quantity of inputs as it can be difficult to make sharp distinctions 6.4 ablations importance of multiple knowledge sources one benefit of the kif module approach is that several modules can be combined each capturing information from a different source in both settings wizard of wikipedia and engaging imagechat two modules were used to incorporate multiple forms of knowledge—training utterances to capture the capability of a retrieval-based model and knowledge from wikipedia or related chats based on image features we perform here an ablation study to evaluate the impact of using only one source of information as can be seen in table , performance decreases when only one source of information is used see table ) for engaging imagechat this study also underlines the importance of being able to fetch in a multimodal form fashion the general of the kif module—requiring only a feature vector to find nearest neighbors from—allows fetching on multiple modalities such as text and images in table , using the image-based kif to fetch text from related images is important to reach the strongest performance compare training utterances only that uses text-based kif and using both training utterances and related images . using dialog features for knn performance the quality of the knn search is critical to the performance of kif modules as the external knowledge is kept fixed kif must be able to align the dialog context with the knowledge to identify relevant pieces of information in table , we show that matching on more features can improve the quality of the retrieved information using only the encoding of the immediate previous utterance can improve results on wizard of wikipedia by f1 points but this is further improved by also leveraging the encoding of context +1.8 f1 and using the dialog turn number +1 f1 . these features are available in the datasets and 94 knowledge training utterance generation buzz lightyear s name is in honor of astronaut edwin buzz aldrin my favorite character in that book series is hermione granger cool my favorite character in that movie is buzz lightyear mr potato head is based on the real-life mr. potato head toy my favorite character in that book series is hermione granger slinky dog is a toy dachschund with a metal slinky for a body my favorite character in that book series is hermione granger my favorite character in that movie is real-life mr potato head cool my favorite character is the slinky dog slinky dog is a toy dachschund with a metal slinky for a body really like hermione granger the character cool i really like slinky dog slinky dog is a toy dachschund with a metal slinky for a body my favorite character of all time has to be hermione granger love that movie my favorite character has to be slinky dog the dachshund slinky dog is a toy dachschund with a metal slinky for a body agree with you that s my favorite character as well think so too my favorite is slinky table : effect of fetched information on generated utterances the top section provides examples for a fixed training utterance changing the knowledge—the generated text maintains the construction of the training utterance but changes the favorite character to match the knowledge the bottom section provides examples for fixed knowledge but changing the training utterance—the generated text modifies its form to match the training utterance but the favorite character information remains consistent figure : ablations on wizard of wikipedia ( a kif can scale to hundreds of relevant sentences blue while the baseline model the generative transformer memnet gray , scales poorly b gating can remove irrelevant information in the sources case one source of external information is unrelated ( ) performance as varies we leverage them to improve the relatedness of retrieved knowledge multi-hop retrieval with kif work in memory networks weston et al. 2015 sukhbaatar et al. 2015 utilized multi-hop mechanisms such capacity could be useful when multiple sources are necessary or information is incrementally fetched to emulate multi-hop memory mechanisms we use kif to retrieve relevant information for = or = fixed hops as the number of hops is fixed the multi-hop operation remains differentiable we do not allow the model to retrieve the same information in a second hop we experimented in two settings first the same kif module is used multiple times to fetch different information and then all of the fetched knowledge is concatenated results are shown in table ( top . second we examine spreading the fetches into different kif modules at various 95 model test f1 model valid f1 wizard of wikipedia 18.1 training utterances only wiki knowledge only 23.9 training utterances and wiki knowledge 25.9 engaging imagechat training utterances only related images only training utterances and related images 13.9 13.8 14.4 fetches fetches fetches fetches kif-augmented transformer 27.4 one kif module fetches multiple times multiple kif modules fetch once each 26.9 26.0 26.5 25.9 table : using multiple kif modules on multiple sources is important for improved performance table : multi-hop with kif to retrieve information with multiple fetch steps model valid f1 wizard of wikipedia previous utterance only dialog context turn embedding engaging imagechat previous utterance only dialog context turn embedding personality 24.6 26.4 27.4 13.3 14.5 15.1 table : important features for knn search using kif salient conversation features improve performance on both datasets encoder depths this could be interpreted as the model learning to access more information each layer as the model progresses deeper more abstract and high level representations are built which could allow different knowledge to be retrieved results are shown in table ( bottom . in both multi-hop settings no improvement in performance on the wizard of wikipedia dataset is observed we hypothesize that this can be partially attributed to the construction of the dataset—as humans explicitly based their written dialog utterance on one knowledge sentence further it is possible that concatenation brings together too much information for the model to incorporate and thus adding additional fetches makes the retrieval more noisy effect of gating we analyze the effect of the gating mechanism by evaluating the capability of the gate to identify and focus on salient information on wizard of wikipedia we concatenate a third source of information dialog turns from a completely different corpus called personachat zhang et al. 2018 . this dataset looks quite different—short utterances without factual knowledge—and should be easy for the model to identify as distinct from wizard of wikipedia as shown in figure ( ) if kif on personachat is included without gating it has a harmful effect as the model includes irrelevant information when equipped with gating the model learns to use the gate to ignore some inputs and can recover almost the full performance of a model without this irrelevant information source size of in knn figure ( ) shows the performance on wizard of wikipedia when varying the amount of knowledge being able to access multiple relevant pieces of information is helpful but too much information can be harmful this is likely because the weighted sum becomes blurry if too many sentences are incorporated 7 conclusion we present a knn-based information fetching module that learns to identify relevant information from external knowledge sources by learning a mapping-based read operation kif modules benefit from the scalability and efficiency of knn search enabling computation with large external memories we show in the context of two dialog datasets that relevant knowledge can be identified and incorporated to create more engaging high-quality dialog acknowledgments we thank the reviewers and action editor for their comments and insightful discussion we thank emily dinan and kurt shuster for providing assistance to reproduce their original works 96 references antoine bordes y-lan boureau and jason weston 2017. learning end-to-end goaloriented dialog in 5th international conference on learning representations iclr 2017 toulon france april 24-26 2017 conference track proceedings a matching-to-generation deng cai yan wang wei bi zhaopeng tu xiao-jiang liu and shuming shi 2019. retrieval-guided dialogue response generation via framework in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 1866–1875 doi https //doi.org/10.18653/v1/d19 -1195 sarath chandar sungjin ahn hugo larochelle pascal vincent gerald tesauro and yoshua bengio 2016. hierarchical memory networks corr abs/1605.07427 danqi chen adam fisch jason weston and antoine bordes 2017. reading wikipedia to answer open-domain questions in proceedings of the 55th annual meeting of the association for computational linguistics volume : long papers , pages 1870–1879 doi https // doi.org/10.18653/v1/p17-1171 pmcid pmc5579958 wenlin chen david grangier and michael auli 2016. strategies for training large vocabulary neural language models in proceedings of the 54th annual meeting of the association volume : for computational linguistics 1975–1985 doi long papers , : //doi.org/10.18653/v1/p16 -1186 pages emily dinan stephen roller kurt shuster angela fan michael auli and jason weston 2018. wizard of wikipedia knowledgepowered conversational agents in international conference on learning representations angela fan claire gardent chlo´e braud and antoine bordes 2019. using local knowledge graph construction to scale seq2seq models to multi-document inputs in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural emnlp-ijcnlp , language pages 4177–4187 processing angela fan david grangier and michael auli 2018a controllable abstractive summarization in proceedings of the 2nd workshop on neural machine translation and generation pages 45–54 angela fan mike lewis and yann dauphin 2018b hierarchical neural story generation in proceedings of the 56th annual meeting of the association for computational linguistics volume : long papers , pages 889–898 edouard grave armand joulin moustapha ciss´e david grangier and herv´e j´egou 2017a efficient softmax approximation for gpus in proceedings of the 34th international conference on machine learning-volume 70 pages 1302–1310 edouard grave armand joulin and nicolas usunier 2017b improving neural language models with a continuous cache in 5th international conference on learning representations iclr 2017 toulon france april 24-26 2017 conference track proceedings alex graves greg wayne and ivo danihelka 2014. neural turing machines arxiv preprint arxiv:1410.5401. kelvin guu kenton lee zora tung panupong pasupat and ming-wei chang 2020. retrieval augmented language model pre-training in proceedings of the international conference on machine learning pages 5695–5704 samuel humeau kurt shuster marie-anne lachaux and jason weston 2019. polyencoders architectures and pre-training strategies for fast and accurate multi-sentence scoring in international conference on learning representations jeff johnson matthijs douze and herv´e j´egou 2019. billion-scale similarity search with gpus ieee transactions on big data doi https //doi.org/10.1109/tbdata .2019.2921572 armand joulin and tomas mikolov 2015. inferring algorithmic patterns with stackaugmented in advances recurrent nets 97 in neural information processing systems pages 190–198 urvashi khandelwal omer levy dan jurafsky luke zettlemoyer and mike lewis 2019. generalization through memorization nearest neighbor language models in international conference on learning representations diederik p. kingma and jimmy ba adam a method for stochastic optimization in 3rd international conference on learning representations iclr 2015 san diego ca usa may 7-9 2015 conference track proceedings guillaume lample alexandre sablayrolles marc aurelio ranzato ludovic denoyer and herv´e j´egou 2019. large memory layers with product keys in advances in neural information processing systems pages 8548–8559 margaret li jason weston 2019. acute-eval and stephen roller improved dialogue evaluation with optimized questions and multi-turn comparisons arxiv preprint arxiv:1909.03087. rongzhong lian min xie fan wang jinhua peng and hua wu 2019. learning to select knowledge for response generation in dialog systems in proceedings of the 28th international joint conference on artificial intelligence pages 5081–5087 aaai press dhruv mahajan ross girshick vignesh ramanathan kaiming he manohar paluri yixuan li ashwin bharambe and laurens van der maaten 2018. exploring the limits of weakly supervised pretraining in proceedings of on computer vision eccv , pages 181–196 doi https //doi.org/10.1007/978-3-030 -01216-8 12 the european conference bryan mccann james bradbury caiming xiong and richard socher 2017. learned in translation contextualized word vectors in advances in neural information processing systems pages 6294–6305 dialog research software platform pages 79–84 https //arxiv.org/abs/1705.06476 doi https //doi.org/10.18653/v1 /d17-2014 andriy mnih and geoffrey hinton 2009. a scalable hierarchical distributed language model in advances in neural information processing systems pages 1081–1088 fabio petroni tim rockt¨aschel sebastian riedel patrick lewis anton bakhtin yuxiang wu and alexander miller 2019. language models as knowledge bases in the 2019 conference on proceedings of empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 2463–2473 doi https //doi.org/10.18653/v1/d19 -1250 tobias pl¨otz and stefan roth 2018. neural nearest neighbors networks in advances in neural information processing systems pages 1087–1098 lianhui qin michel galley chris brockett xiaodong liu xiang gao bill dolan yejin choi and jianfeng gao 2019. conversing by reading contentful neural conversation with on-demand machine reading in proceedings of the 57th annual meeting of the association for computational linguistics pages 5427–5436 jack rae jonathan j. hunt ivo danihelka timothy harley andrew w. senior gregory wayne alex graves and timothy lillicrap 2016. scaling memory-augmented neural networks with sparse reads and writes in advances in neural information processing systems pages 3621–3629 rico sennrich barry haddow and alexandra birch 2016. neural machine translation of rare words with subword units in proceedings of the 54th annual meeting of the association for computational linguistics volume : 1715–1725 doi long papers , : //doi.org/10.18653/v1/p16 -1162 pages alexander miller will feng dhruv batra antoine bordes adam fisch jiasen lu devi parikh and jason weston 2017. parl.ai a minjoon seo jinhyuk lee tom kwiatkowski ankur parikh ali farhadi and hannaneh hajishirzi open-domain 2019. real-time 98 question answering with dense-sparse phrase index in proceedings of the 57th annual meeting of the association for computational linguistics pages 4430–4441 iulian v. serban ryan lowe laurent charlin and joelle pineau 2016a generative deep neural networks for dialogue a short review arxiv preprint arxiv:1611.06216. iulian v. serban alessandro sordoni yoshua bengio aaron courville and joelle pineau 2016b building end-to-end dialogue systems using generative hierarchical neural network models in thirtieth aaai conference on artificial intelligence kurt shuster samuel humeau antoine bordes and jason weston 2020. image-chat engaging grounded conversations in proceedings of the 58th annual meeting of the association for computational linguistics pages 2414–2429 doi https //doi.org/10.18653/v1 /2020.acl-main.219 kurt shuster samuel humeau hexiang hu antoine bordes and jason weston 2019. engaging image captioning via personality in proceedings of the ieee conference on computer vision and pattern recognition pages 12516–12526 doi https //doi .org/10.1109/cvpr.2019.01280 haoyu song yan wang wei-nan zhang xiaojiang liu and ting liu 2020. generate delete and rewrite a three-stage framework for improving persona consistency of dialogue generation arxiv preprint arxiv:2004.07672. doi https //doi.org/10.18653/v1 /2020.acl-main.516 pmid 32249355 yiping song rui yan xiang li dongyan zhao and ming zhang 2016. two are better than one an ensemble of retrieval-and generation-based dialog systems arxiv preprint arxiv:1610.07149. sainbayar sukhbaatar edouard grave guillaume lample herve jegou and armand joulin 2019. augmenting self-attention with persistent memory https //arxiv.org/abs/1907 .01470 sainbayar sukhbaatar jason weston rob fergus et al 2015. end-to-end memory networks in advances in neural information processing systems pages 2440–2448 bart thomee david a shamma gerald friedland benjamin elizalde karl ni douglas poland damian borth and li-jia li 2016. yfcc100m the new data in multimedia research communications of the acm 59 2 :64–73 doi https //doi.org/10 .1145/2812802 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n. gomez łukasz kaiser and illia polosukhin 2017. attention is all you need in advances in neural information processing systems pages 5998–6008 jason weston sumit chopra and antoine bordes 2015. memory networks in 3rd international conference on learning representations iclr 2015 san diego ca usa may 7-9 2015 conference track proceedings jason weston emily dinan and alexander miller 2018. retrieve and refine improved sequence generation models for dialogue in proceedings of the 2018 emnlp workshop scai the 2nd international workshop ai on pages 87–92 doi https //doi.org/10 .18653/v1/w18-5713 search-oriented conversational saining xie ross girshick piotr doll´ar zhuowen tu and kaiming he 2017. aggregated residual transformations for deep neural networks in proceedings of the ieee conference on computer vision and pattern recognition pages 1492–1500 saizheng zhang emily dinan jack urbanek arthur szlam douwe kiela and jason weston 2018. personalizing dialogue agents i have a dog do you have pets too in proceedings of the 56th annual meeting of the association for computational linguistics volume : 2204–2213 doi long papers , : //doi.org/10.18653/v1/p18 -1205 pages yutao zhu zhicheng dou jian-yun nie and ji-rong wen 2020. reboost a retrievalboosted sequence-to-sequence model for neural response generation information retrieval journal 23 1 :27–48 doi https //doi .org/10.1007/s10791-019-09364-x 99 pair planning and iterative reﬁnement in pre-trained transformers for long text generation xinyu hua lu wang khoury college of computer sciences computer science and engineering northeastern university boston ma hua.x northeastern.edu university of michigan ann arbor mi wangluxy umich.edu abstract pre-trained transformers have enabled impressive breakthroughs in generating long and ﬂuent text yet their outputs are often rambling without coherently arranged content in this work we present a novel content-controlled text generation framework pair with planning and iterative reﬁnement which is built upon a large model bart we ﬁrst adapt the bert model to automatically construct the content plans consisting of keyphrase assignments and their corresponding sentence-level positions the bart model is employed for generation without modifying its structure we then propose a reﬁnement algorithm to gradually enhance the generation quality within the sequence-tosequence framework evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains with an average of 20 bleu points and 12 meteor points improvements in addition human judges rate our system outputs to be more relevant and coherent than comparisons without planning introduction large pre-trained language models are the cornerstone of many state-of-the-art models in various natural language understanding and generation tasks devlin et al. 2019 liu et al. 2019 lewis et al. 2020 , yet they are far from perfect in generation tasks although models like gpt-2 radford et al. 2019 are able to produce plausible text their spontaneous nature limits their utility in actual applications e.g. users can not specify what contents to include and in what order to make large models more useful in practice and to improve their generation quality we believe it is critical to inform them of when to say what which is addressed as content planning in traditional generation systems duboue and mckeown figure : an argument generation example using reddit changemyview [ top partial output by our planner with keyphrase assignment and positions in subscripts for each sentence segmented by special token sen , from which a template is constructed [ bottom a draft is ﬁrst produced and then reﬁned with updated words highlighted in italics 2001 stent et al. 2004 . specially designed control codes and auxiliary planning modules have been integrated into neural models keskar et al. 2019 moryossef et al. 2019 hua and wang 2019 , yet those solutions require model architecture modiﬁcation or retraining making text generation with large models a very costly endeavor to this end this work aims to bring new insights into how to effectively incorporate content plans into large models to generate more rele proceedings of the 2020 conference on empirical methods in natural language processing pages 781–793 november 16–20 2020. ( ) 2020 association for computational linguistics 781 prompt cmv donald trump is a communist content plan output by planning model : 1 a communist3 begin with8 coherent ideology15 [ sen 21 2 [ sen 4 3 no evidence2 any coherent8 held beliefs12 any topic15 [ sen 18 template ( ) __0 __1 __2 a communist __5 __6 __7 begin with __10 : template construction a n t l __11 __12 __13 __14 coherent ideology__17 __18 __19 __20 2 __0 __1 __2 __3 3 __0 __1 no evidence __4 __5 __6 __7 any coherent __10 __11 held beliefs __14 any topic __17 draft initial generation : 1 well call him a communist you must begin with that donald trump has some kind of coherent ideology to begin with ( ) which is unlikely ( ) there is no evidence to suggest donald trump has any coherent or commonly held beliefs on any topic refined final generation : 1 to call him a communist you must begin with that he has some kind of coherent ideology in the first place ( ) he does not ( ) there is no evidence whatsoever that trump has any coherent commonly held beliefs on any topic n i a e e : i o h i t e e i e : i vant and coherent text we ﬁrst study a planning model trained from bert devlin et al. 2019 to produce the initial content plan which assigns keyphrases to different sentences and predicts their positions next we propose a contentcontrolled text generation framework built upon the pre-trained sequence-to-sequence seq2seq transformer model bart lewis et al. 2020 . as shown in figure , our generation model takes in a content plan consisting of keyphrase assignments and their corresponding positions for each sentence the plan is encoded as a template with mask tokens added at positions where no content is speciﬁed our model then outputs a ﬂuent and coherent multi-sentence text draft to reﬂect the plan this is done by ﬁne-tuning bart without modifying its architecture furthermore we present an iterative reﬁnement algorithm to improve the generation in multiple passes within the seq2seq framework at each iteration tokens with low generation conﬁdence are replaced with mask to compose a new template from which a new output is produced unlike prior reﬁnement algorithms that only permit editing in place our solution offers more ﬂexibility figure exempliﬁes the reﬁnement outcome we call our system pair planning and iterative reﬁnement .1 it is experimented on three distinct domains counter-argument generation with reddit changemyview data opinion article writing with the new york times nyt corpus2 sandhaus 2008 , and news report production on nyt automatic evaluation with bleu rouge and meteor shows that by informing the generation model with sentence-level content plans our model signiﬁcantly outperforms a bart model ﬁne-tuned with the same set of keyphrases as input § 5.1 . human judges also rate our system outputs as more relevant and coherent § 5.2 . additionally our iterative reﬁnement strategy consistently improves the generation quality according to both automatic scores and human evaluation finally our model achieves better content control by reﬂecting the speciﬁed keyphrases in the content plan whose outputs are preferred by human to another version with weaker control to summarize our major contributions include • we propose a novel content planner built upon 1code and data are available at http //xinyuhua github.io/resources/emnlp20/ : //catalog.ldc.upenn.edu/ ldc2008t19 bert to facilitate long-form text generation • we present a novel template mask-and-ﬁll method to incorporate content planning into generation models based on bart • we devise an iterative reﬁnement algorithm that works within the seq2seq framework to ﬂexibly improve the generation quality 2 related work content planning as a generation component despite the impressive progress made in many generation tasks neural systems are known to produce low-quality content wiseman et al. 2017 rohrbach et al. 2018 , often with low relevance li et al. 2016 and poor discourse structure zhao et al. 2017 xu et al. 2020 . consequently planning modules are designed and added into neural systems to enhance content relevance wiseman et al. 2018 moryossef et al. 2019 yao et al. 2019 hua and wang 2019 . however it is still an open question to include content plans in large models given the additional and expensive model retraining required this work innovates by adding content plans as masked templates and designing reﬁnement strategy to further boost generation performance without architectural change controlled text generation our work is also in line with the study of controllability of neural text generation models this includes manipulating the syntax duˇsek and jurˇc´ıˇcek 2016 goyal and durrett 2020 and semantics wen et al. 2015 chen et al. 2019 of the output speciﬁc applications encourage the model to cover a given topic wang et al. 2017 see et al. 2019 , mention speciﬁed entities fan et al. 2018 , or display a certain attribute hu et al. 2017 luo et al. 2019 balakrishnan et al. 2019 . however most existing work relies on model engineering limiting the generalizability to new domains and adaptability to large pre-trained transformers one exception is the plug and play model dathathri et al. 2020 , which directly modiﬁes the key and value states of gpt2 radford et al. 2019 . however since the signal is derived from the whole generated text it is too coarse to provide precise sentence-level content control here we instead gain ﬁne-grained controllability through keyphrase assignment and positioning per sentence which can be adapted to any off-the-shelf pre-trained transformer generators iterative reﬁnement has been studied in machine translation lee et al. 2018 freitag et al. 2019 782 mansimov et al. 2019 kasai et al. 2020 to gradually improve translation quality reﬁnement is also used with masked language models to improve ﬂuency of non-autoregressive generation outputs ghazvininejad et al. 2019 lawrence et al. 2019 . our work uses bart lewis et al. 2020 , a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions novak et al. 2016 xia et al. 2017 weston et al. 2018 , as we leverage the seq2seq architecture to offer more ﬂexible edits 3 content-controlled text generation with pair task description our input consists of 1 a sentence-level prompt , such as a news headline or a proposition in an argument and 2 a set of keyphrases that are relevant to the prompt the system aims to generate that contains multiple sentences as in a news report or an argument by reﬂecting the keyphrases in a coherent way in this section we ﬁrst introduce content planning built upon bert that assigns keyphrases into sentences and predicts their positions § 3.1 . then we propose a seq2seq generation framework with bart ﬁne-tuning that includes a given content plan derived from keyphrases ( 3.2 . finally § 3.3 discusses improving generation quality by iteratively masking the less conﬁdent predictions and regenerating within our framework 3.1 content planning with bert our content planner is trained from bert to assign keyphrases to different sentences and predict their corresponding positions as shown in figure , the concatenation of prompt and unordered keyphrases is encoded with bidirectional selfattentions keyphrase assignments are produced autoregressively as a sequence of tokens ( ) { wj , with their positions in the sentence = sj predicted as a sequence tagging task we choose bert because it has been shown to be effective at both language modeling and sequence tagging moreover we leverage its segment embedding to distinguish the input and output sequences speciﬁcally we reuse its pre-trained language model output layer for keyphrase assignment we further design a separate keyphrase positioning layer to predict token position sj as the relative figure : content planning with bert we use bidirectional self-attentions for input encoding and apply causal self-attentions for keyphrase assignment and position prediction the input x m and output keyphrase assignments m cid:48 ) are distinguished by different segment embeddings distance from each sentence s beginning p sj|w≤j = softmax h lws ( ) where l is the last layer hidden states of the transformer and ws are the newly added keyphrase positioning parameters learned during bert ﬁne-tuning the range of allowed positions is from to 127. noticeably as our prediction is done autoregressively attentions should only consider the generated tokens but not the future tokens however bert relies on bidirectional self-attentions to attend to both left and right to resolve this discrepancy we apply causal attention masks dong et al. 2019 over ( ) to disallow attending to the future gray arrows in figure ) training the planner we extract keyphrases and acquire their ground-truth positions from humanwritten references and ﬁne-tune bert with crossentropy losses for both assignment and positioning with a scaling factor 0.1 over the positioning loss inference a bok token signals the beginning of keyphrase assignment generation we employ a greedy decoding algorithm and limit the output vocabulary to tokens in and ensure each keyphrase is generated at most once to allow sentence-level content planning a special sen token is generated to represent the sentence boundary with its predicted position indicating the length the planning process terminates when eos is produced 783 segment type 1 segment type 2 bidirectional self-attention causal attention language model output layer position prediction layer kp-1 kp-2 w1 w3 w4 w2 s1 s2 s3 s4 bert prompt [ sep keyphrase set [ bok w1 w2 w3 figure : our content-controlled text generation framework pair which is built on bart decoding is executed iteratively at each iteration the encoder consumes the input prompt , the keyphrase assignments ( ) as well as a partially masked template t r−1 for the r-th iteration [ ] for masks . the autoregressive decoder produces a complete sequence ( ) a subset of which is further masked to serve as the next iteration s template ( ) 3.2 adding content plan with a template mask-and-fill procedure given a content planning model we invoke it to output keyphrase assignments to different sentences m cid:48 ) their corresponding positions , along with each sentence s length based on the prediction of sen ) we ﬁrst employ a post-processing step to convert between different tokenizers and correct erroneous position predictions that violate the assignment ordering or break the consecutivity of the phrase appendix a . we then convert the plan into a template ( ) as follows for each sentence the assigned keyphrases are placed at their predicted positions and empty slots are ﬁlled with mask symbols figure illustrates the template construction process and our seq2seq generation model in appendix , we show statistics on the constructed templates the input prompt , keyphrase assignments ( ) and template ( ) are concatenated as the input to the encoder the decoder then generates an output ( ) according to the model s estimation of ( ( ) |x m cid:48 , ( ) . ( ) is treated as a draft to be further reﬁned as described in the next section our method is substantially different from prior work that uses constrained decoding to enforce words to appear at speciﬁc positions hokamp and liu 2017 post and vilar 2018 hu et al. 2019 , which is highly biased by the surrounding few words and suffers from disﬂuency since bart is trained to denoise the masked input with contextual understanding it naturally beneﬁts our method decoding we employ the nucleus sampling strategy holtzman et al. 2019 , which is shown to yield superior output quality in long text generation in addition to the standard top-k sampling from tokens with the highest probabilities nucleus sam pling further limits possible choices based on a cumulative probability threshold set to 0.9 in all experiments below . we also require the keyphrases to be generated at or nearby their predicted positions concretely for positions that match any keyphrase token we force the decoder to copy the keyphrase unless it has already been generated in the previous ﬁve tokens we sample three times to choose the one with the lowest perplexity as estimated by gpt-2base radford et al. 2019 . iterative reﬁnement 3.3 outputs generated in a single pass may suffer from incorrectness and incoherence see figure ) therefore we propose an iterative reﬁnement procedure to improve the quality in each pass tokens with low generation conﬁdence are masked algorithm ) this is inspired by iterative decoding designed for inference acceleration in non-autoregressive generation lee et al. 2018 lawrence et al. 2019 , though their reﬁnement mostly focuses on word substitution and lacks the ﬂexibility for other operations moreover our goal is to improve ﬂuency while ensuring the generation of given keyphrases at each iteration the least conﬁdent tokens are replaced with mask . similar as the maskpredict algorithm ghazvininejad et al. 2019 , we gradually reduce the number of masks in our experiments each sample is reﬁned for iterations with decaying linearly from 80 of |y r | to 0. training the generator our training scheme is similar to masked language model pre-training given the training corpus = ( xi m cid:48 i yi } we consider two approaches that add noise to the target yi by randomly masking a subset of 1 any tokens or 2 tokens that are not within the span 784 prompt encoder `` $ & ( keyphrase assignment masked template ( ) update mask a communist begin with coherent ideology sen [ ] m [ ] m a communist3 m [ ] m begin with8 m [ ] m [ ] m [ ] coherent ideology15 … well call him a communist3 you must begin with8 donald trump has some kind of coherent ideology15 … initial template construction positions generation with content plan ( ) decoder draft * & $ + algorithm : iteratively reﬁnement via template mask-and-ﬁll the sample with the lowest perplexity thus with better ﬂuency is selected for each iteration data prompt , keyphrase assignments ( ) keyphrase positions , reﬁnement iterations ρ nucleus sampling runs result ﬁnal output ( ) construct template ( ) based on ( ) and ; for = to do run encoder over ⊕ ( ) t r−1 ; ← ; for = to do run nucleus sampling to generate yi with keyphrase position enforcement append yi to ; ( ) argminyi∈y gpt2-ppl yi ; ← |y r | ( − r/r ; mask tokens with the lowest probabilities to create new template ( ) of any keyphrase the latter is better aligned with our decoding objective since keyphrases are never masked we concatenate xi m cid:48 i and the corrupted target cid:101 yi as input and ﬁne-tine bart to recon struct the original yi with a cross-entropy loss 4 experiment setups 4.1 tasks and datasets we evaluate our generation and planning models on datasets from three distinct domains for multiparagraph-level text generation ( ) argument generation arggen ( hua et al. 2019 , to produce a counter-argument to refute a given proposition ( ) writing opinionated articles opinion , e.g. editorials and op-eds to show idea exchange on a given subject and 3 composing news reports news to describe events the three domains are selected with diverse levels of subjectivity and various communicative goals persuading vs. informing , with statistics shown in table 1. task : argument generation we ﬁrst evaluate our models on persuasive argument generation based on a dataset collected from reddit r/changemyview cmv in our prior work hua et al. 2019 . this dataset contains pairs of original post op statement on a contro sample |prompt| |target| kp kp cov arggen 56,504 19.4 opinion 104,610 6.1 news 239,959 7.0 116.6 205.6 282.7 20.6 30.5 19.0 26.0 30.3 32.6 table : statistics of the three datasets we report average lengths of the prompt and the target generation number of unique keyphrases # kp used in the input and the percentage of content words in target covered by the keyphrases kp cov. . versial issue about politics and ﬁltered high-quality counter-arguments covering 14 833 threads from 2013 to 2018. we use the op title which contains a proposition e.g the minimum wage should be abolished , to form the input prompt x. in our prior work only the ﬁrst paragraphs of high-quality counter-arguments are used for generation here we consider generating the full post which is signiﬁcantly longer keyphrases are identiﬁed as noun phrases and verb phrases that contain at least one topic signature word lin and hovy 2000 , which is determined by a log-likelihood ratio test that indicates word salience following our prior work we expand the set of topic signatures with their synonyms hyponyms hypernyms and antonyms according to wordnet miller 1994 . the keyphrases longer than 10 tokens are further discarded task : opinion article generation we collect opinion articles from the new york times nyt corpus sandhaus 2008 . an article is selected if its taxonomies label has a preﬁx of top/opinion we eliminate articles with an empty headline or less than three sentences keyphrases are extracted in a similar manner as done in argument generation samples without any keyphrase are removed the article headline is treated as the input and our target is to construct the full article table shows that opinion samples have shorter input than arguments and the keyphrase set also covers fewer content words in the target outputs requiring the model to generalize well to capture the unseen tokens task : news report generation similarly we collect and process news reports from nyt ﬁltering by taxonomy labels starting with top/news , removing articles that have no content word overlap with the headline and ones with material-types labeled as one of statistics , list , correction , biography , or review. news reports describe events and facts and in this domain we aim to study and emphasize the impor 785 arggen b-4 r-l mtr len 13.80 9.36 97 0.76 seq2seq 6.78 19.43 15.98 97 kpseq2seq pairlight 26.38 47.97 31.64 119 pairlight w/o reﬁne 25.17 46.84 31.31 120 36.09 56.86 33.30 102 pairfull pairfull w/o reﬁne 34.09 55.42 32.74 101 opinion b-4 r-l mtr len 1.42 15.97 10.97 156 11.38 22.75 18.38 164 16.27 33.30 24.32 210 15.45 32.35 24.11 214 23.12 40.53 24.73 167 22.17 39.71 24.65 169 news b-4 r-l mtr len 1.11 15.60 10.10 242 11.61 21.05 18.61 286 28.03 43.39 27.70 272 27.32 43.08 27.35 278 34.37 51.10 29.50 259 33.48 50.27 29.26 260 table : key results on argument generation opinion article writing and news report generation bleu-4 b4 , rouge-l r-l , meteor mtr , and average output lengths are reported for references the lengths are 100 166 and 250 respectively . pairlight using keyphrase assignments only consistently outperforms baselines adding keyphrase positions pairfull further boosts scores improvements by our models over baselines are all signiﬁcant p 0.0001 approximate randomization test . iterative reﬁnement helps on both setups tance of faithfully reﬂecting content plans during generation and reﬁnement data split and preprocessing for argument generation we split the data into 75 , 12.5 , and 12.5 for training validation and test sets to avoid test set contamination the split is conducted on thread level for opinion and news generation we reserve the most recent 5k articles for testing another 5k for validation and the rest 23k for news and 10k for opinion are used for training we apply the bpe tokenization sennrich et al. 2016 for the generation model as bart does and use wordpiece wu et al. 2016 for bert-based planner to ﬁt the data into our gpus we truncate the target size to 140 tokens for argument sizes of 243 and 335 are applied for opinion and news for both training and inference implementation details 4.2 our code is written in pytorch paszke et al. 2019 . for ﬁne-tuning we adopt the standard linear warmup and inverse square root decaying scheme for learning rates with a maximum value of 5× 10−5 adam kingma and ba 2014 is used as the optimizer with a batch size of 10 for reﬁnement and 20 for content planning and a maximum gradient clipped at 1.0. all hyperparameters are tuned on validation set with early stopping used to avoid overﬁtting more details are in appendix a 4.3 baselines and comparisons we consider two baselines both are ﬁne-tuned from bart as in our models ( ) seq2seq directly generates the target from the prompt ( ) kpseq2seq encodes the concatenation of the prompt and the unordered keyphrase set to study if using only sentence-level keyphrase assignments helps we include a model variant pairlight by removing keyphrase position information s from the input of our generator and using an initial template with all mask symbols our model with full plans is denoted as pairfull we ﬁrst report generation results using ground-truth content plans constructed from human-written text and also show the end-to-end results with predicted content plans by our planner 5 results 5.1 automatic evaluation we report scores with bleu papineni et al. 2002 , which is based on n-gram precision up to 4-grams ; rouge-l lin 2004 , measuring recall of the longest common subsequences and meteor lavie and agarwal 2007 , which accounts for paraphrase for our models pairfull and pairlight we evaluate both the ﬁrst draft and the ﬁnal output after reﬁnement table lists the results when ground-truth content plans are applied first our content-controlled generation model with planning consistently outperforms comparisons and other model variants on all datasets with or without iterative reﬁnement among our model variants pairfull that has access to full content plans obtains signiﬁcantly better scores than pairlight that only includes keyphrase assignments but not their positions lengths of pairfull s outputs are also closer to those of human references both imply the beneﬁt of keyphrase positioning table also shows that the iterative reﬁnement strategy can steadily boost performance on both of our setups by inspecting the performance of reﬁnement in different iterations figure ) we observe that both bleu and rouge-l scores gradually increase while perplexity lowers as the 786 fluency coherence relevance arggen kpseq2seq 4.63 4.75 pairlight pairfull 4.46 3.28 3.97∗ 3.76∗ 2.79 3.85∗ 3.79∗ table : human evaluation for argument generation on ﬂuency coherence and relevance with as the best the krippendorff s are 0.28 0.30 and 0.37 respectively our model outputs are signiﬁcantly more coherent and relevant than kpseq2seq ∗ p 0.0001 , with comparable ﬂuency prompt news : arrested in theft of baby jesus figurines pairfull four new jersey teenagers arrested yesterday were accused of stealing more than 25 plastic baby jesus ﬁgurines from a church before they burn in a bonﬁre the police said the police in sayreville n.j. arrested michael payne 18 and t.j. jones 18 of union city nicholas m. hess 18 of matawan and jason l. ’ neill 18 of port new york n.j. and charged them with burglary their vandals removed more than 100 ﬁgurines of jesus from a cemetery outside st. stanislaus kostka church in sayreville the police said detectives said the four had taken their ﬂashlights to the cemetery and jotted down the license plates of several cars in the vicinity . . prompt opinion : drowning in a sea of college debt pairfull once again the appalling problem of increasing debt in ﬁnancing higher education that has taken place is more intolerable particularly because of the arrogance of our colleges and universities the burden of saddling college students with debt in the middle of their teenage years when they were in debt is essential for a good education our educational system is designed to allow kids to develop the skills necessary but it does not create optimal conditions for mature students who know they will not be able . . table : sample outputs in the news and opinion domain keyphrases assigned to different sentences are in boldface and color-coded guideline is in the supplementary material table shows that both of our models are rated with better coherence and relevance than kpseq2seq which uses the same but unordered keyphrases as input interestingly outputs by pairlight are regarded as more ﬂuent and coherent though the difference is not signiﬁcant however discourse analysis in 6 reveals that clauses produced by pairlight are more locally related compared to pairfull which can be perceived as easier to read in addition to the sample argument in figure , table shows pairfull s output in the news and opinion domains more samples by different systems are in the supplementary material effect of reﬁnement and keyphrase enforce figure : results on iterative reﬁnement with ﬁve iterations both bleu and rouge-l scores steadily increase with perplexity lowers in later iterations figure : end-to-end generation results with automatically predicted content plans our models outperform kpseq2seq in both metrics except for bleu-4 on opinion articles where results are comparable reﬁnement progresses this indicates that iterative post-editing improves both content and ﬂuency results with predicted content plans we further report results by using content plans predicted by our bert-based planner figure compares pairfull and pairlight with kpseq2seq our models yield better meteor scores on all three domains that said the improvement from predicted plans is not as pronounced as that from ground-truth plans upon inspection we ﬁnd that our planner often falls short of accurately positioning the given keyphrases leading to degraded generation performance this points to a potential direction for future work where better positioning model should be developed 5.2 human evaluation we hire four proﬁcient english speakers3 to rate three aspects of the generated arguments on a scale of ( worst to ( best : ﬂuency coherence—if the information organization is natural and logical and relevance—if the topic is related to the prompt and whether the stance is correct 50 samples are randomly selected with system outputs by kpseq2seq pairfull and pairlight shown to human judges in random order the evaluation 3they are all us-based college students each of them is paid 15 hourly for the task 787 bleu-4 rouge-l perplexity 35 30 25 20 55 50 45 40 40 30 20 2 4 2 arggen 4 opinion 2 news 4 arggen 12.0 12.4 13.2 opinion 11.4 9.0 10.7 11.2 news 11.6 11.8 12.0 12.5 24.2 23.7 23.8 20 18.4 20.6 19.6 19.7 20 18.6 20.9 20.5 20.5 15 10 u l r e e 6.8 30 20 10 kpseq2seq 16.0 15 10 10 pairlight w/o refine pairfull w/o refine pairfull 10 10 ment we further ask whether human judges prefer the reﬁned text and whether enforcing keyphrases to be generated yields noticeable content improvement in a second study we present the same 50 prompts from the previous evaluation on argument generation and an additional 50 samples for opinion article writing to the same group of human judge for each sample pairfull s outputs with and without reﬁnement are shown in random order judges indicate their preference based on the overall quality the same procedure is conducted to compare with a version where we do not enforce keyphrases to be copied at their predicted positions during decoding table demonstrates that the reﬁned text is preferred in more than of the cases for both domains enforcing keyphrase generation based on their positions is also more favorable than not enforcing such constraint pairfull w/o reﬁne pairfull w/o enforce arggen 52.7 33.3 45.3 40.0 opinion 52.7 30.7 50.0 29.3 table : percentages of samples preferred by human judges before and after reﬁnement left ; with and without enforcing keyphrases to appear at the predicted positions right . ties are omitted what is updated during iterative reﬁnement since reﬁnement yields better text we compare generations before and after the reﬁnement first we ﬁnd that masks are regularly put on functional words and phrases for example stopwords and punctuation along with their bigrams are often swapped out with new words ﬁlled in to improve ﬂuency moreover about 85 of the reﬁnement operations result in new content being generated this includes changing prepositions and paraphrasing e.g. replacing a research fellow with a graduate student. on both news and opinion domains numerical and temporal expressions are often incorrectly substituted suggesting that better fact control needs to be designed to maintain factuality 6 further discussions on discourse prior work s evaluation mainly focuses on ﬂuency and content relevance and largely ignores the discourse structure exposed by the generated text however unnatural discourse and lack of focus are indeed perceived as major problems of longform neural generations as identiﬁed by human ex figure : distributions of rst tree depth pairfull better resembles the patterns in human-written texts perts.4 here we aim to investigate whether contentcontrolled generation with ground-truth content plans resembles human-written text by studying discourse phenomena are pair generations similar to humanwritten text in discourse structure we utilize dplp ji and eisenstein 2014 , an off-theshelf rhetorical structure theory rst discourse parser dplp converts a given text into a binary tree with elementary discourse units edus usually clauses as nucleus and satellite nodes for instance a relation ns-elaboration indicates the second node as a satellite s elaborating on the ﬁrst nucleus n node dplp achieves f1 scores of 81.6 for edu detection and 71.0 for relation prediction on news articles from the annotated rst discourse treebank carlson et al. 2001 . we run this trained model on our data for both human references and model generations first we analyze the depth of rst parse trees which exhibits whether the text is more locally or globally connected for all trees we truncate at a maximum number of edus based on the 90 percentile of edu count for human references distributions of tree depth are displayed in figure 6. as can be seen generations by pairfull show similar patterns to human-written arguments and articles we also ﬁnd that trees by pairlight tend to have a more linear structure highlighting the dominance of local relations between adjacent edus compared with pairfull which uses knowledge of keyphrases positions this implies that content positioning helps with structure at a more global level we further look into the ratios of ns nn sn relations and observe that most model outputs have similar trends as human-written texts except for kpseq2seq which has more sn relations e.g. it produces twice as many sns than others on arguments 4https //www.economist.com/open-future/2019/10/01/ how-to-respond-to-climate-change-if-you-are-an-algorithm 788 t e e r t r 15 10 0 arggen opinion news 15 10 15 10 kpseq2seq pairlight pairfull human figure : discourse markers that are correctly and incorrectly shaded generated by pairfull compared to aligned sentences in human references discourse markers are grouped from left to right into senses of contingency higher marker generation accuracy observed , comparison and expansion y-axis # of generated sentences with the corresponding marker can pair correctly generate discourse markers since discourse markers are crucial for coherence grote and stede 1998 callaway 2003 and have received dedicated research efforts in rulebased systems reed et al. 2018 balakrishnan et al. 2019 , we examine if pairfull can properly generate them for each sample we construct sentence pairs based on content word overlaps between system generation and human reference we manually select a set of unambiguous discourse markers from appendix a of the penn discourse treebank manual prasad et al. 2008 . when a marker is present in the ﬁrst three words in a reference sentence we check if the corresponding system output does the same figure displays the numbers of generated sentences with markers produced as the same in human references correct or not wrong . the markers are grouped into three senses contingency comparison and expansion the charts indicates that pairfull does better at reproducing markers for contingency followed by comparison and expansion manual inspections show that certain missed cases are in fact plausible replacements such as using at the same time for in addition or also for further while in other cases the markers tend to be omitted overall we believe that content control alone is still insufﬁcient to capture discourse relations motivating future work on discourse planning 7 ethics statement the generated output in real world applications 8 conclusion we present a novel content-controlled generation framework that adds content planning to large pretrained transformers without modifying model architecture a bert-based planning model is ﬁrst designed to assign and position keyphrases into different sentences we then investigate an iterative reﬁnement algorithm that works with the sequenceto-sequence models to improve generation quality with ﬂexible editing both automatic evaluation and human judgments show that our model with planning and reﬁnement enhances the relevance and coherence of the generated content acknowledgements this research is supported in part by national science foundation through grant iis-1813341 and nvidia gpu gifts we thank three anonymous reviewers for their constructive suggestions on many aspects of this work references anusha balakrishnan jinfeng rao kartikeya upasani michael white and rajen subba 2019. constrained decoding for neural nlg from compositional representations in task-oriented dialogue in proceedings of the 57th annual meeting of the association for computational linguistics pages 831– 844 florence italy association for computational linguistics we recognize that the proposed system can generate fabricated and inaccurate information due to the systematic biases introduced during model pretraining based on web corpora we urge the users to cautiously examine the ethical implications of charles b. callaway 2003. integrating discourse markers into a pipelined natural language generation architecture in proceedings of the 41st annual meeting of the association for computational linguistics pages 264–271 sapporo japan association for computational linguistics 789 103 102 101 100 argument correct opinion wrong news if c a u egiv n le st til le st g o h h v w r h t e d s pit in fa ct r y t fo e a ple fu rt m r in a ditio in st d a n t nly h a is lynn carlson daniel marcu and mary ellen okurovsky 2001. building a discourse-tagged corpus in the framework of rhetorical structure theory in proceedings of the second sigdial workshop on discourse and dialogue mingda chen qingming tang sam wiseman and kevin gimpel 2019. a multi-task approach for disentangling syntax and semantics in sentence representations in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2453–2464 minneapolis minnesota association for computational linguistics sumanth dathathri andrea madotto janice lan jane hung eric frank piero molino jason yosinski and rosanne liu 2020. plug and play language models a simple approach to controlled text generation in international conference on learning representations jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirectional transformers for language underin proceedings of the 2019 conference standing of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 4171–4186 minneapolis minnesota association for computational linguistics li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao-wuen hon 2019. uniﬁed language model pre-training for natural language understanding and generation in advances in neural information processing systems pages 13042–13054 pablo a. duboue and kathleen r. mckeown 2001. empirically estimating order constraints for content planning in generation in proceedings of the 39th annual meeting of the association for computational linguistics pages 172–179 toulouse france association for computational linguistics ondˇrej duˇsek and filip jurˇc´ıˇcek 2016. sequence-tosequence generation for spoken dialogue via deep in proceedings of the syntax trees and strings 54th annual meeting of the association for computational linguistics volume : short papers , pages 45–51 berlin germany association for computational linguistics wa falcon 2019. pytorch lightning github note https //github com/williamfalcon/pytorch-lightning cited by 3. markus freitag isaac caswell and scott roy 2019. ape at scale and its implications on mt evaluation biases in proceedings of the fourth conference on machine translation volume : research papers , pages 34–44 florence italy association for computational linguistics marjan ghazvininejad omer levy yinhan liu and luke zettlemoyer 2019. mask-predict parallel decoding of conditional masked language models in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 6112– 6121 hong kong china association for computational linguistics tanya goyal and greg durrett 2020. neural syntactic preordering for controlled paraphrase generation in proceedings of the 58th annual meeting of the association for computational linguistics pages 238– 252 online association for computational linguistics brigitte grote and manfred stede 1998. discourse marker choice in sentence planning in natural language generation chris hokamp and qun liu 2017. lexically constrained decoding for sequence generation using grid in proceedings of the 55th annual beam search meeting of the association for computational linguistics volume : long papers , pages 1535–1546 vancouver canada association for computational linguistics ari holtzman jan buys li du maxwell forbes and yejin choi 2019. the curious case of neural text dein international conference on learngeneration ing representations j. edward hu huda khayrallah ryan culkin patrick xia tongfei chen matt post and benjamin van durme 2019. improved lexically constrained decoding for translation and monolingual rewriting in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 839–850 minneapolis minnesota association for computational linguistics zhiting hu zichao yang xiaodan liang ruslan salakhutdinov and eric xing 2017. toward in proceedings controlled generation of text of the 34th international conference on machine learning-volume 70 pages 1587–1596 jmlr org angela fan david grangier and michael auli 2018. controllable abstractive summarization in proceedings of the 2nd workshop on neural machine translation and generation pages 45–54 melbourne australia association for computational linguistics xinyu hua zhe hu and lu wang 2019. argument generation with retrieval planning and realization in proceedings of the 57th annual meeting of the association for computational linguistics pages 2661–2672 florence italy association for computational linguistics 790 xinyu hua and lu wang 2019. sentence-level content planning and style speciﬁcation for neural text generation in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 591–602 hong kong china association for computational linguistics yangfeng ji and jacob eisenstein 2014. representation learning for text-level discourse parsing in proceedings of the 52nd annual meeting of the association for computational linguistics volume : long papers , pages 13–24 baltimore maryland association for computational linguistics jungo kasai james cross marjan ghazvininejad and jiatao gu 2020. non-autoregressive machine translation with disentangled context transformer in proc of icml nitish shirish keskar bryan mccann lav varshney caiming xiong and richard socher 2019. ctrl a conditional transformer language model for controllable generation arxiv preprint arxiv:1909.05858. diederik kingma and jimmy ba 2014. adam a method for stochastic optimization alon lavie and abhaya agarwal 2007. meteor an automatic metric for mt evaluation with high levels in proceedof correlation with human judgments ings of the second workshop on statistical machine translation pages 228–231 prague czech republic association for computational linguistics carolin lawrence bhushan kotnis and mathias niepert 2019. attending to future tokens for bidiin proceedings of rectional sequence generation the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 1–10 hong kong china association for computational linguistics jason lee elman mansimov and kyunghyun cho 2018. deterministic non-autoregressive neural sein proquence modeling by iterative reﬁnement ceedings of the 2018 conference on empirical methods in natural language processing pages 1173– 1182 brussels belgium association for computational linguistics mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer 2020. bart denoising sequence-to-sequence pretraining for natural language generation translation and comprehension in proceedings of the 58th annual meeting of the association for computational linguistics pages 7871–7880 online association for computational linguistics jiwei li michel galley chris brockett jianfeng gao and bill dolan 2016. a diversity-promoting objective function for neural conversation models in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies pages 110–119 san diego california association for computational linguistics chin-yew lin 2004. rouge a package for automatic evaluation of summaries in text summarization branches out pages 74–81 barcelona spain association for computational linguistics chin-yew lin and eduard hovy 2000. the automated acquisition of topic signatures for text summarization in coling 2000 volume : the 18th international conference on computational linguistics yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov 2019. roberta a robustly optimized bert pretraining approach arxiv preprint arxiv:1907.11692. fuli luo damai dai pengcheng yang tianyu liu baobao chang zhifang sui and xu sun 2019. learning to control the ﬁne-grained sentiment for story ending generation in proceedings of the 57th annual meeting of the association for computational linguistics pages 6020–6026 florence italy association for computational linguistics elman mansimov alex wang sean welleck and kyunghyun cho 2019. a generalized framework of sequence generation with application to undirected sequence models arxiv preprint arxiv:1905.12790. george a. miller 1994. wordnet a lexical database for english in human language technology proceedings of a workshop held at plainsboro new jersey march 8-11 1994. amit moryossef yoav goldberg and ido dagan 2019. step-by-step separating planning from realization in neural data-to-text generation in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2267–2277 minneapolis minnesota association for computational linguistics roman novak michael auli and david grangier iterative reﬁnement for machine translation 2016. arxiv preprint arxiv:1610.06602. kishore papineni salim roukos todd ward and weijing zhu 2002. bleu a method for automatic evalin proceedings of uation of machine translation the 40th annual meeting of the association for computational linguistics pages 311–318 philadelphia pennsylvania usa association for computational linguistics adam paszke sam gross francisco massa adam lerer james bradbury gregory chanan trevor killeen zeming lin natalia gimelshein luca antiga et al 2019. pytorch an imperative style 791 in adhigh-performance deep learning library vances in neural information processing systems pages 8024–8035 matt post and david vilar 2018. fast lexically constrained decoding with dynamic beam allocation for in proceedings of the neural machine translation 2018 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long papers , pages 1314–1324 new orleans louisiana association for computational linguistics rashmi prasad nikhil dinesh alan lee eleni miltsakaki livio robaldo aravind joshi and bonnie webber 2008. the penn discourse treebank 2.0. in lrec citeseer alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever 2019. language models are unsupervised multitask learners openai blog 1 8 :9. lena reed shereen oraby and marilyn walker 2018. can neural generators for dialogue learn sentence in proceedplanning and discourse structuring ings of the 11th international conference on natural language generation pages 284–295 tilburg university the netherlands association for computational linguistics anna rohrbach lisa anne hendricks kaylee burns trevor darrell and kate saenko 2018. object proceedings of lucination in image captioning the 2018 conference on empirical methods in natural language processing pages 4035–4045 brussels belgium association for computational linguistics evan sandhaus 2008. the new york times annotated corpus linguistic data consortium philadelphia 6 12 : e26752 abigail see stephen roller douwe kiela and jason weston 2019. what makes a good conversation how controllable attributes affect human judgments in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 1702–1723 minneapolis minnesota association for computational linguistics rico sennrich barry haddow and alexandra birch 2016. neural machine translation of rare words with subword units in proceedings of the 54th annual meeting of the association for computational linguistics volume : long papers , pages 1715– 1725 berlin germany association for computational linguistics the association for computational linguistics acl04 , pages 79–86 barcelona spain di wang nebojsa jojic chris brockett and eric nyberg 2017. steering output style and topic in neural response generation in proceedings of the 2017 conference on empirical methods in natural language processing pages 2140–2150 copenhagen denmark association for computational linguistics tsung-hsien wen milica gaˇsi´c nikola mrkˇsi´c peihao su david vandyke and steve young 2015. semantically conditioned lstm-based natural language generation for spoken dialogue systems in proceedings of the 2015 conference on empirical methods in natural language processing pages 1711–1721 lisbon portugal association for computational linguistics jason weston emily dinan and alexander miller 2018. retrieve and reﬁne improved sequence generation models for dialogue in proceedings of the 2018 emnlp workshop scai the 2nd international workshop on search-oriented conversational ai pages 87–92 brussels belgium association for computational linguistics sam wiseman stuart shieber and alexander rush 2017. in data-to-document generation in proceedings of the 2017 conference on empirical methods in natural language processing pages 2253–2263 copenhagen denmark association for computational linguistics sam wiseman stuart shieber and alexander rush 2018. learning neural templates for text generain proceedings of the 2018 conference on tion empirical methods in natural language processing pages 3174–3187 brussels belgium association for computational linguistics thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pierric cistac tim rault r emi louf morgan funtowicz and jamie brew 2019. huggingface s transformers state-of-the-art natural language processing arxiv abs/1910.03771 yonghui wu mike schuster zhifeng chen quoc le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al 2016. google s neural machine translation system bridging the gap between huarxiv preprint man and machine translation arxiv:1609.08144. yingce xia fei tian lijun wu jianxin lin tao qin nenghai yu and tie-yan liu 2017. deliberation networks sequence generation beyond one-pass decoding in advances in neural information processing systems pages 1784–1794 amanda stent rashmi prasad and marilyn walker 2004. trainable sentence planning for complex information presentations in spoken dialog systems in proceedings of the 42nd annual meeting of jiacheng xu zhe gan yu cheng and jingjing liu 2020. discourse-aware neural extractive text summarization in proceedings of the 58th annual meeting of the association for computational linguistics 792 ref opinion sys ref arggen sys 133.3 130.2 228.5 246.3 424.5 435.5 tokens sentences 8.6 19.2 13.5 kp per sent 2.96 3.77 2.22 2.49 3.40 3.24 kp distance 2.61 2.95 5.70 6.02 3.76 5.08 news ref 11.1 8.2 5.6 sys table : statistics on generated templates by our content planner tokens are measured in units of wordpiece sennrich et al. 2016 . kp distance denotes the average number of tokens between two keyphrases that are in the same sentence both system output sys and human reference ref are reported masked template construction ( ) for a predicted keyphrase its token positions are adjusted to a consecutive segment so that the phrase is kept intact in the template ( ) if the predicted positions are not monotonic to the assignment ordering they will be rearranged for instance if the assignment contains kp1 cid:46 kp2 but position of kp2 is not strictly larger than that of kp1 we instead place kp2 immediately after kp1 in the template ( ) finally since the planner and generator have different subword vocabularies it is necessary to detokenize the predicted keyphrase assignment and re-tokenize with the bpe vocabulary of the generator b template construction statistics we characterize the content planning results in table 6. speciﬁcally we show the statistics on the automatically created templates based on the planner s output as we can see our system predicted templates approach human reference in terms of length per sentence keyphrase count and the average keyphrase spacing sentence segmentation occurs more often in our templates than the reference text likely due to the frequent generation of sen tokens pages 5021–5031 online association for computational linguistics lili yao nanyun peng ralph weischedel kevin knight dongyan zhao and rui yan 2019. planand-write towards better automatic storytelling in proceedings of the aaai conference on artiﬁcial intelligence volume 33 pages 7378–7385 tiancheng zhao ran zhao and maxine eskenazi 2017. learning discourse-level diversity for neural dialog models using conditional variational autoenin proceedings of the 55th annual meetcoders ing of the association for computational linguistics volume : long papers , pages 654–664 vancouver canada association for computational linguistics et al ( 2019 , by wolf a reproducibility computing infrastructure our model is built upon the pytorch transformers-2.6.0 library with falcon pytorch-lightning-0.7.3 2019 for training routines to improve training efﬁciency we adopt mixed-precision ﬂoating point fp16 computation using the o2 option of nvidia apex5 for both training and decoding we utilize the titan rtx gpu card with 24 gb memory model sizes our generation model has the same architecture as bart lewis et al. 2020 with 406m parameters the content planner is built on top of bertbase which has 110m parameters running time training the generation model takes 2.5 hours for argument 5 hours for opinion and 24 hours for news the content planning model converges in 2.5-4 hours for three domains decoding settings at inference time we set = 50 temperature=1.0 and = 0.9 for nucleus sampling the relatively large value is determined based on a pilot study where we ﬁnd that the reﬁnement lacks diversity if is set to small values moreover since the transformer states need to be cached during autoregressive decoding and we perform three complete nucleus sampling runs in each reﬁnement iteration the gpu memory consumption is substantially increased we therefore limit the maximum generation steps to 140 for argument 243 and 335 for opinion and news auto-correction for content plan when the content plan is predicted by the planner the following post-processing steps are employed prior to the : //github.com/nvidia/apex 793 wikitablet a large-scale data-to-text dataset for generating wikipedia article sections mingda chen sam wiseman kevin gimpel toyota technological institute at chicago chicago il 60637 usa mchen swiseman kgimpel @ ttic.edu abstract datasets for data-to-text generation typically focus either on multi-domain single-sentence generation or on single-domain long-form generation in this work we cast generating wikipedia sections as a data-to-text generation task and create a large-scale dataset wikitablet that pairs wikipedia sections with their corresponding tabular data and various metadata wikitablet contains millions of instances covering a broad range of topics as well as a variety of ﬂavors of generation tasks with different levels of ﬂexibility we benchmark several training and decoding strategies on wikitablet our qualitative analysis shows that the best approaches can generate ﬂuent and high quality texts but they struggle with coherence and factuality showing the potential for our dataset to inspire future work on long-form generation.1 introduction data-to-text generation kukich 1983 mckeown 1992 is the task of generating text based on structured data most existing data-to-text datasets focus on single-sentence generation such as wikibio lebret et al. 2016 , logicnlg chen et al. 2020 , and totto parikh et al. 2020 . other datasets are relatively small-scale and focus on long-form text generation such as rotowire wiseman et al. 2017 and mlb puduppully et al. 2019 . in this work we cast generating wikipedia sections as a data-to-text generation task and build a large-scale dataset targeting multi-sentence data-to-text generation with a variety of domains and data sources to this end we create a dataset that we call wikitablet “ wikipedia tables to text ) that pairs wikipedia sections with their corresponding 1code data and pretrained models are available at : //github.com/mingdachen/wikitablet tabular data and various metadata the data resources we consider are relevant either to entire wikipedia articles such as wikipedia infoboxes and wikidata tables or to particular sections data from the latter category is built automatically from either naturally-occurring hyperlinks or from named entity recognizers this data construction approach allows us to collect large quantities of instances while still ensuring the coverage of the information in the table we also perform various types of ﬁltering to ensure dataset quality wikitablet contains millions of instances covering a broad range of topics and a variety of ﬂavors of generation with different levels of ﬂexibility figure shows two examples from wikitablet the ﬁrst instance has more ﬂexibility as it involves generating a ﬁctional character biography in a comic book whereas the second is more similar to standard data-to-text generation tasks where the input tables contain all of the necessary information for generating the text while the open-ended instances in wikitablet are to some extent similar to story generation propp 1968 mcintyre and lapata 2009 fan et al. 2018 , the fact that these instances are still constrained by the input tables enables different evaluation approaches and brings new ( i.e. being coherent and faithful to the input tables at the same time . because of the range of knowledge-backed generation instances in wikitablet models trained on our dataset can be used in assistive writing technologies for a broad range of topics and types of knowledge for example technologies can aid students in essay writing by drawing from multiple kinds of factual sources moreover wikitablet can be used as a pretraining dataset for other relatively small-scale data-to-text datasets e.g. rotowire . a similar idea that uses data-to-text generation to create corpora for pretraining language models has shown promising results agarwal et al. findings of the association for computational linguistics acl-ijcnlp 2021 pages 193–209 august 1–6 2021 ©2021 association for computational linguistics 193 2021 . in experiments we train several baseline models on wikitablet and empirically compare training and decoding strategies we ﬁnd that the best training strategies still rely on enforcing hard constraints to avoid overly repetitive texts human evaluations reveal that 1 humans are unable to differentiate the human written texts from the generations from our neural models ( ) while the annotations show that grammatical errors in the reference texts and the generations may prevent humans from fully understanding the texts the best decoding strategy i.e. beam search with n-gram blocking paulus et al. 2018 ) does not have such a problem and shows the best performance on several aspects ( ) the degree of topical similarity between the generations and the reference texts depends on the open-endedness of the instances our analysis shows that the generations are ﬂuent and generally have high quality but the models sometimes struggle to generate coherent texts for all the involved entities suggesting future research directions for example when the instance has a high degree of ﬂexibility we ﬁnd the models making mistakes about what a particular entity type is capable of we also ﬁnd errors in terms of the factuality of the generated text both in terms of contradictions relative to the tables and commonsense violations 2 related work there have been efforts in creating data-to-text datasets from various resources including sports summaries wiseman et al. 2017 puduppully et al. 2019 , weather forecasts liang et al. 2009 , and commentaries chen and mooney 2008 . most of the recent datasets focus on generating single sentences given tables such as wikibio totto logicnlg and wikitabletext bao et al. 2018 , or other types of data formats such as data triples vougiouklis et al. 2017 gardent et al. 2017 nan et al. 2021 , abstract meaning representations flanigan et al. 2016 , minimal recursion semantics hajdik et al. 2019 , or a set of concepts lin et al. 2020 . other than single sentences there have been efforts in generating groups of sentences describing humans and animals wang et al. 2018 , and generating a post-modiﬁer phrase for a target sentence given a sentence context kang et al. 2019 . in this work our focus is long-form text generation and we are interested in automatically creating a large-scale dataset containing multiple types of data-to-text instances as shown in table , wikitablet differs from these datasets in that it is larger in scale and contains multi-sentence texts more details are in the next section wikipedia has also been used to construct datasets for other text generation tasks such as generating wikipedia movie plots orbach and goldberg 2020 rashkin et al. 2020 and short wikipedia event summaries gholipour et al. 2020 , and summarizing wikipedia documents zopf 2018 liu et al. 2018 or summaries of aspects of interests hayashi et al. 2020 from relevant documents as part of this work involves ﬁnding aligned tables and text it is related to prior work on aligning wikipedia texts to knowledge bases elsahar et al. 2018 logan et al. 2019 . the wikitablet dataset the wikitablet dataset pairs wikipedia sections2 with their corresponding tabular data and various metadata some of this data is relevant to entire wikipedia articles “ article data ) or article structure “ title data ) while some is section-speciﬁc “ section data ) each data table consists of a set of records each of which is a tuple containing an attribute and a value the instances in wikitablet cover a range of ﬂavors of language generation some have more ﬂexibility requiring models to generate coherent stories based on the entities and knowledge given in the tables the ﬁrst instance in figure is such an example the text is from the wikipedia article entitled wolfsbane comics ” and resides within two nested sections the higher-level section fictional character biography and the lower-level section messiah complex . the task is as models need to generate a coherent passage that can connect all the entities in the section data and the story also needs to ﬁt the background knowledge provided in the article data other instances are more similar to standard datato-text generation tasks where the input tables contain all the necessary information for generating 2we deﬁne a wikipedia section to be all text starting after a sub section heading and proceeding until the next sub section heading we include wikipedia sections at various nesting levels for example a top level section may start with a few paragraphs describing general information followed by two subsections with more speciﬁc information in which case the example will be converted into three instances in our dataset 194 figure : two examples from wikitablet only parts of the tables are shown due to space constraints underlined texts are hyperlinks records with the attributes date , per , person , or gpe are from ner the subscripts for section titles indicate the ordering of nesting where smaller numbers are for higher level sections the text the second instance in figure is an example of this sort of task however these tasks are still due to the wide variety of topics contained in wikitablet 3.1 dataset construction we begin by describing the steps we take to construct wikitablet more details are in the supplementary material in general the steps can be split into two parts collecting data tables and ﬁltering out texts when collecting data we consider ﬁve resources wikidata tables infoboxes in wikipedia pages,3 hyperlinks in the passage named entities in the passage obtained from named entity recognition ner , and wikipedia article structure for a given wikipedia article we use the same infobox and wikidata table for all sections these tables can serve as background knowledge for the article for each section in the article we create a second table corresponding to section-speciﬁc data i.e. section data the section data contains records constructed from hyperlinks and entities identiﬁed by a named entity recognizer.4 3wikidata is a consistently-structured knowledge base e.g. has a ﬁxed set of attributes , whereas infoboxes are not consistently-structured and this ﬂexibility sometimes allows the infobox to contain extra information therefore we consider using infoboxes as extra resources 4we use the ner tagger from spacy honnibal and montani 2017 and a bert model devlin et al. 2019 ﬁnetuned we form records for named entities by using the type of the entity as the attribute and the identiﬁed entity as the value we form records for hyperlinks as follows for the attribute for a hyperlink with surface text and hyperlinked article cid:96 , we use the value of the instance of or subclass of tuple in the wikidata table for cid:96 . for example the ﬁrst instance in figure will be turned into a record with attribute superhero and value wolfsbane comics ” if cid:96 does not have a wikidata table or no appropriate tuple we consider the parent categories of cid:96 . for the value of the tuple we use the document title of cid:96 rather than the actual surface text to avoid giving away too much information in the reference text complementary to the article data we create a title table that provides information about the position in which the section is situated which includes the article title and the section titles for the target section as the initial sections in wikipedia articles do not have section titles we use the section title introduction for these we also perform various ﬁltering to ensure the quality of the data records the coverage of the input data and the length of the reference text the ﬁnal dataset contains approximately 1.5 million instances we randomly sample 4533 instances as the development set and 4351 as the test set we on conll03 data tjong kim sang and de meulder 2003 . 195 during the 2007–2008 `` messiah complex '' storyline rahne helps rictor infiltrate the purifiers she fakes being shot by rictor she is also a member of the new x-force during a battle against lady deathstrike and the reavers rahne learns that father craig was in league with the purifiers supposedly divulging enough information about her that the purifiers can claim to `` know her well. she travels with xforce to her former home muir island now the base of the marauders during the climactic battle rahne is injured by riptide but her wounds according to professor , are superficial and she will recover section data value reavers muir island purifiers marvel comics the 2007-2008 riptide comics lady deathstrike father craig attribute person gpe group of fictional characters date film character film character per article data value rahne sinclair superhero x-men marvel universe attribute birth name instance of member of from narrative universe title data document title section title1 section title2 wolfsbane comics fictional character biography `` messiah complex '' journey to the center of the earth also called jules verne 's journey to the center of the earth is a 1959 american science fiction adventure film in color by de luxe distributed by 20th century fox the film produced by charles brackett and directed by henry levin stars james mason pat boone and arlene dahl bernard herrmann wrote the film score and the film 's storyline was adapted by charles brackett from the 1864 novel of the same name by jules verne attribute musical composition person dependence syndrome film genre business section data value 20th century fox jules verne alcoholic adventure film deluxe entertainment services group inc. based on a journey to the center of the earth article data attribute instance of director composer released genre genre starring value film henry levin bernard herrmann 1959 12 16 science fiction film fantasy film james mason pat boone arlene dahl title data document title journey to the center of the earth 1959 film section title introduction also ensure that there are no overlapping wikipedia articles among splits makes wikitablet more and applicable to various scenarios 3.2 dataset characteristics table shows statistics for wikitablet and related datasets while the average length of a wikitablet instance is not longer than some of the existing datasets wikitablet offers more diverse topics than the sports-related datasets rotowire and mlb or the biography-related dataset wikibio compared to the prior work that also uses wikipedia for constructing datasets wikibio logicnlg totto and dart nan et al. 2021 all focus on sentence generation whereas wikitablet requires generating wikipedia article sections which are typically multiple sentences and therefore more . wikitablet is also much larger than all existing datasets to demonstrate the diversity of topics covered in wikitablet we use either the instance of or subclass of relation from wikidata as the category of the article.5 we show the top 10 most frequent document categories in table 2. due to the criteria we use for ﬁltering only 1.05 of articles in wikitablet do not have these relations or wikidata entries and we omit these articles in the table as the table demonstrates more than 50 of the articles in wikitablet are not about people i.e. the topic of wikibio , within which the most frequent category covers only 4.61 . 3.3 dataset in this subsection we highlight two of wikitablet 1. in contrast to work on evaluating commonsense knowledge in generation where reference texts are single sentences describing everyday scenes lin et al. 2020 , wikitablet can serve as a testbed for evaluating models abilities to use world knowledge for generating coherent longform text 2. compared to other long-form data-to-text datasets such as rotowire where the input tables are box scores the input tables in wikitablet are more diverse including both numbers e.g. economy and population data of an area throughout years , and short phrases this 5when there are multiple values in these two relations we pick the one that has the smallest number of words as it often is the most generic phrase suitable for representing the topic 4 methods in this section we describe details of models that we will benchmark on wikitablet our base model is based on the transformer vaswani et al. 2017 . to encode tables we linearize the tables by using special tokens to separate cells and using feature embeddings to represent records in tables for the title table in the ﬁrst instance in figure the linearized table will be cid:104 boc cid:105 1doc.1 title1 cid:104 bov cid:105 1 wolfsbane1 comics 1 cid:104 boc cid:105 2sec.2 title2 cid:104 bov cid:105 2 fictional2 character2 biography2 cid:104 boc cid:105 3 ··· cid:104 eoc cid:105 ( ) as shown in eq 1 we employ several techniques when encoding tables ( ) we use special tokens cid:104 boc cid:105 and cid:104 bov cid:105 to separate attributes and values and cid:104 eoc cid:105 to indicate the end of a sequence ( ) we use subscript indices to indicate unique id embeddings that are added to the embeddings for each record which helps models align attributes with values and 3 we restart the positional embeddings at each cid:104 boc cid:105 , such that models will not use the ordering of the input records in addition we add a special embedding to each record to indicate if it is from the section table or the article/title table in wikidata there could be multiple qualiﬁers attached to a record in which case we replicate the record for each qualiﬁer separately similar linearization approaches have been used in prior work dhingra et al. 2019 hwang et al. 2019 herzig et al. 2020 yin et al. 2020 . with linearized tables training and inference become similar to other sequence-to-sequence settings we train our models with teacher-forcing and standard cross entropy loss unless otherwise speciﬁed 4.1 training strategies we experiment with three types of modiﬁcations to standard sequence-to-sequence training α-entmax α-entmax peters et al. 2019 is a mapping from scores to a distribution that permits varying the level of sparsity in the distribution this mapping function has been used in machine translation peters et al. 2019 and text generation martins et al. 2020 . when using α-entmax in the decoder we also replace the cross entropy loss with the α-entmax loss peters et al. 2019 . both 196 examples avg len record types avg records wikitabletext wikibio rotowire mlb logicnlg totto dart wikitablet vocab tokens 185.0k 19.0m 400.0k 1.6m 11.3k 14.3m 38.9k 52.7k 122.0k 1.3m 136.0k 33.2k 717.1k 1.9m 169.0m 13.3k 728.0k 4.9k 26.3k 37.0k 136.0k 82.2k 1.5m∗ 13.9 26.1 337.1 542.1 14.2 17.4 21.6 115.9 3.0k 1.7k 39.0 53.0 11.7k 41.8k 147.4k† 4.1 19.7 628.0 565.0 13.5 32.7 51.9 domain wikipedia biography sports sports wikipedia wikipedia wikipedia wikipedia+restaurant table : statistics for several data-to-text datasets wikitablet combines a large number of examples moderate generation length typically more than one sentence , and a large variety of record types we omit record types and avg records for dart as its input units are triple sets instead of table records ∗887.7k unique wikipedia articles †number of record types for each resource 31.8k infobox , 1.7k wikidata , 115.6k hyperlinks , 17 ner . category human ﬁlm single music human settlement album sports season television series village taxon fraction % 45.62 4.61 1.74 1.53 1.41 1.26 1.17 1.12 0.89 table : top 10 most frequent article categories and their corresponding proportions in wikitablet α-entmax and the α-entmax loss have a hyperparameter α. we follow martins et al ( 2020 and use = 1.2 as they found it to be the best value for reducing repetition in generation copy mechanism similar to prior work on datato-text generation wiseman et al. 2017 puduppully et al. 2019 , we use pointer-generator network style copy attention see et al. 2017 in the decoder cyclic loss cyclic losses have been shown to be effective in textual style transfer shetty et al. 2018 pang and gimpel 2019 and neural machine translation cheng et al. 2016 he et al. 2016 tu et al. 2017 . wiseman et al ( 2017 also used this for data-to-text and found it helpful for generating long sequences in this work we experiment with adding the cyclic loss to our transformer models where the backward model can be seen as an information extraction system we expect that adding the cyclic loss should enable a data-to-text model to generate sentences that are more faithful to the conditioned tables the cyclic loss is used during training only and does not affect the models during inference more details are in the appendix 4.2 decoding strategies massarelli et al ( 2020 showed that the choice of decoding strategy can affect the faithfulness or repetitiveness of text generated by language models we are also interested in these effects in the context of data-to-text generation and therefore benchmark several decoding strategies on wikitablet our models use byte-pair encoding bpe sennrich et al. 2016 and for all of the following strategies we always set the minimum number of decoding steps to 100 as it improves most of the evaluation metrics and the maximum number of decoding steps to 300. speciﬁcally we benchmark 1 greedy decoding ( ) nucleus sampling holtzman et al. 2020 with threshold 0.9 as suggested by holtzman et al ( 2020 ; 3 beam search and 4 beam search with n-gram blocking paulus et al. 2018 where we set the probabilities of repeated trigrams to be during beam search we set the beam size to be by default the appendix has more details about the decoding strategies 5 experiments 5.1 setup we experiment with two sizes of transformer models one is base , where we use a 1-layer encoder and a 6-layer decoder each of which has 512 hidden size and attention heads the other one is large , where we use a 1-layer encoder and a 12layer decoder each of which has 1024 hidden size and attention heads models similar to the base conﬁguration have shown strong performance on rotowire gong et al. 2019 .6 due to limited 6when training the base model with entmax on wikibio it achieves bleu-4 45.75 and rouge-4 39.39 on the test set using greedy decoding which are comparable to the current state-of-the-art results of liu et al ( 2018 . 197 references linearized article tables linearized section tables linearized tables linearized tables references base base entmax base copy base copy cyclic loss rl 100.0 14.7 27.9 22.0 61.3 rep bleu 1.2 100.0 2.2 8.0 1.9 1.0 6.4 7.9 7.6 36.5 par-p 100.0 100.0 100.0 100.0 99.9 base models trained on the 500k training set beam search 66.3 64.6 67.1 67.5 met 100.0 9.3 15.5 18.3 56.5 33.0 25.9 30.1 28.0 36.9 36.2 37.5 37.5 15.6 15.4 15.9 15.7 20.3 20.3 20.7 20.8 large models trained on the full training set different decoding strategies large greedy large nucleus sampling large beam search large beam search n-gram blocking 26.8 2.3 18.8 1.9 18.9 18.3 19.5 19.3 38.5 36.1 39.9 39.3 23.5 23.7 23.9 24.4 60.4 54.2 65.8 62.2 28.8 29.0 29.4 29.7 33.1 32.5 34.3 35.3 par-r par-f1 59.2 16.3 20.9 48.3 100.0 72.9 25.6 33.4 63.0 100.0 37.7 37.7 38.5 38.9 40.4 38.7 42.8 43.0 table : test set results for our models when training the large models we use the copy cyclic loss setting as it gives the best performance for the base models for most of the metrics computational power we parameterize our backward model as a transformer model with a 2-layer encoder and a 2-layer decoder.7 we use bpe with 30k merging operations we randomly sample 500k instances from the training set and train base models on them when exploring different training strategies we train a large model with the best setting using the copy mechanism and cyclic loss on the full training set we train both models for epochs during training we perform early stopping on the development set using greedy decoding we report bleu papineni et al. 2002 , rouge-l rl ( lin 2004 , meteor met ( banerjee and lavie 2005 , and parent dhingra et al. 2019 , including precision par-p , recall par-r , and f1 par-f1 scores the ﬁrst three metrics consider the similarities between generated texts and references whereas parent also considers the similarity between the generation and the table when using parent we use all three tables i.e. the section article and title tables as we are also interested in the repetitiveness of generated texts we deﬁne a metric based on ngram repetitions which we call rep . rep computes the ratio of the number of repeated n-grams to the total number of n-grams within a text so when rep has higher value it indicates that the text has more repetitions here we consider ngrams that appear or more times as repetitions and the n-grams we consider are from bigrams to 4-grams when reporting rep scores for a dataset we average the rep scores for each instance in the 7we did not experiment with pretrained models because they typically use the entirety of wikipedia which would presumably overlap with our test set dataset similar metrics have been used in prior work holtzman et al. 2020 welleck et al. 2020 . 5.2 results in table , we report the test results for both our base models and large models we also report a set of baselines that are based on simply returning the linearized tables and their concatenations with the references the linearized table baselines show how much information is already contained in the table while the reference baselines show the upper bound performance for each metric in comparing training strategies we ﬁnd that using α-entmax improves rep signiﬁcantly but not other metrics adding the cyclic loss or the copy mechanism helps improve performance for the par scores and rep and combining both further improves these metrics when comparing decoding strategies we ﬁnd that both nucleus sampling and n-gram blocking are effective in reducing repetition nucleus sampling harms the par scores especially par-p but has less impact on the other metrics indicating that it makes the model more likely to generate texts that are less relevant to the tables using beam search improves all metrics signiﬁcantly when compared to greedy decoding especially the par-p and rep scores adding n-gram blocking further reduces the rep score pushing it to be even lower than that from nucleus sampling but still retains the improvements in par scores from beam search the best overall decoding strategy appears to be beam search with n-gram blocking 198 reference beam search nucleus sampling n-gram blocking grammar coherence faithfulness 4.0 1.0 4.0 1.0 4.0 0.8 4.2 0.9 4.1 0.9 4.0 1.0 4.1 0.9 4.2 0.9 3.8 0.8 3.9 1.0 3.9 0.8 3.9 1.0 table : average human ratings standard deviations in parentheses for grammaticality coherence and faithfulness to the input article table beam search nucleus sampling n-gram blocking relevance 3.8 1.1 3.7 1.2 3.9 1.0 support 3.6 1.2 3.8 1.1 3.8 1.0 table : average human ratings standard deviations in parentheses of relevance and support when comparing to the reference text 6 analysis we now describe a manual evaluation and analyze some generated examples all results in this section use the development set we also conduct experiments on analyzing the effect of using the section data and the article data during training ﬁnding that the beneﬁts that they bring to the model performance are complementary see the appendix for more details 6.1 human evaluation we conduct a human evaluation using generations from the large model on the development set we choose texts shorter than 100 tokens and that cover particular topics as we found during pilot studies that annotators struggled with texts that were very long or about unfamiliar topics.8 we design two sets of questions the ﬁrst focuses on the text itself i.e. grammaticality and coherence and its faithfulness to the input article table since this set does not involve the reference we can ask these questions about both generated texts and the reference texts themselves the second set of questions evaluates the differences between the generations and the reference texts i.e. relevance and support , allowing us to see if the generated text matches the human written section text speciﬁcally relevance evaluates topical similarity between generations and references and support evaluates whether the facts expressed in the generations are supported by or contradictory to those in the references the full questions and numerical answer descriptions are in the appendix 8we did not ﬁnd the ﬁltering to change the observed trends for the automatic metrics and provide the list of selected topics in the appendix we report results in tables and 5. the scores are on a 1-5 scale with being the best for the ﬁrst set we collect 480 annotations from 38 annotators for the second set we collect 360 annotations from 28 annotators we also ensure that each system has the same number of annotations.9 it is interesting to note from table that human annotators are unable to differentiate the human written texts from the generations from our neural models since the wikipedia section texts are parts of wikipedia articles showing the section texts in isolation can make them difﬁcult to understand potentially resulting in noisy annotations as shown by the ﬁrst instance in table , the text uses the pronoun he without clarifying what the pronoun refers to the paragraph is rated for coherence presumably due to this ambiguity also wikipedia texts are sometimes grammatically complex and annotators can mistake them for being ungrammatical e.g. the second instance in table 6. on the other hand the coherence errors in the generated texts are not always easy to spot see for example the last two instances in table , where the incoherence lies in the facts that 1 it is impossible to marry a person before the person is born and 2 senior year takes place after junior year these details are embedded in long contexts which may be overlooked by annotators and lead to results favorable to these neural models to study the relationship between coherence and grammaticality we compute spearman s correlations between the human annotations for coherence and grammaticality after removing the ones with perfect scores for coherence table shows the results the correlations are much higher for references beam search and nucleus sampling than for n-gram blocking this trend suggests that the imperfect coherence scores for the reference texts are likely because annotators ﬁnd the texts to contain grammatical errors or to possess grammatical complexity which may prevent them from fully understanding the texts however n-gram blocking does not have this problem and thus achieves the best results for both coherence and grammaticality we hypothesize that n-gram blocking is able to avoid the types of grammatical errors that 9we used amazon mechanical turk to ensure annotation quality we only recruited annotators with master qualiﬁcation we collected one annotation for each instance so that we can cover more instances and paid 30 cents per annotation the amount of wage per annotation is by 1 the amount of time each annotator spent on the task during our pilot study and 2 a target hourly wage of approximately 11 199 method reference he contested the parliamentary seat of meriden at the 1987 general election where he was defeated by the sitting conservative mp iain mills by a margin of 16,820. he was then selected to ﬁght the conservative-held marginal seat of birmingham northﬁeld ... text reference boscawen married on 23 april 1700 in henry vii s chapel westminster abbey charlotte godfrey elder daughter and coheir of colonel charles godfrey master of the jewel ofﬁce and his wife arabella churchill ... 7th marquess of exeter married ﬁrstly edith csanady de telegd born september 1935 in england died 16 june 1956 in london , on 17 january 1934 ... ... he averaged 10.9 rebounds and 3.0 assists per game as a senior in 1987-88. he was selected to the sweet 16 of the ncaa tournament as a junior in 1988-89 ... sampling blocking c 3 4 4 5 table : human annotation examples for grammaticality g and coherence c . due to space constraints only parts of the texts are shown we highlight texts that are incoherent spearman corr # annotations ref beam samp block 39.6 16.4 67 67 40.8 76 39.7 80 table : spearman correlations between the human evaluation results for grammaticality and coherence we omit annotations with perfect scores for coherence relevance annotations support annotations 24.2 10 17.0 13 19.2 48 11.0 47 13.6 65 17.5 68 12.0 124 12.5 135 8.9 113 9.4 97 table : averaged perplexities and the corresponding numbers of annotations for each option for the relevance and support questions 5 is the best option . we aggregate annotations for different decoding algorithms we note that the perplexities are computed based on the reference texts using the large model prevent understanding because 1 unlike nucleus sampling n-gram blocking does not rely on randomness to avoid repetition ( ) n-gram blocking does not suffer from repetitions like beam search we report results for the second set of questions in table 5. the three evaluated systems show similar performance to investigate the relationship between the degree of open-endedness of a wikitablet instance and its corresponding evaluation scores we compute the averaged perplexities based on our large models for each option in table 8. the most relevant generations are typically from more closed-ended or constrained instances.10 similarly for the support scores more open-ended instances are distributed at score , which means that there is no fact supported by or contradictory to the shown tables while the open-endedness of an instance usually depends on its topics e.g. movie plots are open-ended , there are many cases where the models can beneﬁt from better entity modeling 10li and hovy 2015 use entropy as a proxy to quantify complexity of tasks in this work we use perplexity to measure how open-ended the instances are percentile 10 20 30 40 50 60 70 80 90 100 train perp 2.3 3.1 4.0 4.9 6.1 7.6 9.8 13.3 20.6 278.8 dev perp 2.5 3.6 4.7 6.1 7.6 9.8 12.9 18.2 29.3 129.4 table : percentiles of perplexities for training and development splits of our dataset we use the large model such as understanding what a particular entity type is capable of e.g. see the last example in sec 6.3 . recent work has also found conducting human evaluation for long-form generation to be , for example in the context of question answering krishna et al. 2021 and story generation akoury et al. 2020 . our observations for data-to-text generation complement theirs and we hope that our dataset can inspire future research on human evaluation for long-form text generation 6.2 distribution of perplexity to determine the fraction of wikitablet that can be seen as constrained we report the percentiles of perplexities for training and development splits in table 9. from table , it can be observed that instances with perplexities around 9.0 generally lead to model generations that are closely relevant to the reference texts and mostly supported by the input tables and therefore are likely to be the constrained instances from table , we see that at least of our dataset has perplexities lower than 9.0 so we conjecture that of our dataset consists of constrained instances 6.3 generation examples table 10 shows generation examples for nucleus sampling and beam search with n-gram blocking 200 method nucleus sampling threshold 0.9 n-gram blocking nucleus sampling threshold 0.9 n-gram blocking generated text in a two-issue mini-series titled messiah complex , portrayed by maisie williams father craig is ordered to rescue the world from reavers and run out of the x-force causing the x-force to permanently withdraw the original humans were replaced by slots and muir island being pulled away they also possessed the entire x-force after issue 9 in which the x-force appeared claremont and sinclair later reprised their parts in the donderful world of wolverine with wolfsbane as one of the new x-men and her group taken back to their reservation for unknown reasons in the messiah complex storyline wolfsbane is a member of the x-men a group of mutants recruited by father craig to ﬁght the reavers in muir island father craig is killed in the battle with reavers but father craig manages to rescue the mutant from the rain of gesham and retrieve the mutant she is later revealed to have been killed by rain in a battle with father craig in the aftermath father craig tells father craig that she is the only mutant to be killed by a mutant in may 2013 mccoy allegedly assault a spoiler business ofﬁcial the motivated him to take a business trip to harrisburg pennsylvania and sign a letter from the buffalo bills paying him over 50,000 as well as verbally prohibiting legal kickoff work in september 2014 mccoy shared the issue with some former players of the ﬁrst two ﬁlm franchises mccoy played a leading role in the 2012 marvel cinematic universe ﬁlm erab of the press , and was named one of the ﬁlm s biggest stars on twitter on september , 2014 mccoy ﬁled a lawsuit against pyt to change its twitter name to lesean mccoy productions on september , 2014 mccoy was arrested in harrisburg pennsylvania on suspicion of assault he was charged with assault and battery in may 2013 he was ﬁned over 50,000 by the buffalo bills in september 2014 he was suspended for two games by the pyt for violating the marvel cinematic universe he was released by the bills in october of the same year he was cleared of all charges on twitter and was banned from playing in the 2014 pro bowl due to his twitter account table 10 generation examples from the large model the ﬁrst example corresponds to the ﬁrst instance in figure 1. the complete set of generations is in the appendix we observe very different trends between the two instances in figure 1. for the ﬁrst instance about the x-men although both generations look ﬂuent their stories differ dramatically the generated text for nucleus sampling describes a story that starts by saying father craig rescues the world from reavers and ends with wolfsbane joining as one of the new x-men on the other hand n-gram blocking generates a story where wolfsbane already is a member of x-men and the story says father craig fought and was killed by the reavers but manages to rescue the mutant for the less open-ended instances e.g. the second instance in figure ) different decoding strategies mostly generate similar details see the appendix for generations . despite having different details these generations appear to try to ﬁt in as many entities from the tables as possible in contrast to beam search shown in the appendix which mostly degenerates into repetition for more open-ended instances this explains our previous observation that n-gram blocking helps with the par-r score even though the generations are of good quality for most instances their implausibility becomes more apparent when readers have enough background knowledge to understand the involved entities for example the second instance in table 10 comes from the wikipedia page lesean mccoy ( a football player under the sections personal life and controversies ( details in the appendix . the generation from nucleus sampling is implausi ble/nonsensical in some places “ assault a spoiler business ofﬁcial ) and factually incorrect elsewhere mccoy did not play a leading role in any ﬁlm and erab of the press is not an actual ﬁlm . the fourth generation is implausible because a player is unlikely to be suspended for violating the marvel cinematic universe , and it is unlikely for a person to be cleared of all charges on twitter our models have limited access to knowledge about entities e.g. the capabilities of a social media company like twitter future research may incorporate extra resources make use of pretrained models or incorporate factuality modules to solve these problems 7 conclusion we created wikitablet a dataset that contains wikipedia article sections and their corresponding tabular data and various metadata wikitablet contains millions of instances covering a broad range of topics and kinds of generation tasks our manual evaluation showed that humans are unable to differentiate the references and model generations and n-gram blocking performs the best on grammaticality and coherence however qualitative analysis showed that our models sometimes struggle with coherence and factuality suggesting several directions for future work acknowledgments this work was supported in part by a google fellowship to m. chen 201 impact statement we highlight a few limitations as follows ( ) wikipedia texts are generally written in objective tones but some of the texts may contain controversial content that even the community contributors do not agree upon ( ) models trained on our dataset may generate deceitful texts that are unfaithful to what actually happened to particular entities ( ) though the instances in wikitablet cover various topics the writing style is almost always the same future work may explore more diverse writing styles references oshin agarwal heming ge siamak shakeri and rami al-rfou 2021. knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training in proceedings of the 2021 conference of the north american chapter of the association for computational linguistics human language technologies pages 3554–3565 online association for computational linguistics nader akoury shufan wang josh whiting stephen hood nanyun peng and mohit iyyer 2020. storium a dataset and evaluation platform for machine-in-the-loop story generation in proceedings of the 2020 conference on empirical methods in natural language processing emnlp , pages 6470–6484 online association for computational linguistics satanjeev banerjee and alon lavie 2005. meteor an automatic metric for mt evaluation with improved correlation with human judgments in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization pages 65–72 ann arbor michigan association for computational linguistics junwei bao duyu tang nan duan zhao yan yuanhua lv ming zhou and tiejun zhao 2018. tableto-text describing table region with natural lanin aaai conference on artiﬁcial intelliguage gence david l. chen and raymond j. mooney 2008. learning to sportscast a test of grounded language acquisition in proceedings of the 25th international conference on machine learning icml 08 page 128–135 new york ny usa association for computing machinery wenhu chen jianshu chen yu su zhiyu chen and william yang wang 2020. logical natural language generation from open-domain tables in proceedings of the 58th annual meeting of the association for computational linguistics pages 7929– 7942 online association for computational linguistics yong cheng wei xu zhongjun he wei he hua wu maosong sun and yang liu 2016. semisupervised learning for neural machine translation in proceedings of the 54th annual meeting of the association for computational linguistics volume : long papers , pages 1965–1974 berlin germany association for computational linguistics gonc¸alo m. correia vlad niculae and andr´e f. t. martins 2019. adaptively sparse transformers in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 2174– 2184 hong kong china association for computational linguistics jacob devlin ming-wei chang kenton lee and kristina toutanova 2019. bert pre-training of deep bidirectional transformers for language underin proceedings of the 2019 conference standing of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 4171–4186 minneapolis minnesota association for computational linguistics bhuwan dhingra manaal faruqui ankur parikh ming-wei chang dipanjan das and william cohen 2019. handling divergent reference texts when evaluating table-to-text generation in proceedings of the 57th annual meeting of the association for computational linguistics pages 4884–4895 florence italy association for computational linguistics hady elsahar pavlos vougiouklis arslen remaci christophe gravier jonathon hare frederique laforest and elena simperl 2018. t-rex a large scale alignment of natural language with knowledge base triples in proceedings of the eleventh international conference on language resources and evaluation lrec 2018 , miyazaki japan european language resources association elra . angela fan mike lewis and yann dauphin 2018. hiin proceedings erarchical neural story generation of the 56th annual meeting of the association for computational linguistics volume : long papers , pages 889–898 melbourne australia association for computational linguistics jeffrey flanigan chris dyer noah a. smith and jaime carbonell 2016. generation from abstract meaning representation using tree transducers in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies pages 731–739 san diego california association for computational linguistics claire gardent anastasia shimorina shashi narayan and laura perez-beltrachini 2017. the webnlg 202 : generating text from rdf data in proceedings of the 10th international conference on natural language generation pages 124–133 santiago de compostela spain association for computational linguistics demian gholipour , chris hokamp nghia the pham john glover and georgiana ifrim 2020. a large-scale multi-document summarization dataset from the wikipedia current events portal in proceedings of the 58th annual meeting of the association for computational linguistics pages 1302–1308 online association for computational linguistics li gong josep crego and jean senellart 2019. enhanced transformer model for data-to-text generation in proceedings of the 3rd workshop on neural generation and translation pages 148–156 hong kong association for computational linguistics valerie hajdik jan buys michael wayne goodman and emily m. bender 2019. neural text generation from rich semantic representations in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2259–2266 minneapolis minnesota association for computational linguistics hiroaki hayashi prashant budania peng wang chris ackerson raj neervannan and graham neubig 2020. wikiasp a dataset for multi-domain aspectbased summarization arxiv abs/2011.07832 di he yingce xia tao qin liwei wang nenghai yu tie-yan liu and wei-ying ma 2016. dual learning for machine translation in d. d. lee m. sugiyama u. v. luxburg i. guyon and r. garnett editors advances in neural information processing systems 29 pages 820–828 curran associates inc. jonathan herzig pawel krzysztof nowak thomas m¨uller francesco piccinno and julian eisenschlos 2020. tapas weakly supervised table parsing via in proceedings of the 58th annual pre-training meeting of the association for computational linguistics pages 4320–4333 online association for computational linguistics ari holtzman jan buys li du maxwell forbes and yejin choi 2020. the curious case of neural text dein international conference on learngeneration ing representations matthew honnibal and ines montani 2017. spacy : natural language understanding with bloom embeddings convolutional neural networks and incremental parsing to appear jun seok kang robert logan zewei chu yang chen dheeru dua kevin gimpel sameer singh and niranjan balasubramanian 2019. pomo generating in proentity-speciﬁc post-modiﬁers in context ceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 826–838 minneapolis minnesota association for computational linguistics guillaume klein yoon kim yuntian deng jean senellart and alexander rush 2017. opennmt opensource toolkit for neural machine translation in proceedings of acl 2017 system demonstrations pages 67–72 vancouver canada association for computational linguistics kalpesh krishna aurko roy and mohit iyyer 2021. hurdles to progress in long-form question answerin proceedings of the 2021 conference of ing the north american chapter of the association for computational linguistics human language technologies pages 4940–4957 online association for computational linguistics karen kukich 1983. design of a knowledge-based rein 21st annual meeting of the asport generator sociation for computational linguistics pages 145– 150 cambridge massachusetts usa association for computational linguistics r´emi lebret david grangier and michael auli 2016. neural text generation from structured data with in proceedapplication to the biography domain ings of the 2016 conference on empirical methods in natural language processing pages 1203–1213 austin texas association for computational linguistics jiwei li and eduard hovy 2015. the nlp engine a universal turing machine for nlp percy liang michael jordan and dan klein 2009. learning semantic correspondences with less superin proceedings of the joint conference of vision the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp pages 91–99 suntec singapore association for computational linguistics bill yuchen lin wangchunshu zhou ming shen pei zhou chandra bhagavatula yejin choi and xiang ren 2020. commongen a constrained text generation for generative commonsense reasoning in findings of the association for computational linguistics emnlp 2020 pages 1823–1840 online association for computational linguistics wonseok hwang jinyeung yim seunghyun park and minjoon seo 2019. a comprehensive exploration on wikisql with table-aware word contextualization arxiv preprint arxiv:1902.01069. chin-yew lin 2004. rouge a package for automatic evaluation of summaries in text summarization branches out pages 74–81 barcelona spain association for computational linguistics 203 peter j. liu , mohammad saleh , etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer 2018. generating wikipedia by summarizing long sequences in international conference on learning representations tianyu liu kexiang wang lei sha baobao chang and zhifang sui 2018. table-to-text generation in proceedby structure-aware seq2seq learning ings of the thirty-second aaai conference on artiﬁcial intelligence ( aaai-18 , the 30th innovative applications of artiﬁcial intelligence iaai-18 , and the 8th aaai symposium on educational advances in artiﬁcial intelligence eaai-18 , new orleans louisiana usa february 2-7 2018 pages 4881– 4888. aaai press robert logan nelson f. liu matthew e. peters matt gardner and sameer singh 2019. barack s wife hillary using knowledge graphs for fact-aware language modeling in proceedings of the 57th annual meeting of the association for computational linguistics pages 5962–5971 florence italy association for computational linguistics pedro henrique martins zita marinho and andr´e f. t. martins 2020. sparse text generation in proceedings of the 2020 conference on empirical methods in natural language processing emnlp , pages 4252–4273 online association for computational linguistics luca massarelli fabio petroni aleksandra piktus myle ott tim rockt¨aschel vassilis plachouras fabrizio silvestri and sebastian riedel 2020. how decoding strategies affect the veriﬁability of generated text in findings of the association for computational linguistics emnlp 2020 pages 223–235 online association for computational linguistics neil mcintyre and mirella lapata 2009. learning to tell tales a data-driven approach to story generation in proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp pages 217–225 suntec singapore association for computational linguistics kathleen mckeown 1992. text generation cam bridge university press linyong nan dragomir radev rui zhang amrit rau abhinand sivaprasad chiachun hsieh xiangru tang aadit vyas neha verma pranav krishna yangxiaokang liu nadia irwanto jessica pan faiaz rahman ahmad zaidi mutethia mutuma yasin tarabar ankit gupta tao yu yi chern tan xi victoria lin caiming xiong richard socher and nazneen fatema rajani 2021. dart open-domain structured data record to text generation in proceedings of the 2021 conference of the north american chapter of the association for computational linguistics human language technologies pages 432–447 online association for computational linguistics eyal orbach and yoav goldberg 2020. facts2story controlling text generation by key facts in proceedings of the 28th international conference on computational linguistics pages 2329–2345 barcelona spain online . international committee on computational linguistics richard yuanzhe pang and kevin gimpel 2019. unsupervised evaluation metrics and learning criteria for non-parallel textual transfer in proceedings of the 3rd workshop on neural generation and translation pages 138–147 hong kong association for computational linguistics kishore papineni salim roukos todd ward and weijing zhu 2002. bleu a method for automatic evalin proceedings of uation of machine translation the 40th annual meeting of the association for computational linguistics pages 311–318 philadelphia pennsylvania usa association for computational linguistics ankur parikh xuezhi wang sebastian gehrmann manaal faruqui bhuwan dhingra diyi yang and dipanjan das 2020. totto a controlled table-totext generation dataset in proceedings of the 2020 conference on empirical methods in natural language processing emnlp , pages 1173–1186 online association for computational linguistics romain paulus caiming xiong and richard socher 2018. a deep reinforced model for abstractive summarization in international conference on learning representations jeffrey pennington richard socher and christopher manning 2014. glove global vectors for word representation in proceedings of the 2014 conference on empirical methods in natural language processing emnlp , pages 1532–1543 doha qatar association for computational linguistics ben peters vlad niculae and andr´e f. t. martins 2019. sparse sequence-to-sequence models in proceedings of the 57th annual meeting of the association for computational linguistics pages 1504– 1519 florence italy association for computational linguistics vladimir propp 1968. morphology of the folktale vol ume 9. university of texas press ratish puduppully li dong and mirella lapata 2019. in data-to-text generation with entity modeling proceedings of the the 57th annual meeting of association for computational linguistics pages 2023–2035 florence italy association for computational linguistics hannah rashkin asli celikyilmaz yejin choi and jianfeng gao 2020. plotmachines outlineconditioned generation with dynamic plot state in proceedings of the 2020 conference tracking on empirical methods in natural language processing emnlp , pages 4274–4295 online association for computational linguistics 204 in proceedings of the 2017 conference on empirical methods in natural language processing pages 2253–2263 copenhagen denmark association for computational linguistics pengcheng yin graham neubig wen-tau yih and sebastian riedel 2020. tabert pretraining for joint in prounderstanding of textual and tabular data ceedings of the 58th annual meeting of the association for computational linguistics pages 8413– 8426 online association for computational linguistics markus zopf 2018. auto-hmds automatic construction of a large heterogeneous multilingual multidocument summarization corpus in proceedings of the eleventh international conference on language resources and evaluation lrec 2018 , miyazaki japan european language resources association elra . abigail see peter j. liu and christopher d. manning 2017. get to the point summarization with pointergenerator networks in proceedings of the 55th annual meeting of the association for computational linguistics volume : long papers , pages 1073– 1083 vancouver canada association for computational linguistics rico sennrich barry haddow and alexandra birch 2016. neural machine translation of rare words with subword units in proceedings of the 54th annual meeting of the association for computational linguistics volume : long papers , pages 1715– 1725 berlin germany association for computational linguistics rakshith shetty bernt schiele and mario fritz 2018. author attribute anonymity by adversarial a4nt in 27th training of neural machine translation usenix security symposium usenix security 18 , pages 1633–1650 erik f. tjong kim sang and fien de meulder 2003. introduction to the conll-2003 shared task language-independent named entity recognition in proceedings of the seventh conference on natural language learning at hlt-naacl 2003 pages 142–147 zhaopeng tu yang liu lifeng shang xiaohua liu and hang li 2017. neural machine translation with reconstruction in thirty-first aaai conference on artiﬁcial intelligence ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ł ukasz kaiser and illia polosukhin 2017. attention is all you need in i. guyon u. v. luxburg s. bengio h. wallach r. fergus s. vishwanathan and r. garnett editors advances in neural information processing systems 30 pages 5998–6008 curran associates inc. pavlos vougiouklis hady elsahar lucie-aim´ee kaffee christophe gravier fr´ed´erique laforest jonathon s. hare and elena simperl 2017. neural wikipedian generating textual summaries from knowledge base triples corr abs/1711.00155 qingyun wang xiaoman pan lifu huang boliang zhang zhiying jiang heng ji and kevin knight in proceed2018 describing a knowledge base ings of the 11th international conference on natural language generation pages 10–21 tilburg university the netherlands association for computational linguistics sean welleck ilia kulikov stephen roller emily dinan kyunghyun cho and jason weston 2020. neural text generation with unlikelihood training in international conference on learning representations sam wiseman stuart shieber and alexander rush 2017. in data-to-document generation 205 a dataset construction when collecting data we consider ﬁve resources wikidata tables infoboxes in wikipedia pages hyperlinks in the passage named entities in the passage obtained from named entity recognition ner , and wikipedia article structure for each article in wikipedia we use the same infobox and wikidata table for all sections these tables can serve as background knowledge for the article for each section in the article we create a second table corresponding to section-speciﬁc data i.e. section data the section data contains records constructed from hyperlinks and entities identiﬁed by a named entity recognizer section data contributes around 25 of the records in wikitablet we ﬁlter out several entity types related to numbers11 as the speciﬁc meanings of these numbers in the section of interest are difﬁcult to recover from the information in the tables after ﬁltering we use the identiﬁed entities as the values and the entity types as the attributes this contributes roughly 12 of the records in our ﬁnal dataset we also create records from hyperlinks in the section of interest we ﬁrst expand the hyperlinks available for each section with hyperlinks available in the parent categories we ﬁrst group hyperlinks across all wikipedia articles with those same categories and then we perform string matching between these hyperlinks and the text in the section if there are exact matches we will include those hyperlinks as part of the hyperlinks in this section details for constructing a record with attribute a and value for a hyperlink with surface text and hyperlinked article cid:96 are as follows to set a we use the value of the instance of or subclass of tuple in the wikidata table for cid:96 . if cid:96 does not have a wikidata table or no appropriate tuple we consider the parent categories of cid:96 as candidates for a. if there are multiple candidates for a we ﬁrst embed these candidates and a using glove pennington et al. 2014 embeddings and then choose the one that maximizes cosine similarity between the document titles or section titles and the candidates for a. for the value of the tuple we use the document title of cid:96 rather than the actual surface text to avoid giving away too much information in the reference text the records formed by hyperlinks contribute approximately 13 of the records in wikitablet 11list of ﬁltered entity types percent time quan tity ordinal cardinal we shufﬂe the ordering of the records from ner and the hyperlinks to prevent models from relying on the ordering of records in the reference text the records from the section data can be seen as section-speciﬁc information that can make the task more solvable complementary to the article data we create a title table that provides information about the position in which the section is situated which includes the article title and the section titles for the target section as the initial sections in wikipedia articles do not have section titles we use the section title introduction for these.12 as the records in our data tables come from different resources we perform extra ﬁltering to remove duplicates in the records in particular we give wikidata the highest priority as it is a humanannotated well-structured data resource infoboxes are human-annotated but not well-structured due to the way they are stored on wikipedia and the entities from ner the lowest priority as they are automatically constructed that is when we identify duplicates across different resources we will keep the records from the higher priority resource and drop those from the lower one more speciﬁcally the duplicates between wikidata records and infoboxes are determined by whether there are duplicate values or duplicate attributes for hyperlinks and infoboxes or wikidata they are judged by duplicate values for ner and hyperlinks they are based on whether there is any token overlapping between values after table collection we have the following criteria for ﬁltering out the texts ( ) we limit the text length to be between 50 and 1000 word tokens ( ) to ensure that there is sufﬁcient information in the table we only keep data-text pairs that contain more than records per sentence and more than 15 records per 100 tokens from wikidata and infoboxes ( ) to avoid texts such as lists of hyperlinks we ﬁlter out texts where more than 50 of their word tokens are from hyperlink texts b human evaluation the selected topics for human evaluations are human excluding the introduction and biography section , ﬁlm single song , song album television series when evaluating grammaticality and coherence only the generated text is shown to annotators 12among millions of section titles in wikipedia there are only 4672 sections including nested sections that are called introduction . therefore we believe this process will not introduce much noise into the dataset 206 = it is completely ungrammatical as it is impossible to understand the text 2 it has many grammatical errors and these errors make the text very difﬁcult to understand 3 it has grammatical errors and some of them make part of the text difﬁcult to understand 4 it has some grammatical errors but they are minor errors that do not affect reading 5 it is completely grammatical as it does not have any grammatical errors table 11 rating explanations for grammaticality 1 it is completely incoherent as it is impossible to piece together information in the text 2 it is incoherent in most places you can only understand part of the story 3 it is incoherent in many places but if you spend time reading it you still can understand the whole story 4 it is mostly coherent although the text is incoherent in some places it does not affect reading 5 it is completely coherent table 12 rating explanations for coherence the question for grammaticality is on a scale of 1-5 how much do you think the text is grammatical ( note repetitions are grammatical errors ) ( option explanations are shown in table 11 , and the question for coherence is on a scale of 1-5 how much do you think the text is coherent ( coherence does the text make sense internally avoid self-contradiction and use a logical ordering of information ) ( rating explanations are in table 12 . when evaluating faithfulness we show annotators the article data and the generation the question is on a scale of 1-5 how much do you think the text is supported by the facts in the following table ” rating explanations are in table 13 . when evaluating coherence and relevance annotators were shown the reference text and the generation as well as the wikipedia article title and section titles for ease of understanding the texts annotators were asked two questions with one being on a scale of 1-5 how much do you think the text is relevant to the reference ( table 14 , and the other being on a scale of 1-5 how much do you think the text is supported by the facts in the reference ” table 15 . effect of α-entmax in this section we disentangle the effect of αentmax and that of α-entmax loss we note that 1 when not using the α-entmax loss we use standard cross entropy loss e.g. in the case of base+ent. 1 it is completely contradictory to what is described in the table 2 it has some facts contradictory to what is described in the table 3 it is not supported by the table and it does not contradict the table 4 some of the text is supported by the facts in the table and the rest of it does not contradict the facts in the table 5 it is completely supported by the table table 13 rating explanations for faithfulness 1 the text is completely irrelevant to the reference 2 most of the text is irrelevant to the reference 3 some of the text is relevant to the reference 4 most of the text is relevant to the reference 5 the text is talking about the same thing as the reference table 14 rating explanations for relevance we maximize the log probabilities generated by αentmax ; 2 when combining α-entmax and copy mechanism we aggregate the probabilities generated by α-entmax and those from softmax this is because we use the ﬁrst attention head in the transformer decoder as the copy attention following the implementation in opennmt klein et al. 2017 . while it is feasible to combine the α-entmax and α-entmax loss with the copy mechanism if we use the sparse transformer correia et al. 2019 , we leave this for future study we report the results in table 16. it is interesting to see that when using greedy decoding “ ent + ent loss outperforms the baseline model by a signiﬁcant margin on all the metrics however the improvement disappears except for repetition after we switch to use beam search as the decoding strategy this is likely because α-entmax promotes sparsity in the generated probabilities making beam search decoding unnecessary removing the α-entmax loss hurts the performance but its gains become larger in switching to beam search decoding adding copy mechanism improves the performance leading to comparable performance to the baseline model although base+ent.+copy still underperforms base+copy when using beam search we believe that combining α-entmax and α-entmax loss with the copy mechanism is promising as 1 α-entmax is not used in our large models and the initial results have shown that α-entmax and the copy mechanism are complementary so it may further improve our current best performance ( ) α-entmax already shows the best performance when using greedy decoding which has speed and optimization advantages compared to the beam search based decoding strategies especially considering the long-form characteristic 207 = it has quite a few facts contradictory to what is described in the reference 2 it has some facts contradictory to what is described in the reference 3 it is not supported by the reference and it does not contradict the reference 4 some of the text is supported by the facts in the reference and the rest of it does not contradict the reference 5 it is completely supported by the reference table 15 rating explanations for supportedness rep bleu par-p par-r par-f1 base 38.1 ent + ent loss 36.0 ent 44.5 43.7 ent + copy copy 37.8 greedy decoding 61.6 62.2 63.5 64.2 61.3 14.7 16.2 13.9 14.8 15.8 beam search beam size ) base 33.0 ent + ent loss 25.9 34.7 ent 34.1 ent + copy copy 30.1 15.6 15.4 13.8 15.0 15.9 66.3 64.6 67.2 69.4 67.1 27.7 28.9 25.5 26.6 28.3 28.8 29.0 26.6 28.1 29.4 35.8 37.0 33.9 35.2 36.3 37.7 37.7 35.8 37.6 38.5 table 16 effect of using α-entmax and α-entmax loss when not using the α-entmax loss we use standard cross entropy loss of wikitablet d details of cyclic loss in this section we will denote the linearized table where the values are replaced with a special cid:104 mask cid:105 token by u1 ··· un and denote the reference text by x1 ··· xm formally the training loss is log ( w|u1 ··· un v1 ··· vm ( ) 2 w∈s cid:88 w∈s where represents the set of masked tokens and v1 ··· vm is the sequence of token-level probabilities predicted by the forward model in our experiments these could either come from the softmax function or the α-entmax function . speciﬁcally we multiply the backward transformer s input embedding matrix by the probability vectors to obtain the input representations to the ﬁrst encoder layer we ﬁnd that it is helpful to add a reference loss while training with the cyclic loss deﬁned as log ( w|u1 ··· un x1 ··· xm ( ) this loss does not contain the generation model in it explicitly but it does lead to an improved backward model by training it with clean inputs improving both art only sec only rep bleu par-p par-r par-f1 38.1 60.9 39.0 61.6 55.2 56.1 27.7 14.7 24.3 35.8 20.8 31.7 14.7 8.4 13.4 table 17 effect of dropping section or article data from the input using the base setting . none both art only sec only rep bleu par-p par-r par-f1 37.8 35.9 37.2 34.8 15.8 15.8 15.8 15.9 28.3 28.5 28.1 28.2 36.3 36.7 36.2 36.2 61.3 62.0 61.7 61.9 table 18 effect of dropping section or article data when using cyclic training the results are based on the base copy and base copy cyclic loss settings the backward model then increases the beneﬁts of the cyclic loss.13 effect of article data and section data we report results in table 17 for the models that are trained with partial data input where art only and sec only indicate that we use only article data or section data respectively we always use title data section data contributes the most to the bleu and par scores but using section data and article data together is the best setting we also investigate the effect of partial data input for the cyclic loss in table 18 where none is the model that is not trained with the cyclic loss we note that in this setting we still use both data resources as the input to the forward model but vary the input data and the gold standard for the backward model although using only section data gives the best rep score and improves the parp score it does not help the model in other metrics combining the article data with the section data gives signiﬁcant improvements to the par-f1 score compared to section data alone both experiments show that there are interactions between these two data resources that can help models to learn better from both kinds f generation examples we show the full set of generations in table 19. the part of input data and reference text for table 19 is shown in figure . 13we experimented with initializing the backward model with pretrained checkpoints but did not ﬁnd it helpful 208 figure : an example from wikitablet only parts of the tables and reference are shown due to space constraints underlined texts are hyperlinks records with the attributes date , org , money are from ner subscripts for section titles indicate the ordering of nesting where smaller numbers indicate higher level sections method sampling 0.9 beam search size=5 blocking size=5 sampling 0.9 beam search size=5 blocking size=5 sampling 0.9 beam search size=5 blocking size=5 generated text in a two-issue mini-series titled messiah complex , portrayed by maisie williams father craig is ordered to rescue the world from reavers and run out of the x-force causing the x-force to permanently withdraw the original humans were replaced by slots and muir island being pulled away they also possessed the entire x-force after issue 9 in which the x-force appeared claremont and sinclair later reprised their parts in the donderful world of wolverine with wolfsbane as one of the new x-men and her group taken back to their reservation for unknown reasons in the messiah complex storyline wolfsbane is a member of the x-men she is a member of the x-factor investigations team and a member of the x-force/x-force/xforce/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/xforce/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/xforce/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/x-force/xforce/x-force/x-force/x-force/x-force/x-force/ in the messiah complex storyline wolfsbane is a member of the x-men a group of mutants recruited by father craig to ﬁght the reavers in muir island father craig is killed in the battle with reavers but father craig manages to rescue the mutant from the rain of gesham and retrieve the mutant she is later revealed to have been killed by rain in a battle with father craig in the aftermath father craig tells father craig that she is the only mutant to be killed by a mutant journey to the center of the earth is a 1959 american science ﬁction ﬁlm based on jules verne s 1959 novel of the same name the ﬁlm directed by henry levin stars james mason as professor otto lidenbrock a scientist who gets entangled in the power of the archenemy as he tries to impress his closest allies and compromises them with the future the ﬁlm marks the ﬁrst screen appearance of future cinemascope of 20th century fox lead singer peter ronson ” the 20th century fox earned critical acclaim and was a box-ofﬁce success grossing 10 million worldwide against a production budget of 3.44 million journey to the center of the earth is a 1959 american science ﬁction ﬁlm directed by henry levin and starring james mason pat boone and arlene dahl the screenplay by charles brackett and walter reisch is based on jules verne s novel a journey to the center of the earth ( 1959 . the ﬁlm was produced by charles brackett and walter reisch and distributed by twentieth century-fox film corporation the ﬁlm was nominated for an academy award for best art direction it was also nominated for an academy award for best art direction and a golden globe award for best visual effects it was also nominated for a golden globe award for best motion picture musical or comedy but lost to 20th century fox s cinemascope . journey to the center of the earth is a 1959 american science ﬁction ﬁlm directed by henry levin and starring james mason pat boone and arlene dahl the screenplay by charles brackett and walter reisch is based on jules verne s novel of the same name the ﬁlm was produced by cinemascope and 20th century fox and distributed by twentieth century-fox film corporation and joseph m. schenck enterprises inc. it was nominated for an academy award for best art direction in may 2013 mccoy allegedly assault a spoiler business ofﬁcial the motivated him to take a business trip to harrisburg pennsylvania and sign a letter from the buffalo bills paying him over 50,000 as well as verbally prohibiting legal kickoff work in september 2014 mccoy shared the issue with some former players of the ﬁrst two ﬁlm franchises mccoy played a leading role in the 2012 marvel cinematic universe ﬁlm erab of the press , and was named one of the ﬁlm s biggest stars on twitter on september , 2014 mccoy ﬁled a lawsuit against pyt to change its twitter name to lesean mccoy productions on september , 2014 mccoy was arrested in harrisburg pennsylvania on suspicion of assault he was charged with assault and battery in may 2013 mccoy was ﬁned over 50,000 by the buffalo bills in september 2014 mccoy was arrested in harrisburg pennsylvania on suspicion of assaulting a woman who had been raped by mccoy mccoy was charged with assault and possession of marijuana mccoy was suspended from the pyt for the ﬁrst two games of the marvel cinematic universe on september , 2014 mccoy was arrested in harrisburg pennsylvania on suspicion of assault he was charged with assault and battery in may 2013 he was ﬁned over 50,000 by the buffalo bills in september 2014 he was suspended for two games by the pyt for violating the marvel cinematic universe he was released by the bills in october of the same year he was cleared of all charges on twitter and was banned from playing in the 2014 pro bowl due to his twitter account table 19 top generation examples for the ﬁrst instance in figure 1. middle generation examples for the second instance in figure 1. bottom generation examples that correspond to the instance in figure 2. details of decoding strategies nucleus sampling generating long sequences usually suffers from repetitions nucleus sampling holtzman et al. 2020 aims to reduce the repetitions in generations by sampling from truncated probability distributions the truncation is based on whether the cumulative probability is above a threshold we set the threshold to be 0.9 as suggested in holtzman et al ( 2020 . beam search with n-gram blocking paulus et al ( 2018 found it effective to reduce the repetitions during beam search by blocking n-grams that have been generated in previous decoding steps we follow their approach by using trigram blocking and setting the probability of repeated trigrams to be during beam search 209 in september 2014 mccoy received criticism for leaving a 0.20 tip at the pyt burger restaurant in philadelphia mccoy claimed the low tip was due to `` bad service. the owner responded on september , 2014 saying that mccoy had been `` verbally abusive '' and made `` derogatory comments about women. in april 2019 mccoy tweeted an important plot point in the film avengers endgame shortly after watching the film angering his followers and marvel fans to the point that some blocked him and even called for the termination of his contract with the buffalo bills or for him to get injured mccoy later called this the `` worst decision '' of his life in an interview with good morning football attribute information violent crime org date money social networking service film series section data value spoiler media assault the buffalo bills september , 2014 0.200 twitter marvel cinematic universe attribute date of birth instance of place of birth occupation article data value 1988-07-12 human harrisburg american football player title data document title section title1 section title2 lesean mccoy personal life controversies mixed-lingual pre-training for cross-lingual summarization ruochen xu∗ chenguang zhu∗ yu shi michael zeng xuedong huang microsoft cognitive services research group ruox chezhu yushi nzeng xdh @ microsoft.com abstract cross-lingual summarization cls aims at producing a summary in the target language for an article in the source language traditional solutions employ a twotranslate→summarize or step approach i.e summarize→translate recently end-to-end models have achieved better results but these approaches are mostly limited by their dependence on large-scale labeled data we propose a solution based on mixed-lingual pretraining that leverages both cross-lingual tasks such as translation and monolingual tasks like masked language models thus our model can leverage the massive monolingual data to enhance its modeling of language moreover the architecture has no task-speciﬁc components which saves memory and increases optimization efﬁciency we show in experiments that this pre-training scheme can effectively boost the performance of cross-lingual summarization in neural cross-lingual summarization ncls ( zhu et al. 2019b dataset our model achieves an improvement of 2.82 english to chinese and 1.15 chinese to english rouge-1 scores over state-of-the-art results introduction text summarization can facilitate the propagation of information by providing an abridged version for long articles and documents meanwhile the globalization progress has prompted a high demand of information dissemination across language barriers thus the cross-lingual summarization cls task emerges to provide accurate gist of articles in a foreign language traditionally most cls methods follow the twostep pipeline approach either translate the article into the target language and then summarize it leuski et al. 2003 , or summarize the article in the source language and then translate it wan equal contribution et al. 2010 . although this method can leverage off-the-shelf summarization and mt models it suffers from error accumulation from two independent subtasks therefore several end-to-end approaches have been proposed recently zhu et al. 2019b ouyang et al. 2019 duan et al. 2019 , which conduct both translation and summarization simultaneously easy to optimize as these methods are they typically require a large amount of cross-lingual summarization data which may not be available especially for low-resource languages for instance ncls zhu et al. 2019b proposes to co-train on monolingual summarization ms and machine translation mt tasks both of which require tremendous labeling efforts on the other hand the pre-training strategy has proved to be very effective for language understanding devlin et al. 2018 holtzman et al. 2019 and cross-lingual learning lample and conneau 2019 chi et al. 2019 . one of the advantages of pre-training is that many associated tasks are selflearning by nature which means no labeled data is required this greatly increases the amount of training data exposed to the model thereby enhancing its performance on downstream tasks therefore we leverage large-scale pre-training to improve the quality of cross-lingual summarization built upon a transformer-based encoderdecoder architecture vaswani et al. 2017 , our model is pre-trained on both monolingual tasks including masked language model mlm , denoising autoencoder dae and monolingual summarization ms , and cross-lingual tasks such as crosslingual masked language model cmlm and machine translation mt . this mixed-lingual pretraining scheme can take advantage of massive unlabeled monolingual data to improve the model s language modeling capability and leverage crosslingual tasks to improve the model s cross-lingual representation we then ﬁnetune the model on the proceedings of the 1st conference of the asia-paciﬁc chapter of the association for computational linguistics and the 10th international joint conference on natural language processing pages 536–541 december 7 2020. ( ) 2020 association for computational linguistics 536 downstream cross-lingual summarization task furthermore based on a shared multi-lingual vocabulary our model has a shared encoder-decoder architecture for all pre-training and ﬁnetuning tasks whereas ncls zhu et al. 2019b sets aside taskspeciﬁc decoders for machine translation monolingual summarization and cross-lingual summarization in the experiments our model outperforms various baseline systems on the benchmark dataset ncls zhu et al. 2019b . for example our model achieves 3.27 higher rouge-1 score in chinese to english summarization than the state-of-the-art result and 1.28 higher rouge-1 score in english to chinese summarization we further conduct an ablation study to show that each pretraining task contributes to the performance especially our proposed unsupervised pretraining tasks 2 related work 2.1 pre-training pre-training language models devlin et al. 2018 dong et al. 2019 have been widely used in nlp applications such as question answering zhu et al. 2018 , sentiment analysis peters et al. 2018 , and summarization zhu et al. 2019a yang et al. 2020 . in multi-lingual scenarios recent works take input from multiple languages and shows great improvements on cross-lingual classiﬁcation lample and conneau 2019 pires et al. 2019 huang et al. 2019 and unsupervised machine translation liu et al. 2020 . artetxe and schwenk 2019 employs the sequence encoder from a machine translation model to produce cross-lingual sentence embeddings chi et al ( 2019 uses multi-lingual pre-training to improve cross-lingual question generation and zero-shot cross-lingual summarization their model trained on articles and summaries in one language is directly used to produce summaries for articles in another language which is different from our task of producing summaries of one language for an article from a foreign language 2.2 cross-lingual summarization early literatures on cross-lingual summarization focus on the two-step approach involving machine translation and summarization leuski et al. 2003 wan et al. 2010 , which often suffer from error propagation issues due to the imperfect modular systems recent end-to-end deep learning models have greatly enhanced the performance shen et al ( 2018 presents a solution to zero-shot cross-lingual headline generation by using machine translation and summarization datasets duan et al ( 2019 leverages monolingual abstractive summarization to achieve zero-shot cross-lingual abstractive sentence summarization ncls zhu et al. 2019b proposes a cross-lingual summarization system for large-scale datasets for the ﬁrst time it uses multitask supervised learning and shares the encoder for monolingual summarization cross-lingual summarization and machine translation however each of these tasks requires a separate decoder in comparison our model shares the entire encoder-decoder architecture among all pre-training and ﬁnetuning tasks and leverages unlabeled data for monolingual masked language model training a concurrent work by zhu et al ( 2020 improves the performance by combining the neural model with an external probabilistic bilingual lexicon 3 method 3.1 pre-training objectives we propose a set of multi-task pre-training objectives on both monolingual and cross-lingual corpus for monolingual corpus we use the masked language model mlm from raffel et al ( 2019 . the input is the original sentence masked by sentinel tokens and the target is the sequence consists of each sentinel token followed by the corresponding masked token the other monolingual task is the denoising auto-encoder dae , where the corrupted input is constructed by randomly dropping masking and shufﬂing a sentence and the target is the original sentence since our ﬁnal task is summarization we also include monolingual summarization ms as a pre-training task to leverage cross-lingual parallel corpus we introduce the cross-lingual masked language model cmlm . cmlm is an extension of mlm on the parallel corpus the input is the concatenation of a sentence in language a and its translation in language b. we then randomly select one sentence and mask some of its tokens by sentinels the target is to predict the masked tokens in the same way as mlm different from mlm the masked tokens in cmlm are predicted not only from the context within the same language but also from their translations in another language which encourages the model to learn language-invariant representations note that cmlm is similar to the translation language model tlm loss proposed in lample 537 objective masked language model denoising auto-encoder monolingual summarization cross-lingual mlm cross-lingual mlm machine translation cid:88 ( ) cid:88 ( ) supervised multi-lingual inputs targets france x morocco in y exhibition match < > beats y an france beats m in m exhibition france beats morocco in an exhibition match world champion france overcame a stuttering start to beat morocco 1-0 in a scrappy exhibition match on wednesday night france x morocco in y exhibition match 法国队在一场表演赛中击败摩洛哥队。 france beats morocco in an exhibition match < > 队在一场表演赛中 y 摩洛哥队。 france beats morocco in an exhibition match ( ) cid:88 ( ) france beats morocco in an exhibition match < > beats y an x 法国 y 击败 法国队在一场表演赛中击败摩洛哥队。 table : examples of inputs and targets used by different objectives for the sentence france beats morocco in an exhibition match with its chinese translation we use x and y to denote sentinel tokens and m to denote shared mask tokens and conneau 2019 . the key differences are 1 tlm randomly masks tokens in sentences from both languages while cmlm only masks tokens from one language 2 tlm is applied on encoderonly networks while we employ cmlm on the encoder-decoder network in addition to cmlm we also include standard machine translation mt objective in which the input and output are the unchanged source and target sentences respectively the examples of inputs and targets used by our pre-training objectives are shown in table . 3.2 uniﬁed model for pre-training and finetuning while ncls zhu et al. 2019b uses different decoders for various pre-training objectives we employ a uniﬁed transformer vaswani et al. 2017 encoder-decoder model for all pre-training and ﬁnetuning tasks this makes our model learn a crosslingual representation efﬁciently a shared dictionary across all languages is used to accommodate multi-task and multilingual objectives we introduce language id symbols to indicate the target language and task symbols to indicate the target task for instance for the cmlm objective where the target language is chinese the decoder takes cmlm and zh as the ﬁrst two input tokens we empirically ﬁnd that our model does not suffer from the phenomenon of forgetting target language controllability as in chi et al ( 2019 , which requires manual freezing of encoder or decoder during ﬁnetuning after pretraining we conduct ﬁnetuning on cross-lingual summarization data 4 experiments 4.1 dataset we conduct our experiment on ncls dataset zhu et al. 2019b , which contains paired data of english articles with chinese summaries and chinese articles with english summaries the cross-lingual training data is automatically generated by a machine translation model for ﬁnetuning and testing we followed the same train/valid/test split of the original dataset we refer readers to table in zhu et al ( 2019b for detailed statistics of the dataset for pre-training we obtain monolingual data for english and chinese from the corresponding wikipedia dump there are 83 million sentences for english monolingual corpus and 20 million sentences for chinese corpus for parallel data between english and chinese we use the parallel corpus from lample and conneau 2019 , which contains 9.6 million paired sentences for monolingual summarization objective we use cnn/dailymail dataset nallapati et al. 2016 for english summarization and lcsts dataset hu et al. 2015 for chinese summarization implementation details 4.2 our transformer model has layers and heads in attention the input and output dimensions dmodel for all transformer blocks are 512 and the inner dimension df is 2048. we use a dropout probability of 0.1 on all layers we build a shared sentencepiece kudo and richardson 2018 vocabulary of size 33 000 from a balanced mix of the monolingual wikipedia corpus the model has approximately 61m parameters for mlm we use a mask probability of 0.15. for dae we set both the mask and drop out rate 538 chinese→english english→chinese rouge-1 rouge-2 rouge-l rouge-1 rouge-2 rouge-l 26.15 tetran 28.19 getran 30.22 tltran gltran 32.17 36.82 ncls ncls-ms 38.25 ncls-mt 40.23 xnlg 39.85 40.68 ats 43.50 ours 23.24 25.77 27.04 29.43 33.20 34.76 36.59 28.28 36.97 29.66 10.60 11.40 12.20 13.85 18.72 20.20 22.32 24.47 24.12 25.41 23.09 24.34 33.92 35.45 38.85 40.34 40.25 38.34 40.47 41.62 7.33 9.14 15.81 16.86 21.93 22.65 22.58 19.65 22.21 23.35 18.74 20.13 29.86 31.28 35.05 36.39 36.21 33.66 36.89 37.26 table : rouge-1 rouge-2 rouge-l for english to chinese and chinese to english summarization on ncls dataset to 0.1. for all pre-training and ﬁnetuning we use radam optimizer liu et al. 2019 with β1 0.9 β2 0.999. the initial learning rate is set to 10−9 for pre-training and 10−4 for ﬁnetuning the learning rate is linearly increased to 0.001 with 16 000 warmup steps followed by an exponential decay for decoding we use a beam size of and a maximum generation length of 200 tokens for all experiments english→chinese rouge-1 rouge-2 rouge-l 43.50 42.48 42.12 41.82 41.12 ours ms mt mlm dae all pretraining table : finetuning performance on english→chinese summarization starting with various ablated pre-trained models 29.66 28.49 28.74 28.40 28.53 25.41 24.45 23.97 23.85 23.67 4.3 baselines we ﬁrst include a set of pipeline methods from zhu et al ( 2019b which combines monolingual summarization and machine translation tetran ﬁrst translates the source document and then uses lexrank erkan and radev 2004 to summarize the translated document tltran ﬁrst summarizes the source document and then translates the summary getran and gltran replace the translation model in tetran and tltran with google translator1 respectively we also include three strong baselines from zhu et al ( 2019b : ncls ncls-ms and ncls-mt 1https //translate.google.com/ ncls trains a standard transformer model on the cross-lingual summarization dataset ncls-ms and ncls-mt both use one encoder and multiple decoders for multi-task scenarios ncls-ms combines the cross-lingual summarization task with monolingual summarization while ncls-mt combines it with machine translation we ﬁnetune xnlg model from chi et al ( 2019 on the same cross-lingual summarization data we ﬁnetune all layers of xnlg in the same way as our pretrained model finally we include the result of ats from the concurrent work of zhu et al ( 2020 . 4.4 results table shows the rouge scores of generated summaries in english-to-chinese and chinese-toenglish summarization as shown pipeline models although incorporating state-of-the-art machine translation systems achieve sub-optimal performance in both directions proving the advantages of end-to-end models our model outperforms all baseline models in all metrics except for rouge-l in english-tochinese for instance our model achieves 2.82 higher rouge-1 score in chinese to english summarization than the previously best result and 1.15 higher rouge-1 score in english to chinese summarization which shows the effectiveness of utilizing multilingual and multi-task data to improve cross-lingual summarization 4.5 ablation study table shows the ablation study of our model on english to chinese summarization we remove 539 a english→chinese rouge-1 b chinese→english rouge-1 figure : rouge-1 performance on ncls dataset when the cross-lingual summarization training data is subsampled to size of 1k and 10k the result on the full dataset is also shown from the pre-training objectives ) all monolingual unsupervised tasks mlm dae , ii machine translation mt , iii monolingual summarization ms , and iv all the objectives note that all pretraining and ncls both only train on the cross-lingual summarization data the performance difference between the two is most likely due to the difference in model size vocabulary and other hyperparameters as shown the pre-training can improve rouge1 rouge-2 and rouge-l by 2.38 1.74 and 1.13 points respectively on chinese-to-english summarization moreover all pre-training objectives have various degrees of contribution to the results and the monolingual unsupervised objectives mlm and dae are relatively the most important this veriﬁes the effectiveness of leveraging unsupervised data in the pre-training low-resource scenario we sample subsets of size 1k and 10k from the training data of crosslingual summarization and ﬁnetune our pre-trained model on those subsets figure shows the the performance of the pre-trained model and the model trained from scratch on the same subsets as shown the gain from pre-training is larger when the size of training data is relatively small this proves the effectiveness of our approach to deal with low-resource language in cross-lingual summarization 5 conclusion we present a mix-lingual pre-training model for cross-lingual summarization we optimize a shared encoder-decoder architecture for multi-lingual and multi-task objectives experiments on a benchmark dataset show that our model outperforms pipelinebased and other end-to-end baselines through an ablation study we show that all pretraining objectives contribute to the model s performance references mikel artetxe and holger schwenk 2019. massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond transactions of the association for computational linguistics 7:597–610 zewen chi li dong furu wei wenhui wang xianling mao and heyan huang 2019. cross-lingual natural language generation via pre-training arxiv preprint arxiv:1909.10481. jacob devlin ming-wei chang kenton lee and kristina toutanova 2018. bert pre-training of deep bidirectional transformers for language understanding arxiv preprint arxiv:1810.04805. li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao-wuen hon 2019. uniﬁed language model pre-training for natural language understanding and generation in advances in neural information processing systems pages 13063–13075 xiangyu duan mingming yin min zhang boxing chen and weihua luo 2019. zero-shot crosslingual abstractive sentence summarization through teaching generation and attention in proceedings of the 57th annual meeting of the association for computational linguistics pages 3162–3172 g¨unes erkan and dragomir radev 2004. lexrank graph-based lexical centrality as salience in text summarization journal of artiﬁcial intelligence research 22:457–479 540 50.00 45.00 40.00 35.00 30.00 25.00 20.00 15.00 10.00 5.00 0.00 train data 1k 10k 364k full ours ours w/o pretraining 45.00 40.00 35.00 30.00 25.00 20.00 15.00 10.00 5.00 0.00 train data 1k 10k 1693k full ours ours w/o pretraining ari holtzman jan buys li du maxwell forbes and yejin choi 2019. the curious case of neural text degeneration arxiv preprint arxiv:1904.09751. baotian hu qingcai chen and fangze zhu 2015. lcsts a large scale chinese short text summarization dataset in proceedings of the 2015 conference on empirical methods in natural language processing pages 1967–1972 haoyang huang yaobo liang nan duan ming gong linjun shou daxin jiang and ming zhou 2019. unicoder a universal language encoder by prein protraining with multiple cross-lingual tasks ceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 2485–2494 taku kudo and john richardson 2018. sentencepiece a simple and language independent subword tokenizer and detokenizer for neural text processing in proceedings of the 2018 conference on empirical methods in natural language processing system demonstrations pages 66–71 brussels belgium association for computational linguistics guillaume lample and alexis conneau 2019. crosslingual language model pretraining arxiv preprint arxiv:1901.07291. anton leuski chin-yew lin liang zhou ulrich germann franz josef och and eduard hovy 2003. cross-lingual * st rd english access to hindi information acm transactions on asian language information processing talip , ( ) :245–269 liyuan liu haoming jiang pengcheng he weizhu chen xiaodong liu jianfeng gao and jiawei han 2019. on the variance of the adaptive learning rate and beyond in international conference on learning representations yinhan liu jiatao gu naman goyal xian li sergey edunov marjan ghazvininejad mike lewis and luke zettlemoyer 2020. multilingual denoising pre-training for neural machine translation arxiv preprint arxiv:2001.08210. ramesh nallapati bowen zhou cicero dos santos caglar guclcehre and bing xiang 2016. abstractive text summarization using sequence-to-sequence in proceedings of the 20th rnns and beyond signll conference on computational natural language learning pages 280–290 jessica ouyang boya song and kathleen mckeown 2019. a robust abstractive system for cross-lingual summarization in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2025–2031 matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer 2018. deep contextualized word representations arxiv preprint arxiv:1802.05365. telmo pires eva schlinger and dan garrette 2019. how multilingual is multilingual bert in proceedings of the 57th annual meeting of the association for computational linguistics pages 4996–5001 colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter liu 2019. exploring the limits of transfer learning with a uniﬁed text-to-text transformer arxiv preprint arxiv:1910.10683. shi-qi shen yun chen cheng yang zhi-yuan liu mao-song sun et al 2018. zero-shot cross-lingual ieee/acm transacneural headline generation tions on audio speech and language processing 26 12 :2319–2327 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser and illia polosukhin 2017. attention is all in advances in neural information proyou need cessing systems pages 5998–6008 xiaojun wan huiying li and jianguo xiao 2010. cross-language document summarization based on machine translation quality prediction in proceedings of the 48th annual meeting of the association for computational linguistics pages 917–926 association for computational linguistics ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang and eric darve 2020. ted a pretrained unsupervised summarization model with theme modeling and denoising arxiv preprint arxiv:2001.00725. chenguang zhu ziyi yang robert gmyr michael zeng and xuedong huang 2019a make lead bias in your favor a simple and effective arxiv preprint method for news summarization arxiv:1912.11602. chenguang zhu michael zeng and xuedong huang 2018. sdnet contextualized attention-based deep network for conversational question answering arxiv preprint arxiv:1812.03593. junnan zhu qian wang yining wang yu zhou jiajun zhang shaonan wang and chengqing zong 2019b ncls neural cross-lingual summarization arxiv preprint arxiv:1909.00156. junnan zhu yu zhou jiajun zhang and chengqing zong 2020. attend translate and summarize an efﬁcient method for neural cross-lingual summarizain proceedings of the 58th annual meeting tion of the association for computational linguistics pages 1309–1321 541 jointly learning to align and summarize for neural cross-lingual summarization yue cao hui liu xiaojun wan wangxuan institute of computer technology peking university the moe key laboratory of computational linguistics peking university center for data science peking university yuecao xinkeliuhui wanxiaojun @ pku.edu.cn abstract cross-lingual summarization is the task of generating a summary in one language given a text in a different language previous works on cross-lingual summarization mainly focus on using pipeline methods or training an endto-end model using the translated parallel data however it is a big for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time in this paper we propose to ease the cross-lingual summarization training by jointly learning to align and summarize we design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages experimental results show that our model can outperform competitive models in most cases in addition we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus introduction neural abstractive summarization has witnessed rapid growth in recent years variants of sequenceto-sequence models have shown to obtain promising results on english see et al. 2017 or chinese summarization datasets however cross-lingual summarization which aims at generating a summary in one language from input text in a different language has been rarely studied because of the lack of parallel corpora early researches on cross-lingual abstracsummarization are mainly based on tive the translationsummarization-translation or summarization pipeline paradigm and adopt different strategies to incorporate bilingual features leuski et al. 2003 orasan and chiorean 2008 wan et al. 2010 wan 2011 into the pipeline model recently shen et al ( 2018 ﬁrst propose a neural cross-lingual summarization system based on a large-scale corpus they ﬁrst translate the texts automatically from the source language into the target language and then use the teacher-student framework to train a cross-lingual summarization model duan et al ( 2019 further improve this teacher-student framework by using genuine summaries paired with the translated pseudo source sentences to train the cross-lingual summarization model zhu et al ( 2019 propose a multi-task learning framework to train a neural cross-lingual summarization model cross-lingual summarization is a task as it requires learning to understand different languages and learning how to summarize at the same time it would be difﬁcult for the model to directly learn cross-lingual summarization in this paper we explore this question can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages learning cross-lingual representations has been proven a beneﬁcial method for cross-lingual transfer for some downstream tasks klementiev et al. 2012 artetxe et al. 2018 ahmad et al. 2019 chen et al. 2019 . the underlying idea is to learn a shared embedding space for two languages to improve the model s ability for cross-lingual transfer recently it has been shown that this method can also be applied to context representations aldarmaki and diab 2019 schuster et al. 2019 . in this paper we show that the learning of cross-lingual representations is also beneﬁcial for neural crosslingual summarization models we propose a multi-task framework that jointly learns to summarize and align context-level representations concretely we ﬁrst integrate monolingual summarization models and cross-lingual summarization models into one uniﬁed model and then proceedings of the 58th annual meeting of the association for computational linguistics pages 6220–6231 july 10 2020. ( ) 2020 association for computational linguistics 6220 build two linear mappings to project the context representation from one language to the other we then design several relevant loss functions to learn the mappers and facilitate the cross-lingual summarization in addition we propose some methods to enhance the isomorphism and cross-lingual transfer between different languages we also show that the learning of aligned representation enables our model to generate cross-lingual summaries even in a fully unsupervised way where no parallel crosslingual data is required we conduct experiments on several public crosslingual summarization datasets experiment results show that our proposed model outperforms competitive models in most cases and our model also works on the unsupervised setting to the best of our knowledge we are the ﬁrst to propose an unsupervised framework for learning neural crosslingual summarization in summary our primary contributions are as follow • we propose a framework that jointly learns to align and summarize for neural cross-lingual summarization and design relevant loss functions to train our system • we propose a procedure to train our crosslingual summarization model in an unsupervised way • the experimental results show that our model outperforms competitive models in most cases and our model has the ability to generate cross-lingual summarization even without any cross-lingual corpus 2 overview we show the overall framework of our proposed model in figure 1. our model consists of two encoders two decoders two linear mappers and two discriminators suppose we have an english source text = x1 . . xm and a chinese source text = y1 . . yn , which consist of and words respectively the english encoder φex res chinese encoder φey transforms ( res y into its context representation zx res zy , and the decoder φdx res φdy reads the memory zx res zy and generates the corresponding english summary ˜x res chinese summary ˜y . the mappers mx zx zy and my zy zx are used for transformations between zx and figure : the overall framework of our proposed model zy and the discriminators dx and dy are used for discriminating between the encoded representations and the mapped representations taking english-to-chinese summarization for example our model generates cross-lingual summaries as follows first we use the english encoder to get the english context representations then we use the mapper to map english representations into chinese space lastly the chinese decoder is used to generate chinese summaries in section , we describe the techniques we adopt to enhance the cross-lingual transferability of the model in section and section , we describe the unsupervised training objective and supervised training objective for cross-lingual summarization respectively 3 model adjustment for cross-lingual transfer 3.1 normalizing the representations in our model we adopt transformer vaswani et al. 2017 as our encoder and decoder which is the same with previous works duan et al. 2019 zhu et al. 2019 . the encoder and decoder are connected via cross-attention the cross-attention is implemented as the following dot-product attention module attention s t = softmax ( ) cid:18 t ( ) ( ) dk where is the packed encoder-side contextual representation t is the packed decoder-side contextual representation and dk is the model size 6221 in the dot-product module it would be beneﬁcial if the contextual representations of the encoder and decoder have the same distributions however in the cross-lingual setting the encoder and decoder deal with different languages and thus the distributions of the learned contextual representations may be inconsistent this motivates us to explicitly learn alignment relationships between languages to make the contextual representations of two languages easier to be aligned we introduce the normalization technique into the transformer model normalizing the word representations has been proved an effective technique on word alignment xing et al. 2015 . after normalization two sets of embeddings are both located on a unit hypersphere which makes them easier to be aligned we achieve this by introducing the prenormalization technique and replacing the layernorm with scalenorm nguyen and salazar 2019 : ( ) +1 layernorm o cid:96 + ( ) o cid:96 ) ⇓ ( ) +1 o cid:96 + ( ) scalenorm o cid:96 ) where ( ) is the cid:96 -th layer and ( ) is its input the formula for calculating scalenorm is scalenorm x g = · x/ cid:107 x cid:107 ( ) where is a hyper-parameter an additional beneﬁt of scalenorm is that after being normalized the dot-product of two vectors ( ) is equivalent to their cosine distance ( ) ( ) ( ) cid:107 v cid:107 , which may beneﬁt the attention module in transformer we will conduct experiments to verify this 3.2 enhancing the isomorphism a key assumption of aligning the representations of two languages is the isomorphism of learned monolingual representations some researchers show that the isomorphism assumption weakens when two languages are etymologically distant søgaard et al. 2018 patra et al. 2019 . however ormazabal et al ( 2019 show that this limitation is due to the independent training of two separate monolingual embeddings and they suggest to jointly learn cross-lingual representations on monolingual corpora inspired by ormazabal et al ( 2019 , we take the following approaches to address the isomorphism problem first we combine the english and chinese summarization corpora and build a uniﬁed vocabulary second we share encoders and decoders in our model sharing encoders and decoders can also enforce the model to learn shared contextual representations across languages for the shared decoder to indicate the target language we set the ﬁrst token of the decoder to specify the language the module is operating with third we train several monolingual summarization steps before cross-lingual training as shown in the ﬁrst line in alg 1. the pre-trained monolingual summarization steps also allow the model to learn easier monolingual summarization ﬁrst then further learn cross-lingual summarization which may reduce the training difﬁculty 4 unsupervised training objective we describe the objective of unsupervised crosslingual summarization in this section the whole training procedure can be found in alg 1. summarization loss given an english textsummary pair and ( ) we use the encoder φex and the decoder φdx to generate the hypothetical english summary ˜x that maximizes the output summary probability given the source text ˜x arg max¯x ( ¯x| ) we adopt maximum loglikelihood training with cross-entropy loss between hypothetical summary ˜x and gold summary ( ) zx φex x , lsummx x x cid:48 ) =− ( ) log ( ) ( ) ˜x φdx zx t ˜x t zx cid:1 ( ) t=1 where is the length of ( ) the chinese summarization loss lsummy is similarly deﬁned for the chinese encoder φey and decoder φdy generative and discriminative loss given an english source text and a chinese source text , we use the encoder φex and φey to obtain the contextual representations zx { zx1 . . zxm and zy { zy1 . . zyn , respectively for zhto-en summarization we use the mapper my to map zy into the english context space zy→x my zy . we hope the mapped distribution zy→x and the real english distribution zx could be as similar as possible such that the english decoder can deal with cross-lingual summarization just like dealing with monolingual summarization to learn this mapping we introduce two discriminators and adopt the adversarial training goodfellow et al. 2014 technique we optimize the 6222 mappers at the sentence-level1 rather than wordlevel which is inspired by aldarmaki and diab 2019 where they found learning the aggregate mapping can yield a more optimal solution compared to word-level mapping concretely we ﬁrst average the contextual rep ( ) i=1 m resentations n cid:88 i=1 n ˜zy→x ( zy→x i ˜zx zxi 4 then we train the discriminator dx to discriminate between ˜zy→x and ˜zx using the following discriminative loss ldisx ˜zy→x ˜zx = log pdx src 0|˜zy→x − log pdx src 1|˜zx ( ) where pdx src|˜z is the predicted probability of dx to distinguish whether ˜z is coming from the real english representation src 1 or from the mapper my src 0 . in our framework the encoder φex and mapper my together make up the generator the generator tries to generate representations which would confuse the discriminator so its objective is to maximize the discriminative loss in eq 5. alternatively we train the generator to minimize the following generative loss lgeny ˜zy→x ˜zx = log pdx src 1|˜zy→x − log pdx src 0|˜zx ( ) the discriminative loss ldisy ˜zx→y ˜zy for dy generative loss lgenx ˜zx→y ˜zy for φey and mx are similarly deﬁned notice that since we use vector averaging and adopt the linear transformation it does not matter whether we apply the linear mapping before or after averaging the contextual representations and the learned sentence-level mappers can be directly applied to word-level mappings cycle reconstruction loss theoretically if we do not add additional constraints there exist inﬁnite mappings that can align the distribution of ˜zx and ˜zy and thus the learned mappers may be invalid in order to learn better mappings we introduce the cycle reconstruction loss and back-translation loss to enhance them 1the sentence in this paper can refer to the sequence containing multiple sentences given zx we ﬁrst use mx to map it to the chi nese space and then use my to map it back zx→y mx zx , ˆzx my zx→y ( ) we force zx and ˆzx to be consistent constrained by the following cycle reconstruction loss lcycx zx ˆzx = cid:107 zx ˆzx cid:107 ( ) the cycle reconstruction loss lcycy for zy and ˆzy is similarly deﬁned back-translation loss the cycle-reconstructed representation ˆzx in eq 8 can be regarded as augmented data to train the decoder which is similar to the back-translation in the neural machine translation area concretely we use the decoder φdx to read ˆzx and generate the hypothetical summary ˆx the back-translation loss is deﬁned as the cross-entropy loss between ˆx and gold summary ( ) ˆx φdx ˆzx lbackx ˆzx = t cid:88 log ( ) ( ) t=1 cid:1 t ˆx t ˆzx 9 the back-translation loss enhances not only the generation ability of the decoder but also the effectiveness of the mapper the back-translation loss lbacky for ˆzy is similarly deﬁned total loss the total loss for optimizing the encoder decoder and mapper of the english side is weighted sum of the above losses lx lsummx λ1lgenx λ2lcycx λ3lbackx 10 where λ1 λ2 and λ3 is the weighted hyperparameters the total loss of the chinese side is similarly deﬁned and the complete loss of our model is the sum of english loss and chinese loss l lx ly 11 ( 12 the total loss for optimizing the discriminators is ldis ldisx ldisy supervised training objective the supervised training objective contains the same summarization loss in unsupervised training objective eq 3 . in addition it has x-summarization loss and reconstruction loss 6223 algorithm cross-lingual summarization input english summarization data and chinese summarization data . : pre-train english and chinese monolingual summarization several epochs on and y. for = to dis iters do sample a batch fromx and a batch fromy if unsupervised then b update φex φey mx and my update dx and dy onldis in eq.5 ( a update φex φey φdx and φdy on lsumm in eq 3. on lgen in eq 6. on lcyc in eq 8. on lback in eq 9 ( ) update mx my φdx and φdy c update φex φey mx and my : for = to max iters do : : : : : : : 10 11 12 13 14 15 16 17 18 19 20 21 else if supervised then on lsumm in eq 3. on lxsumm in eq 13. on lrec in eq 14 ( ) update φex φey φdx and φdy c update φex φey mx and my a upate φex φey φdx and φdy x-summarization loss given a parallel english source text and chinese summary ( ) we use φex mx and φdy to generate the hypothetical chinese summary ˜y then train them with crossentropy loss zx= φex x , zx→y= mx zx , ˜y φdy zx→y lxsummx x y cid:48 ) − ( ) log ( ) ( ) | ˜y t x cid:1 t=1 13 the x-summarization loss for a chinese text and english summary ( ) is similarly deﬁned reconstruction loss since the cross-lingual summarization corpora are constructed by translating the texts to the other language the english texts and the chinese texts are parallel to each other we can build a reconstruction loss to align the sentence representation for the parallel english and chinese texts speciﬁcally supposing and are parallel source english and chinese texts we ﬁrst use φex and φey to obtain contextual representations zx and zy respectively then we average the contextual representations to get their sentence representations and use the mappers to map them into the other language since the english and chinese texts are translations to each other the semantics of their sentence representations should be the same thus we design the following reconstruction loss m cid:88 n cid:88 ˜zx lrecx ˜zx ˜zy→x = cid:107 ˜zx ˜zy→x cid:107 ˜zy→x 1 zxi 1 i=1 i=1 zy→x i 14 and lrecy is similarly deﬁned notice that the generative and discriminative loss cycle-construction loss and back-translation loss are unnecessary here because we can directly use aligned source text with objective 14 to align the context representations total loss the total loss for training the english side is lx lxsummx λ1lsummx λ2lrecx 15 where λ1 and λ2 is the weighted hyper-parameters the total loss of the chinese side is similarly deﬁned 6 experiments 6.1 experiment settings we conduct experiments on english-to-chinese en-to-zh and chinese-to-english zh-to-en summarizations following duan et al ( 2019 , we translate the source texts to the other language to form the pseudo parallel corpus since they do not release their training data we translate the source text ourselves through the google translation service notice that zhu et al ( 2019 translate the summaries rather than source texts since duan et al ( 2019 use gigaword and duc2004 datasets for experiments while zhu et al ( 2019 use lcsts and cnn/dm for experiments we conduct experiments on all the datasets when comparing with duan et al ( 2019 and zhu et al ( 2019 , we use the same number of translated parallel data for training due to limited computing resources we only do unsupervised experiments on gigaword and lcsts datasets notice that the test sets provided by zhu et al ( 2019 are unprocessed therefore we have to process the test samples they provided ourselves 6224 6.2 dataset gigaword english gigaword corpus napoles et al. 2012 contains 3.80m training pairs 2k validation pairs and 1,951 test pairs we use the human-translated chinese source sentences provided by duan et al. 2019 to do zh-to-en tests duc2004 duc2004 corpus only contains test sets we use the model trained on gigaword corpus to generate summaries on duc2004 test sets we use the 500 human-translated test samples provided by duan et al. 2019 to do zh-to-en tests lcsts lcsts hu et al. 2015 is a chinese summarization corpus which contains 2.40m training pairs 10,666 validation pairs and 725 test pairs we use 3k cross-lingual test samples provided by zhu et al ( 2019 to do zh-to-en tests cnn/dm cnn/dm hermann et al. 2015 contains 287.2k training pairs 13.3k validation pairs and 11.5k test pairs we use the 3k cross-lingual test samples provided by zhu et al ( 2019 to do en-to-zh cross-lingual tests 6.3 evaluation metrics we use rouge-1 unigram , rouge-2 bigram , and rouge-l lcs f1 scores as the evaluation metrics which are most commonly used evaluation metrics in the summarization task 6.4 competitive models for unsupervised cross-lingual summarization we set the following baselines • uniﬁed it jointly trains english and chinese monolingual summarizations in a uniﬁed model and uses the ﬁrst token of the decoder to control whether it generates chinese or english summaries • uniﬁed+clwe it builds a uniﬁed model and adopts pre-trained unsupervised cross-lingual word embeddings the cross-lingual word embeddings are obtained via projecting embeddings from source language to target language we use vecmap2 to learn the cross-lingual word embeddings for supervised cross-lingual summarization we compare our model with shen et al. 2018 , duan et al. 2019 , and zhu et al ( 2019 . we also consider the following baselines for comparison 2https //github.com/artetxem/vecmap pipe-ts the pipe-ts baseline ﬁrst uses a transformer-based translation model to translate the source text to the other language then uses a monolingual summarization model to generate summaries to make this baseline stronger we replace the translation model with the google translation system and name it as pipe-ts . pipe-st the pipe-st baseline ﬁrst uses a monolingual summarization model to generate the summaries then uses a translation model to translate the summaries to the other language we replace the translation model with the google translation system as pipest . pseudo the pseudo baseline directly trains a cross-lingual summarization model by using the pseudo parallel cross-lingual summarization data • xlm pretraining this method is proposed by lample and conneau 2019 , where they pretrain the encoder and decoder on largescale multilingual text using causal language modeling clm , masked language modeling mlm , and translation language modeling tlm tasks 3 √ dmodel implementation details 6.5 for transformer architectures we use the same conﬁguration as vaswani et al ( 2017 , where the number of layers model hidden size feed-forward hidden size and the number of heads are , 512 1024 and , respectively we set = 512 in scalenorm the mapper is a linear layer with a hidden size of 512 and the discriminator is a two-layer linear layer with a hidden size of 2048. we use the nltk4 tool to process english texts and use jieba5 tool to process chinese texts the vocabulary size of english words and chinese words are 50,000 and 80,000 respectively we set λ1 1 λ2 5 λ3 2 in unsupervised training and λ1 0.5 λ2 5 in supervised training according to the performance of the validation set we set dis iters 5 in alg 1 3this baseline was suggested by the reviewers and the results are only for reference since it additionally uses a lot of pre-training text 4https //github.com/nltk/nltk : //github.com/fxsjy/jieba 6225 method pipe-ts pipe-st pipe-ts pipe-st pseudo ( shen et al. 2018 ( duan et al. 2019 ( zhu et al. 2019 ( zhu et al. 2019 w/ ldc xlm pretraining ours gigaword r1 22.27 28.27 22.52 29.56 30.93 21.5 30.1 r2 6.58 11.90 6.67 12.50 13.25 6.6 12.2 rl 20.53 26.50 20.76 26.42 27.29 19.6 27.7 r1 21.29 25.73 21.83 26.66 27.03 19.3 26.0 32.28 32.04 14.03 13.60 28.19 27.91 28.27 27.25 zh-to-en duc2004 r2 5.96 8.19 6.11 8.51 8.49 4.3 8.0 9.40 8.71 rl 17.99 21.60 18.42 22.37 23.08 17.0 23.1 23.78 23.36 lcsts r2 10.41 18.87 11.09 19.56 19.76 22.65 22.58 22.80 23.20 rl 21.72 31.44 23.18 32.93 34.63 36.39 36.21 38.73 36.96 r1 27.26 36.48 29.29 38.26 38.61 40.34 40.25 42.75 40.97 en-to-zh cnn/dm r1 r2 rl 25.95 11.01 23.29 27.82 35.81 11.78 14.96 24.97 32.07 38.25 40.23 39.11 38.12 20.20 22.32 17.57 16.76 34.76 36.59 34.14 33.86 table : rouge f1 scores % on cross-lingual summarization tests “ xlm pretraining and zhu et al ( 2019 w/ ldc use additional training data our model signiﬁcantly p 0.01 outperforms all pipeline methods and pseudo-based methods we use adam optimizer kingma and ba 2014 with = 0.9 0.98 for optimization we set the learning rate to 3e 4 and adopt the warm-up learning rate goyal et al. 2017 for the ﬁrst 2,000 steps the initial warm-up learning is set to 1e 7. we adopt the dropout technique and set the dropout rate to 0.2. lcsts gigaword method uniﬁed r1 rl 13.52 1.35 10.02 uniﬁed+clwe 14.02 1.49 12.10 rl 2.09 2.92 20.11 5.46 16.07 13.75 4.29 11.82 r1 5.25 6.51 r2 0.87 1.07 ours r2 table : rouge f1 scores % on unsupervised crosslingual summarization tests our model outperforms all baselines signiﬁcantly p 0.01 . results and analysis 7.1 unsupervised cross-lingual summarization the experiment results of unsupervised crosslingual summarization are shown in table , and it can be seen that our model signiﬁcantly outperforms all baselines by a large margin by training a uniﬁed model of all languages the model s crosslingual transferability is still poor especially for the gigaword dataset incorporating cross-lingual word embeddings into the uniﬁed model can improve the performance but the improvement is limited we think this is due to that the cross-lingual word embeddings learned by vecmap can not leverage the contextual information due to space limitations we present case studies in the appendix after checking the generated summaries of the two baseline models we ﬁnd that they can generate readable texts but the generated texts are far away from the theme of the source text this indicates that the encoder and decoder of these baselines have a large gap such that the decoder can not understand the output of the encoder we also ﬁnd that summaries generated by our model are obviously more relevant demonstrating that aligned representations between languages are helpful but we can also see that there is still a gap be tween our unsupervised results table ) and supervised results table ) indicating that our model has room for improvement 7.2 supervised cross-lingual summarization the experiment results of supervised cross-lingual summarization are shown in table 1. due to the lack of corpus for training chinese long document summarization model we do not experiment with the pipe-ts model on the cnn/dm dataset by comparing our results with pipeline-based or pseudo baselines we can ﬁnd that our model outperforms all these baselines in all cases our model achieves an improvement of 0∼3 rouge scores over the pseudo model trained directly with translated parallel cross-lingual corpus and 1.5∼4 rouge-1 scores over those pipeline models we also observe that models using the google translation system all perform better than models using the transformer-based translation system this may because the transformer-based translation system will bring some unk tokens and the transformer-based translation system trained by ourselves does not perform as well as the google translation system in addition pipe-st models perform better than pipe-ts models which is con 6226 method reference pipest pipets pseudo ours supervised ours unsupervised info ↑ con ↑ flu ↑ 3.60 3.80 4.00 3.56 3.81 3.37 3.89 3.27 3.94 3.56 2.18 2.87 3.50 3.51 3.80 3.81 3.93 3.34 table : results of the human evaluation on the gigaword dataset method reference pipest pipets pseudo ours supervised info ↑ con ↑ flu ↑ 4.21 3.58 4.13 3.38 3.78 3.38 3.46 4.05 4.13 3.55 3.57 3.45 3.93 3.90 4.03 table : cnn/dm dataset results of the human evaluation on the sistent with the conclusions of previous work this is because 1 the translation process may discard some informative clauses ( ) the domain of the translation corpus is different from the domain of summarization corpus which will bring the domain discrepancy problem to the translation process and 3 the translated texts are often translationese ( graham et al. 2019 . the pseudo model performs better than pipe-ts models but performs similarly as pipe-st models by comparing our results with others we can ﬁnd that our model outperforms shen et al ( 2018 and duan et al ( 2019 on both gigaword and duc2004 test sets and it outperforms zhu et al ( 2019 on the lcsts dataset but our rouge scores are lower than zhu et al ( 2019 on the cnn/dm dataset especially the rouge-2 score however our model performs worse than pretrained models 7.3 human evaluation the human evaluation was also performed since we can not get the summaries generated by other models we only compare with our baselines in the human evaluation we randomly sample 50 examples from the gigaword zh-to-en test set and 20 examples from the cnn/dm en-to-zh test set we ask ﬁve volunteers to evaluate the quality of the generated summaries from the following three aspects ( ) informative how much does the generated summaries cover the key content of the source text ( ) conciseness how concise are the generated summaries ( ) fluency how ﬂuent are the generated summaries the scores are gigaword cnn/dm r2 r1 r1 rl method rl ours supervised 32.04 13.60 27.91 38.12 16.76 33.86 w/o summ loss 30.36 12.84 26.41 36.37 15.97 32.11 31.95 13.46 27.88 38.28 16.73 33.93 w/o mappers w/o scalenorm 31.27 13.29 27.22 37.01 16.30 32.87 w/o pre steps 31.33 13.30 27.35 37.23 16.39 33.01 unshare enc/dec30.10 12.71 26.28 35.93 15.86 31.82 r2 table : results of ablation tests in supervised setting statistically signiﬁcant improvement p 0.01 over the complete model are marked with . between 1-5 with being the best we average the scores and show the results in table and table 4. our model exceeds all baselines in informative and conciseness scores but get a slightly lower ﬂuency score than pipe-st . we think this is because the google translation system has the ability to identify grammatical errors and generate ﬂuent sentences 7.4 ablation tests to study the importance of different components of our model we also test some variants of our model for supervised training we set variants of 1 without monolingual summarization loss ( ) without mappers6 ( ) replace scalenorm with layernorm ( ) without pre-trained monolingual steps and 5 unshare the encoder and decoder for unsupervised training we additionally set variants without cyc-reconstruction loss or back-translation loss the results of ablation tests of supervised and unsupervised cross-lingual summarization are shown in table and table , respectively it seems that the role of mappers does not seem obvious in the case of supervised training we speculate that this may be due to the joint training of monolingual and cross-lingual summarizations and directly constraining the context representations before mapping can also yield shared aligned representations but mappers are crucial for unsupervised cross-lingual summarization for supervised cross-lingual summarization except for mappers all components contribute to the improvement of the performance the performance decreases after removing any of the components for unsupervised cross-lingual summarization all components contribute to the improvement of the performance and the mappers and shared encoder/decoder are key components 6in this case we directly constrain the parallel zx and zy to be the same 6227 lcsts gigaword r1 r1 r2 rl method r2 rl ours unsupervised 20.10 5.46 16.07 13.75 4.29 11.82 14.79 2.29 12.36 6.26 1.02 3.11 w/o mappers 17.51 4.70 13.95 7.21 1.31 4.04 w/o cyc loss w/o back loss 19.37 5.23 15.44 13.20 4.11 11.27 w/o scalenorm 19.24 5.21 15.37 13.15 4.08 11.21 w/o pre steps 19.70 5.24 15.72 13.13 4.10 10.91 12.28 0.97 10.37 4.88 0.82 1.91 unshare enc/dec et al ( 2018 learn word mappings between languages to build an initial unsupervised machine translation model and then perform iterative backtranslation to ﬁne-tune the model aldarmaki and diab 2019 propose to directly map the averaged embeddings of aligned sentences in a parallel corpus and achieve better performances than wordlevel mapping in some cases table : results of the ablation tests of unsupervised cross-lingual summarization statistically signiﬁcant improvement p 0.01 over the complete model are marked with . conclusions related work 8.1 cross-lingual summarization early researches on cross-lingual abstractive summarization are mainly based on the monolingual summarization methods and adopt different strategies to incorporate bilingual information into the pipeline model leuski et al. 2003 orasan and chiorean 2008 wan et al. 2010 wan 2011 yao et al. 2015 . recently some neural cross-lingual summarization systems have been proposed for cross-lingual summarization shen et al. 2018 duan et al. 2019 zhu et al. 2019 . the ﬁrst neural-based crosslingual summarization system was proposed by shen et al ( 2018 , where they ﬁrst translate the source texts from the source language to the target language to form the pseudo training samples a teacher-student framework is adopted to achieve end-to-end cross-lingual summarization duan et al ( 2019 adopt a similar framework to train the cross-lingual summarization model but they translate the summaries rather than source texts to strengthen the teacher network zhu et al ( 2019 propose a multi-task learning framework by jointly training cross-lingual summarization and monolingual summarization or machine translation . they also released an english-chinese cross-lingual summarization corpus with the aid of online translation services 8.2 learning cross-lingual representations learning cross-lingual representations is a beneﬁcial method for cross-lingual transfer conneau et al ( 2017 use adversarial networks to learn mappings between languages without supervision they show that their method works very well for word translation even for some distant language pairs like english-chinese lample in this paper we propose a framework that jointly learns to align and summarize for neural crosslingual summarization we design training objectives for supervised and unsupervised cross-lingual summarizations respectively we also propose methods to enhance the isomorphism and crosslingual transfer between languages experimental results show that our model outperforms supervised baselines in most cases and outperforms unsupervised baselines in all cases acknowledgments this work was supported by national natural science foundation of china 61772036 , tencent ai lab rhino-bird focused research program no.jr201953 , and key laboratory of science technology and standard in press industry key laboratory of intelligent press media technology . we thank the anonymous reviewers for their helpful comments xiaojun wan is the corresponding author references wasi ahmad zhisong zhang xuezhe ma eduard hovy kai-wei chang and nanyun peng 2019. on difﬁculties of cross-lingual transfer with order differences a case study on dependency parsing in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 2440–2452 hanan aldarmaki and mona diab 2019. contextaware cross-lingual mapping in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 3906–3911 mikel artetxe gorka labaka eneko agirre and kyunghyun cho 2018. unsupervised neural main international conference on chine translation learning representations 6228 xilun chen ahmed hassan hany hassan wei wang and claire cardie 2019. multi-source cross-lingual model transfer learning what to share in proceedings of the 57th annual meeting of the association for computational linguistics pages 3098–3112 anton leuski chin-yew lin liang zhou ulrich germann franz josef och and eduard hovy 2003. cross-lingual * st rd english access to hindi information acm transactions on asian language information processing talip , ( ) :245–269 alexis conneau guillaume lample marc aurelio ranzato ludovic denoyer and herv´e j´egou 2017. arxiv word translation without parallel data preprint arxiv:1710.04087. xiangyu duan mingming yin min zhang boxing chen and weihua luo 2019. zero-shot crosslingual abstractive sentence summarization through teaching generation and attention in proceedings of the 57th annual meeting of the association for computational linguistics pages 3162–3172 ian goodfellow jean pouget-abadie mehdi mirza bing xu david warde-farley sherjil ozair aaron courville and yoshua bengio 2014. generative adin advances in neural information versarial nets processing systems pages 2672–2680 priya goyal piotr doll´ar ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing jia and kaiming he 2017. accurate large minibatch sgd training imagenet in hour arxiv preprint arxiv:1706.02677. yvette graham barry haddow and philipp koehn 2019. translationese in machine translation evaluation arxiv preprint arxiv:1906.09833. karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom 2015. teaching machines to read and comprehend in advances in neural information processing systems pages 1693–1701 baotian hu qingcai chen and fangze zhu 2015. lcsts a large scale chinese short text summarization dataset in proceedings of the 2015 conference on empirical methods in natural language processing pages 1967–1972 diederik kingma and jimmy ba 2014. adam a method for stochastic optimization arxiv preprint arxiv:1412.6980. alexandre klementiev ivan titov and binod bhattarai 2012. inducing crosslingual distributed representain proceedings of coling 2012 tions of words pages 1459–1474 guillaume lample and alexis conneau 2019. crosslingual language model pretraining advances in neural information processing systems neurips . courtney napoles matthew gormley and benjamin in provan durme 2012. annotated gigaword ceedings of the joint workshop on automatic knowledge base construction and web-scale knowledge extraction pages 95–100 association for computational linguistics toan nguyen and julian salazar 2019. transformers without tears improving the normalization of self-attention arxiv preprint arxiv:1910.05895. constantin orasan and oana andreea chiorean 2008. romanian-english evaluation of a cross-lingual multi-document summariser in lrec 2008. aitor ormazabal mikel artetxe gorka labaka aitor soroa and eneko agirre 2019. analyzing the limitations of cross-lingual word embedding mappings arxiv preprint arxiv:1906.05407. barun patra joel ruben antony moniz sarthak garg matthew gormley and graham neubig 2019. bilingual lexicon induction with semi-supervision in non-isometric embedding spaces arxiv preprint arxiv:1908.06625. tal schuster ori ram regina barzilay and amir globerson 2019. cross-lingual alignment of contextual word embeddings with applications to zeroin proceedings of the shot dependency parsing 2019 conference of the north american chapter of the association for computational linguistics human language technologies volume ( long and short papers , pages 1599–1613 abigail see peter liu and christopher manning 2017. get to the point summarization with pointergenerator networks in proceedings of the 55th annual meeting of the association for computational linguistics volume : long papers , pages 1073– 1083. shi-qi shen yun chen cheng yang zhi-yuan liu and mao-song sun 2018. zero-shot cross-lingual ieee/acm transacneural headline generation tions on audio speech and language processing taslp , 26 12 :2319–2327 anders søgaard sebastian ruder and ivan vuli´c 2018. on the limitations of unsupervised bilingual dictionary induction in proceedings of the 56th annual meeting of the association for computational linguistics volume : long papers , pages 778– 788. guillaume lample myle ott alexis conneau ludovic denoyer et al 2018. phrase-based neuin proceedral unsupervised machine translation ings of the 2018 conference on empirical methods in natural language processing pages 5039–5049 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez łukasz kaiser and illia polosukhin 2017. attention is all in advances in neural information proyou need cessing systems pages 5998–6008 6229 xiaojun wan 2011. using bilingual information for in procross-language document summarization ceedings of the 49th annual meeting of the association for computational linguistics human language technologies pages 1546–1555 xiaojun wan huiying li and jianguo xiao 2010. cross-language document summarization based on machine translation quality prediction in proceedings of the 48th annual meeting of the association for computational linguistics pages 917–926 svante wold kim esbensen and paul geladi 1987. principal component analysis chemometrics and intelligent laboratory systems 2 1-3 :37–52 chao xing dong wang chao liu and yiye lin 2015. normalized word embedding and orthogonal transform for bilingual word translation in proceedings of the 2015 conference of the north american chapter of the association for computational linguistics human language technologies pages 1006–1011 jin-ge yao xiaojun wan and jianguo xiao 2015. phrase-based compressive cross-language summarization in proceedings of the 2015 conference on empirical methods in natural language processing pages 118–127 junnan zhu qian wang yining wang yu zhou jiajun zhang shaonan wang and chengqing zong 2019. ncls neural cross-lingual summarization in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing emnlp-ijcnlp , pages 3045–3055 a visualization we use the pca wold et al. 1987 algorithm to visualize the preand post-aligned context representations of our model in figure 2. the left picture shows the original distribution of two languages and the right picture shows the distribution after we map chinese representations to english figure reveals that the representations of the two languages are originally separated but become aligned after our proposed procedure which demonstrates that our proposed alignment procedure is effective b case studies we show four cases of chinese-to-english summarization in table 7. since most of the summaries generated by other unsupervised baselines are meaningless e.g. far away from the theme of the source text all tokens are unk and so on , we don t show their results here ( a pre-alignment b post-alignment figure : visualization of the preand post-aligned context representations the blue dots are english context representations and the red dots are chinese context representations 6230 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 text 野生动物专家称，除非政府发起全面打击猖獗偷猎的战争，否则印度大象将会灭绝 ( wildlife experts say indian elephants will go extinct unless government launches full-scale war against sting poaching reference india elephant may be facing extinction experts by unk pipe-st wildﬁle expert says indian elephant will die out pipe-ts india to kill elephants in war on poaching pseudo indian elephants face extinction unless government launches war against poaching ours supervised : india elephants face extinction over poaching ours unsupervised : india elephants rise to extinct text 一份媒体报道，一名日本男子周日在台湾上吊自杀，原因是亚洲冠军没能在世界杯上 获得一场胜利 ( report claimed that a japanese man hanged himself in taiwan on sunday because the asian champion failed to win a victory at the word cup reference fan hangs himself for nation s dismal world cup performance pipe-st japanese man hangs himself in taiwan as asian champion fails to win pipe-ts world cup winner commits pseudo man commits because of world cup failure ours supervised : man hangs himself after world cup failure ours unsupervised : failed to secure a single champions text 澳大利亚教练罗比-迪恩斯对上周末在这里对阵意大利的袋鼠测试前被新西兰击败的 球队做了八次改变 ( australian coach robbie deans made eight changes to a team defeated by new zealand before the kangaroo test against italy here last weekend reference < unk : deans rings changes for aussies azzurri test pipe-st australian coach changes team eight times before kangaroo test pipe-ts australia make eight changes for italy test pseudo deans makes eight changes for new zealand ours supervised : australia make eight changes ahead of italy test ours unsupervised : weekend ahead of wallabies test against italy here text 凯尔特人中场保罗哈特利在经历了一个星期痛苦的欧洲之旅后，于周五为苏格兰足球 发起了一场激情的辩护 ( celtic midﬁelder paul hartley launched a passionate defence for scottish football on friday after a week of painful european travel reference football scottish football is not a joke says celtic star pipe-st paul hartley launches passionate defense pipe-ts celtic s hartley launches passionate defense pseudo celtic s hartley launches passionate defense for scotland ours supervised : celtic s hartley defends scottish football ours unsupervised : celtic midﬁelder paul week of european misery table : case studies of chinese-to-english summarization 6231 enhancing content planning for table-to-text generation with data understanding and veriﬁcation heng gong,1 wei bi xiaocheng feng,1 bing qin,1 xiaojiang liu ting liu1 1department of computer science and technology harbin institute of technology harbin china hgong xcfeng qinb tliu @ ir.hit.edu.cn weibi cse.ust.hk xiaojiangliu84 hotmail.com abstract neural table-to-text models which select and order salient data as well as verbalizing them ﬂuently via surface realization have achieved promising progress based on results from previous work the performance bottleneck of current models lies in the stage of content planning selecting and ordering salient content from the input . that is performance drops drastically when an oracle content plan is replaced by a model-inferred one during surface realization in this paper we propose to enhance neural content planning by 1 understanding data values with contextual numerical value representations that bring the sense of value comparison into content planning ( ) verifying the importance and ordering of the selected sequence of records with policy gradient we evaluated our model on rotowire and mlb two datasets on this task and results show that our model outperforms existing systems with respect to content planning metrics introduction table-to-text generation refers to the task of generating text from structured data models for this task can be mainly categorized into two types pipeline-style models which decompose the generation process into sequential stages including content planning stage , selecting and ordering salient content from the input and surface realization stage , converting the content plan to surface string ( kukich 1983 mckeown 1985 ; and end-to-end models which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework wiseman et al. 2017 nie et al. 2018 . as in fig 1 this task provides tables with redundant records each record has three elements table row header entity e.g conley , table column header type e.g points and table cell value e.g 32 . models are expected to generate descriptive text figure : a rotowire s example with ncp s result and gold text important/unimportant entities and records are in red/blue text that accurately/incorrectly report statistics in table is in bold/italic reﬂecting salient records many neural end-to-end models have achieved remarkable progress of generating ﬂuent and natural text on this task puduppully et al. 2019b gong et al. 2019 . however previous work notices that the content planning stage is the key factor in table-to-text generation gkatzia 2016 , but end-to-end models are difﬁcult to explicitly improve their content planning ability recently puduppully et al ( 2019a proposed neural content planning ncp , a twostage model that explicitly selects and orders salient records whilst keeping the ability to generate ﬂuent text of end-to-end models they show that content planning referring to both content selection and planning in puduppully et al ( 2019a ) indeed correlates with the quality of ﬁnal output yet ncp simply maximizes the log-likelihood of findings of the association for computational linguistics emnlp 2020 pages 2905–2914 november 16 20 2020. ( ) 2020 association for computational linguistics 2905 type grizzlies pts win loss fgp fg3p qtr4 109 36 28 44 40 18 grizzlies pts 109 nets pts 122 grizzlies win 36 grizzlies loss 28 grizzlies fgp 53 grizzlies fg3p 47 grizzlies fgp 53 grizzlies fg3p 47 kilpatrick pts 23 kilpatrick reb 5 kilpatrick ast 3 lopez pts 8 lopez reb 4 lopez ast 2 kilpatrick pts 23 kilpatrick reb 5 kilpatrick ast 3 brooklyn fgp 53 memphis fgp 44 kilpatrick pts 23 kilpatrick reb 5 kilpatrick ast 3 lin pts 18 lin reb 4 lopez pts 8 lopez reb 4 foye pts 14 levert pts 14 fgp field goals percentage fg3p 3-pointer percentage qtr4 team points in 4th quarter 0 1 0 0 0 0 0 0 1 4 4 5 1 0 1 player player nets 2 1 3 pts ast reb blk stl pts ast reb blk stl grizzlies 3 1 4 6 9 7 12 parsons wright 18 gasol conley 32 green randolph 10 nets 122 levert 14 11 lopez 51 foye 14 53 lin 18 47 booker 34 kilpatrick 23 pts points ast assists reb rebounds blk blocks stl steals conley pts 32 conley reb 3 conley ast 6 gasol pts 18 gasol reb 6 gasol ast 4 gasol blk 2 randolph pts 10 randolph reb 7 randolph ast 3 the memphis grizzlies defeated the brooklyn nets 109 122 at fedex forum on friday the grizzlies 36 28 came into this game as a huge favorite and they left no doubt with this result in fact there were 10 ties and 11 lead changes in total with no team leading by more than 12 points three-point shooting was the difference in the game with the grizzlies going 53 percent from the field and 47 percent from three point range while the grizzlies went 53 percent from the field and 47 percent from three point range the grizzlies also dominated the assist to turnover ratio recording 10 more assists than brooklyn while committing five less turnovers sean kilpatrick led the nets in scoring as he tallied 23 points five rebounds and three assists brook lopez finished with just eight points four rebounds and two assists sean kilpatrick was huge off the bench providing 23 points five rebounds and three assists mike conley was the player of the game as he collected 32 points three rebounds and six assists marc gasol collected 18 points six rebounds four assists and two blocks zach randolph accrued 10 points seven rebounds and three assists on deck for the nets is a home matchup against the portland trail blazers on friday extracted records ncp+cc ncp conley pts 32 conley reb 3 conley ast 6 gasol pts 18 gasol reb 6 gasol ast 4 gasol blk 2 memphis looked poised for an easy win when seeing this matchup on the schedule but brooklyn had other ideas the nets took care of business when it mattered most winning the fourth quarter 34 19 shooting was key as brooklyn shot 53 percent from the field while holding memphis to just 44 percent shooting sean kilpatrick led the way for brooklyn as he provided 23 points five rebounds and three assists off the bench jeremy lin led the starters as he accrued 18 points and four rebounds the nets won despite getting just eight points and four rebounds from brook lopez randy foye and caris levert scored 14 points apiece mike conley carried the load for memphis as he collected 32 points three rebounds and six assists marc gasol contributed 18 points six rebounds four assists and two blocks extracted records gold pre-extracted sequences of content plans given all records according to their reported results the inferred content plans are still far from the oracle thus we focus on bridging the gap between the inferred content plans and the upper-bounds in stage , and thus improving the ﬁnal generation results we observe that whether a record is important highly depends on its record value however ncp as well as other neural generation models treats numerical values in table as tokens and the prominent role of values in content planning is not recognized let s take fig 1 for example compared to the gold text ncp mistakenly states that the memphis grizzlies defeated the brooklyn nets while nets clearly score more points than grizzlies in this match also ncp neglects important players such as lin who performs the second best in team nets . we hypothesize that this is because the model lacks understanding of values in their given context here context means the structured table information when representing corresponding records in addition we ﬁnd that ncp tends to include redundant information when describing those players for example ncp includes redundant two assists when describing lopez . a possible reason is that the use of maximum likelihood estimation mle is not enough to help verify important records during training to address the aforementioned numeric value understanding and important record veriﬁcation problems we propose a generation model with data understanding and verﬁcation duv , improving content planning in the framework of ncp specifically we design contextual numeric value representations obtained through a pre-trained ranking task in the pre-trained model we compare pairwise numerical values describing the same type of information and which has a higher value in the record encoder when training the model we replace the value representation with its contextual version from the pre-trained model in this way the constructed record representation is also context-aware besides instead of using the simple mle we design integrated rewards to verify content planning results we conducted experiments on rotowire and mlb showing that our model outperforms existing systems regarding the content selection and ordering metric 2 background this task s input consists of tables of records the basics of a record include entity r.e type r.c value r.v and features r.f models need to generate text = y1 y2 ... y|y| ( |y| is number of words to describe important records in tables as stated in sec.1 this task has two main stages ( ) content planning and ii surface realization puduppully et al ( 2019a propose neural content planning ncp to explicitly optimize these two stages in deep neural networks making the generation process more interpretable with an intermediate content plan thus we use it as base model in stage ( content planning , ncp embeds tokens into embedding vectors and encodes each record with one-layer mlp for rotowire 1 . . r∗ = relu wa r.e r.c r.v r.f + ba . 1 here r.∗ represents their embedding vectors wa and ba are trainable parameters and ; denotes vector concatenation the reason to choose mlp is that its records are game statistics without sequential relationship between records for mlb we follow puduppully et al ( 2019b and use lstm instead because its input includes sequential event data next a content selection gate is applied on each to control the amount of information ﬂowing from the record r. a lstm-based pointer network vinyals et al. 2015 is applied to sequentially decode a content plan which is a sequence of important records extracted from the output text denoted as r∗ { r∗ } t is the number of records mentioned in ) here we follow puduppully et al ( 2019a to extract content plans using an information extraction ie approach as oracles in each time step the decoder takes previously selected record s representation as input and use the attention weights to select the next important one in stage ( surface realization , a standard encoder-decoder model is applied taking the output content plan from stage as input and generating text with attention mechanism luong et al. 2015 and conditional copy mechanism gulcehre et al. 2016 . from results in puduppully et al ( 2019a , it is observed that performance bottleneck lies in stage 1. that is if we feed gold content plans into stage , ﬁnal results are much better but if inferred content plans are fed instead performance decreases drastically therefore we focus on improving ncp s stage for better ﬁnal outputs 2906 figure : the architecture of our approach to enhance neural content planning in the base model module ) we propose two modules contextual numerical value representation module ) and content planning veriﬁcation module ) first we use module to pre-train contextual numerical value representation and replace the numerical value s token embedding after module planning records from table we use module to optimize with reinforced supervision signals then module use the content plans to generate text 3 approach we propose to improve content planning stage of ncp from two aspects ( ) during record encoding we design a contextual numeric value representation to improve the understanding of entities ( players and teams ) performance ( ii a reinforced training strategy with targeted supervision signals is used to compensate maximizing the mle in pointer network to boost model s content planning ability fig 2 illustrates the overall training procedure we ﬁrst pre-train a model to learn contextual numeric value representations to understand relationship between records numeric values by pairwise ranking loss secondly given the pretrained model and table , we encode each record with its contextual numeric value representation in decoding phase of stage , the pointer network is guided to favor important records for content planning with the help of reinforced supervision signals stage remains the same as in the base model we describe details in following parts 3.1 contextual numerical value representation current table-to-text models treat numerical values in table as tokens and use embeddings to represent them however a numerical value has some attributes that a text token doesn t have generally a larger numerical value indicates better performance of a player also considering different context an identical numerical value can convey different meaning ( ) one numerical value describing same type of records can correspond to different situations on court for instance if a player got 23 points in a game top among all players it indicates outstanding performance but if there are other players on court with points over 30 it becomes less outstanding ( ) the same numerical value describing different types of information should not be interpreted in the same way for example “ ” assists may indicate good performance while 5 points may suggest disappointing performance hence it is important to model a numerical value in context of other numerical values describing the same type of information in order to understand what is behind those numerical values here we propose to learn contextual numerical value representations for this task we extract numerical values that describe the same type of information from the same table to form training samples e.g players points in nets for a pre-trained task our main idea is to use transformer encoder vaswani et al. 2017 to compare each numerical value with others in each training sample we ﬁrst use it to fuse information of numerical values in the same sample and obtain their contextual numerical value representations next we optimize the pairwise ranking loss using their contextual representations such that a large numerical value is with a higher ranking score taking 2907 self attention gate mlp results parsons_pts_12 gasol_pts_18 conley_pts_32 green_pts_9 levert_ast_1 foye_ ast _1 lin_ ast _1 booker_ ast _1 r1cs r2cs r3cs rics … … … … … … ⨀⨀⨀ ⨀⨀ ⨀⨀⨀ player parsons gasol conley green grizzlies pts ast reb 12 6 18 32 9 4 0 nets player pts ast reb levert 14 foye 14 lin 18 booker self attention 1 1 mlp parsons pts 12 gasol pts 18 …foye ast 12 18 32 r z l e p s e n t 3 4 p112 p218 p332 re13 re23 re34 u c i n c r f n t o s o e … … ranking objectives p332 p218 p112 p218 …p112 p332 ≥ …re34 re23 re34 re23 re13 re13 … 1 … 18 r pla cin contextual … numerical value representation 1. contextual numerical value representation text generation y1 y3 s3 s1 sos y1 y2 y2 s2 lstm 2. model s4 y3 attention +copy h1 record decoder s1 text decoder rics record representation p112 value score embedding layer rics h3 r1cs r1cs h2 r3cs r3cs h1 rs … h4 rics content planning attention mean pooling generated record sequences r3cs rjcs rjcs r3cs r2cs rjcs reference record sequences entity importance entity recall record importance record recall record ordering entity embedding type embedding value token embedding contextual numerical value representation policy gradient reward function 3. content planningverification all raw numerical value embedding ri.v s of each training sample as input we construct the contextual numerical value embeddings ˜r [ ˜r1 . . ˜rn via multi-layer transformer encoder h0 [ r1.v . . ri.v . . rn.v , 2 ak ln hk−1 mhselfatt hk−1 ) 3 hk ln ak ffn ak ) ˜r hk 4 where is the number of numerical values in the sample ln is the layer normalization mhselfatt is the multi-head self-attention function and ffn is position-wise feed-forward network given a pair of contextual numerical value representations ˜ri and ˜rj we use a fully connected layer ( ˜ri = sigmoid wp˜ri bp to calculate the ranking score for each numerical value in the current input sample if ri.v rj.v we expect ( ˜ri to be higher than ( ˜rj . for training contextual numerical value representations we use the hinge loss eq.5 . is the margin and ( ) gives +1 if is true and −1 otherwise ( ) pre max 0 ξ t ri.v rj.v n cid:88 n cid:88 1 n2 f ˜ri − ( ˜rj ) j=1 ( ) =i i=1 5 we construct training samples of the pre-trained task using all training tables note that numerical values from different types of information form different samples when the pre-trained model is converged we use it in the record encoder in eq 1 by replacing the token embedding ri.v with its contextual representation ˜ri via eq 2 to eq 4 3.2 content planning veriﬁcation the original ncp uses the pointer network to explicitly infer a content plan by optimizing the mle of gold content plans as noticed in other generation tasks sordoni et al. 2015 li et al. 2016a dai et al. 2017 , generation models with the mle as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-deﬁned rewards that better mimic the true goal of an ideal output sequence li et al. 2016b , which is the sequence of the content plan in our task in order to explicitly reﬂect the quality of content plans we explore rewards that measure the following ﬁve criteria and optimized the model according to them via policy gradient sutton and barto 1998 . .e | cid:88 | r∗ entity importance ei evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan r∗ } r · function gives +1 reward when is true and -1 otherwise i .e ) ei rt = ( rt.e { r∗ 6 • entity recall er measures how many important entities are covered by the decoded content plan = rt . ( ) is the indicator function which is when is true otherwise . i .e | { r∗ [ r∗ .e { rt.e ] er r = 7 • record importance ri and record recall rr are similar to ei and er respectively but focus on each individual record instead of entity only i=1 8 i ) ri rt = ( rt { r∗ } ( ) { r∗ [ r∗ | r∗ } i { rt ] i=1 rr r = 9 • record ordering ro calculates the normalized damerau-levenshtein distance brill and moore 2000 between the predicted content plan and the reference r∗ in order to measure how well the model organizes the chosen records the above designed rewards measure the content plan on different granularity ei and er focus on whether the selected entity player/team is an important one it is also crucial to which of the entity s records are needed to be mentioned therefore we also include ri and rr afterwards we sample record sequence combine all rewards and use policy gradient to guide the optimization of content selection given as the input table rtok log ( rt|r t s t cid:88 lrl − t 1 t=1 rseq−β log ( r|s ( 10 ( 11 rtok γ1ei rt + γ2ri rt rseq γ3er r +γ4rr r +γ5ro r ( 12 given a batch of input tables s g and gold content plan r∗ g we ﬁrst train the pointer network by optimizing the mle lgen − < , , sg . then we further ﬁnetune it with both the mle loss and policy gradient l γ6lrl ( 1− γ6 lgen please note that represents length of the content plan γ1-γ6 and are hyper-parameters ( ) tg t=1 log ( r∗ cid:80 g , g|r∗ tg g=1 2908 experiments 4.1 setup dataset and evaluation metrics we conducted experiments on both rotowire1 and mlb puduppully et al. 2019b dataset the former provides pairs of nba game statistics and summary differently the latter provides summary and heterogeneous input consisting of mlb game statistics and event data including event type actors etc ) in chronological order for rotowire we follow ofﬁcial training development and test splits of 3398/727/728 instances for mlb as the contents are not released we are able to retrieve a split of 22820/1739/1744 instances via ofﬁcial scripts 2. for evaluations we use bleu papineni et al. 2002 and three extractive metrics which evaluate the generated results from the following aspects ( ) relation generation rg , measuring the text ﬁdelity about whether to describe information from table truthfully ( ) content selection cs to measure whether important information is selected from redundant game statistics ( ) content ordering co evaluates a model s ability to plan and order data records naturally in text more details can be found in wiseman et al ( 2017 . implementation details we follow puduppully et al ( 2019a ’ and puduppully et al ( 2019b ’ training conﬁgurations in the base model for rotowire and mlb respectively we chose the proposed hyper-parameters based on performance on development set due to page limit we include model and training details in appendix codes of our model can be found at : //github.com/ernestgong/data2text-duv 4.2 results comparing methods in this section we compare • template we follow wiseman et al ( 2017 and puduppully et al ( 2019b for constructing templatebased generators for rotowire and mlb respectively the details and conditional copy cc model can be found in those papers • ncp+cc ncp : our base model here we provide both results reported in the original paper and reproduced by us denoted as ncp r . we also try a variant of ncp by using separate sets of embeddings in the encoders of two stages denoted as s-ncp we observe that s-ncp is comparable : //github.com/harvardnlp/boxscore-data : //github.com/ratishsp/mlb-data-scripts with reproduced ncp with the ability to explicitly improve stage without affecting stage 2. thus we use it to further verify our proposed model • entity modeling ent ( puduppully et al. 2019b and hierarchical encoder on three dimensions hetd ( gong et al. 2019 are two stateof-the-art models on rotowire and/or mlb opatt nie et al. 2018 introduces pre-executed operations for text generation • data understanding with content plan veriﬁcation duv : our proposed full model we also include two variants for ablations s-ncp veriﬁcation s-n+v to study our model without data understanding and data understanding du to study without content plan veriﬁcation automatic evaluation for rotowire as shown in table , template system achieves high rg % high-ﬁdelity due to rigid rules also it achieves high cs % since it includes vast amount of information high rg ) and some of which are redundant low cs % . compared with it most neural models perform signiﬁcantly better at ﬁltering redundant records cs % while still covering many important records leading to better cs f1 . the higher co also shows that neural models can better organize data records conditioned on the data among all neural models duv exceeds other neural models in terms of content selection cs f1 ) and content ordering co on test set also by comparing duv with its base model s-ncp , our model improves more on cs % in terms of rg our model also performs better than base model but still has a gap to ent and hetd this is mainly affected by surface realization stage ) which is beyond the scope of this paper for mlb we ﬁnd similar pattern as discussed above the differences are 1 improvements on cs and co are less signiﬁcant than on rotowire since mlb includes additional event data that rotowire doesn t have we separate out the statistical data in table for fair comparison we ﬁnd that base model s-ncp achieves 73.43 ( table ) regarding statistical data on mlb v.s 44.37 ( table ) on rotowire of cs f1 in stage , leaving much less room for improvement ( ) ncp-style models achieve less bleu than ent on mlb the latter brevity penalty bp 0.736 generates longer text compared with duv bp 0.623 . this is mainly due to surface realization stage ) which we leave for future work table also includes ablations of our model 2909 rotowire rw temp ed+cc opatt ncp+cc ncp ent hetd ncp r s-ncp s-ncp+v s-n+v du duv mlb temp ed+cc ncp+cc ncp ent s-ncp s-ncp+v s-n+v du duv rg rg % 99.94 74.80 87.47 92.69 91.46 86.06 85.05 85.29 88.05 87.45 % 97.99 91.74 88.65 84.61 87.80 88.13 87.99 89.02 54.21 23.72 34.28 30.11 31.47 26.60 26.93 25.36 29.42 26.94 57.11 17.10 15.96 22.10 16.67 16.73 16.63 16.65 % 27.02 29.49 34.18 38.64 36.09 36.56 35.59 37.12 38.19 40.73 % 23.51 63.45 64.16 55.32 62.63 62.89 62.80 63.44 cs % 58.22 36.18 51.22 48.51 48.01 43.57 43.76 42.82 49.66 48.78 cs % 65.69 47.27 51.47 60.92 53.56 53.91 53.75 53.63 co f1 dld 15.07 36.91 32.49 15.42 41.00 43.02 41.21 39.76 39.25 39.77 43.18 44.39 18.58 20.17 20.86 18.84 18.51 18.99 22.14 23.32 co f1 dld 10.80 34.63 25.59 54.18 27.11 57.12 57.99 23.59 27.22 57.74 27.69 58.06 57.93 27.47 27.78 58.12 bleu 8.58 14.19 14.74 16.50 16.12 16.85 14.84 14.63 13.77 16.12 15.92 bleu 2.80 9.65 8.39 13.11 9.62 9.54 9.53 9.51 table : automatic evaluation results on test set on rotowire top , results are obtained with updated extractive evaluation models puduppully et al. 2019a . those above the dash line except for temp are from corresponding papers on mlb bottom , since our vocabulary is different from the one in released models we re-train the information extraction ie model via ofﬁcial script puduppully et al. 2019b on re-collected dataset it can recall 96.60 of tuples with precision of 96.39 on test set compared to the released oracle tuples all baselines results on mlb are reproduced by us note that for ent we directly use the released code to train ( s-n+v and du . results show that both data understanding and veriﬁcation modules contribute to the overall improvement due to page limit we include validation performance in appendix human evaluation each example below is evaluated by different annotators from a commercial annotation company who are proﬁcient in english and we report the average of three annotators results in following settings first we sample 30 examples from test set and asked annotators to determine how many information in the summary are correct # sup and how many are contradicting # cont to the table on rotowire our model describes the table more concisely closest sup to gold text while produces signiﬁcantly less contradicting facts than ncp thanks to signiﬁcant improvement on stage 1. we observe that gold text contains incorrect facts e.g wrong ﬁeld-goal percentage while cont of temp is due to annotation error gap between ent and duv on cont shows potential of stage , which is beyond the scope of this paper second we arrange results from models of each example into 10/15 pairs rotowire/mlb and asked annotators to determine which one in the pair performs better in terms of grammaticality coherence and conciseness the reported result is the subtraction of the percentage of time a system is considered better and when considered worse on rotowire duv can generate most coherent text among neural models but less satisfying on grammaticality and conciseness compared with ent this is mainly affected by surface realization stage ) a possible way is to use large-scale pretrained language models such as gpt-2 radford et al. 2019 to address this issue in mlb duv achieves comparable performance with ncp across metrics due to the same stage . 2910 rw sup cont gram coher conc gold 8.89 cid:63 29.11 cid:63 temp 53.87 cid:63 0.02 cid:63 -24.00 cid:63 ncp 40.61 cid:63 6.38 cid:63 ent 35.06 2.69 -5.78 -8.22 duv 30.74 3.61 31.91 1.92 cid:63 26.44 cid:63 6.44 -10.67 2.67 -20.44 cid:63 -8.22 cid:63 8.22 cid:63 -2.67 1.78 -3.56 mlb sup cont gram coher conc 26.67 cid:63 28.74 cid:63 32.59 cid:63 gold 15.27 4.30 -15.56 cid:63 -19.11 cid:63 14.81 cid:63 temp 52.02 cid:63 0.72 cid:63 -22.52 cid:63 -9.48 14.04 1.99 cid:63 cc ncp 13.82 2.87 -1.04 -9.33 -7.70 4.89 cid:63 ent 18.89 cid:63 3.29 duv 13.24 3.80 -5.48 -7.85 -9.33 -3.11 7.85 cid:63 -5.04 table : human evaluation results models with cid:63 perform signiﬁcantly different from duv p 0.05 , using a one-way anova with posthoc tukey hsd tests we omit cc on rotowire because ncp is proven to be better puduppully et al. 2019a . figure : visualization of contextual numerical value representations of pts ast reb of the example in fig 1 obtained by our pre-trained model in blue and the token embeddings of values from to 49 in red via pca pearson 1901 on left rotowire . the right one are obtained on a mlb s example appendix . 4.3 analysis visualization fig 3 visualizes value s token embeddings in red and our contextual numerical value representations in blue . token embeddings are closer between each other while the contextual ones are more discriminative and naturally ordered from low to high along the blue arc . we hypothesize this phenomenon contributes to the improvement of content selection content planning in table , we discuss stage ’ content planning cs and co results on rotowire du improves on all metrics it focuses more on covering important records cs % compared with others by adding veriﬁcation on top of du duv , it can further improve on cs % f1 rw ncp ncp r s-ncp s-n+v du duv -ei -er -ri -rr -ro cs % cs % cs f1 co 38.00 20.27 21.49 41.43 21.28 40.28 42.52 21.34 24.42 43.38 26.63 46.97 26.05 46.75 46.84 26.26 26.18 46.70 26.41 47.00 47.01 26.26 44.51 44.50 44.37 45.15 48.30 50.21 49.98 49.89 50.09 50.19 50.12 53.72 48.05 49.39 48.13 54.48 53.93 53.69 53.36 54.02 53.85 53.67 table : results of stage performance on content planning metrics on rotowire s development set and co. considering both cs % and % duv can generate more concise but informative content plans with little sacriﬁce on recall next by subtracting each reward from duv we observe that all rewards contribute to duv s improvement on content selection and ordering rotowire v.s mlb our model s improvements on cs and co are signiﬁcant on rotowire but less signiﬁcant on mlb different from rotowire mlb additionally provides sequential event data the two different sources of input can be regards as heterogeneous liu et al. 2019 . the average statistical data in gold text is 12.69 while event data is 4.16 extracted by ie model on test set . in table , we discuss cs and co for two types of data respectively du and veriﬁcation both improve over base model with veriﬁcation contributing more overall they consistently improve on cs f1 and co on statistical data but the high cs of base model indicates little room for improvement meanwhile event data is the bottleneck and the drop on that also attributes to the not so signiﬁcant overall cs and co improvement it reveals potential for content planning on heterogeneous input on mlb as future work 4.4 case study compared with ncp and gold text in fig 1 duv fig 4 has nice properties ( ) it accurately states that nets with higher points defeated grizzlies while ncp fails this is due to our model s ability to compare value ( ) our model can better ﬁlter unimportant records cs % while cover the important ones cs % than both ncp and ent note that our model covers all important players 2911 8 2 6 39 32 46 44 29 30 23 16 13 7 789 1 3 5 7 9 18 15 1012 14 6 45 2 mlb cs % cs % cs f1 co oa stat evnt oa stat evnt oa stat evnt oa stat evnt 70.89 71.04 70.19 70.45 76.31 52.53 70.67 73.58 60.09 36.08 43.70 37.91 ncp s-ncp 69.73 69.64 70.15 71.60 77.66 53.05 70.65 73.43 60.41 36.08 43.25 39.06 s-n+v 69.77 69.65 70.32 72.05 78.79 51.39 70.89 73.94 59.38 36.30 43.71 38.42 69.48 69.48 69.48 72.26 78.45 53.29 70.84 73.69 60.32 36.43 43.66 39.29 du duv 70.16 70.07 70.59 72.31 78.97 51.90 71.22 74.25 59.82 36.54 44.07 38.57 -ei 69.84 69.71 70.46 72.32 78.90 52.17 71.06 74.02 59.95 36.15 43.98 38.69 -er 69.94 69.86 70.30 72.30 78.69 52.74 71.10 74.01 60.27 36.22 43.89 38.50 69.76 69.70 70.07 72.41 79.01 52.18 71.06 74.06 59.82 36.29 43.99 38.12 -ri -rr 69.95 69.91 70.15 72.22 78.71 52.36 71.07 74.05 59.96 36.23 43.91 38.54 -ro 69.76 69.65 70.23 72.41 78.75 52.98 71.06 73.92 60.40 36.13 43.81 38.31 table : results of stage performance on metrics about content planning on mlb s development set we study overall oa results with results for statistical stat data only and event evnt data only 5 related work in the past few years table-to-text generation has attracted many attentions to improve text ﬁdelity li and wan 2018 propose to generate templates and then ﬁll the slots while nie et al ( 2018 use pre-executed operations however our work mainly focuses on improving the content planning puduppully et al ( 2019b propose to speciﬁcally model entities when decoding texts different from them we model numerical values during encoding iso et al ( 2019 incorporate writers information to generate text step-by-step our work can also consider such information in surface realization stage ) for a fair comparison of all methods we do not include the use of this model here gong et al ( 2019 utilize hierarchical encoders with dual attention to consider both the table structure and history information in terms of building numerical value representations spithourakis and riedel 2018 explore number prediction for language models while naik et al ( 2019 explore numerical embeddings to capture the numeration and magnitude properties of numbers in our task generation models rely heavily on copy mechanism to cover numerical values in text and achieve good results thus how to understand numerical values to select records becomes important and we propose to understand them through their context 6 conclusion in order to enhance neural content planning for table-to-text generation we proposed 1 contextual numerical value representations to help model understand data values and 2 effective rewards figure : generation examples based on tables in fig 1. important/unimportant entities and records are in red/blue text that accurately/incorrectly reﬂects the statistics in table is in bold/italic due to page limit we include generation example on mlb in appendix and their records in this case while only mention one not so impressive player s records ( ) by comparing the content planning stage ) results and actual records mentioned in our model s text stage ) the main indeed lies in the content planning since surface realization can faithfully deliver most information 93.10 ) in the same order 2912 extracted records ent nets pts 122 grizzlies pts 109 nets win 11 nets loss 51 nets win 36 nets loss 28 nets win 36 nets loss 28 nets win 36 nets loss 28 conley pts 32 conley ast 6 conley reb 3 gasol pts 18 gasol reb 6 gasol ast 4 gasol blk 2 kilpatrick pts 23 nets pts 122 grizzlies pts 109 nets fgp 53 nets fg3p 47 grizzlies fgp 44 grizzlies fg3p 40 nets win 11 nets loss 51 kilpatrick pts 23 kilpatrick reb 5 kilpatrick ast 3 lin pts 18 lin reb 4 levert pts 14 foye pts 14 lopez pts 8 lopez reb 4 grizzlies win 36 grizzlies loss 28 conley pts 32 conley reb 3 conley ast 6 gasol pts 18 gasol reb 6 gasol ast 4 gasol blk 2 randolph pts 10 randolph reb 7 randolph ast 3 planning kilpatrick reb 5 kilpatrick ast 3 lin pts 18 lin reb 4 levert pts 14 levert reb 3 parsons pts 12 parsons ast 3 parsons min 22 foye pts 14 foye reb 3 foye stl 2 foye min 16 randolph pts 10 randolph reb 7 randolph ast 3 booker pts 8 booker reb 9 nets pts 122 grizzlies pts 109 grizzlies fgp 44 grizzlies fg3p 47 grizzlies fgp 44 grizzlies fg3p 40 nets win 11 nets loss 51 kilpatrick pts 23 kilpatrick reb 5 kilpatrick ast 3 lin pts 18 lin reb 4 levert pts 14 foye pts 14 lopez pts 8 lopez reb 4 grizzlies win 36 grizzlies loss 28 conley pts 32 conley reb 3 conley ast 6 gasol pts 18 gasol reb 6 gasol ast 4 gasol blk 2 randolph pts 10 randolph reb 7 randolph ast 3 extracted the brooklyn nets defeated the memphis grizzlies 122 109 at fedex forum on monday the nets 11 51 checked in to monday 's contest with only four road victories in 34 tries away from barclays center but the nets 36 28 checked in to monday 's contest with only three road victories in 16 tries away from barclays center but the nets 36 28 checked in to monday 's contest with only three road victories in 16 tries away from barclays center but the nets 36 28 have now won three straight and five of their last six games they were led by mike conley 's 32 points which he supplemented with six assists three rebounds and a steal marc gasol was right behind him with 18 points six rebounds four assists and two blocks sean kilpatrick was next in line with 23 points five rebounds three assists a steal and a block jeremy lin followed with 18 points four rebounds and an assist caris levert led the bench with 14 points three rebounds and an assist chandler parsons was right behind him with 12 points three assists a rebound and a steal over 22 minutes randy foye was the only other starter to manage double digit scoring supplying 14 points three rebounds an assist and two steals over 16 minutes zach randolph 10 points seven rebounds three assists and trevor booker eight points nine rebounds an assist a steal and a block were highly productive in reserve roles the brooklyn nets defeated the host memphis grizzlies 122 109 at fedex forum on thursday these are two teams going in opposite directions and this result shows just how scary they can be it was a wire to wire win for the nets as they held a double digit lead for the final 30 plus minutes defense was key for the nets as they held the grizzlies to 44 percent from the field and 47 percent from three point range while the grizzlies shot 44 percent from the field and 40 percent from three point range the nets 11 51 have now won three of their last four games as they continue to fall down the standings sean kilpatrick led the way for brooklyn as he tallied 23 points five rebounds and three assists jeremy lin was second on the team finishing with 18 points and four rebounds caris levert and randy foye each scored 14 points off the bench brook lopez was memphis 's best player as he totaled just eight points and four rebounds on deck for the nets is a road matchup with the new orleans pelicans on monday the grizzlies 36 28 have now won five of their last six games as they continue to fall down the standings mike conley did all he could for memphis as he accumulated 32 points three rebounds and six assists marc gasol finished second on the team accruing 18 points six rebounds four assists and two blocks zach randolph was the only other starter in double figures as he accrued 10 points seven rebounds and three assists memphis will look to keep rolling on wednesday in a home matchup against the milwaukee bucks duv ours to verify a model s inferred important records during training experimental results show that our model outperforms competitive baselines in terms of content planning in the future we would like to explore enhancement on surface realization jointly to generate better text acknowledgements we would like to thank the anonymous reviewers for their helpful comments xiaocheng feng is the corresponding author of this work this work is supported by the national key & program of china via grant 2018yfb1005103 and national natural science foundation of china nsfc via grant 61906053 and 61632011. references eric brill and robert moore 2000. an improved error model for noisy channel spelling correction in proceedings of the 38th annual meeting on association for computational linguistics pages 286–293 association for computational linguistics bo dai sanja fidler raquel urtasun and dahua lin 2017. towards diverse and natural image descripin proceedings of the tions via a conditional gan ieee international conference on computer vision pages 2970–2979 dimitra gkatzia 2016. content selection in dataarxiv preprint systems a survey to-text arxiv:1610.08375. heng gong xiaocheng feng bing qin and ting liu 2019. table-to-text generation with effective hierarchical encoder on three dimensions row column in proceedings of the conference on and time . empirical methods in natural language processing and the 9th international joint conference on natural language processing caglar gulcehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio 2016. pointing the unknown words in proceedings of the 54th annual meeting of the association for computational linguistics volume : long papers , berlin germany association for computational linguistics hayate iso yui uehara tatsuya ishigaki hiroshi noji eiji aramaki ichiro kobayashi yusuke miyao naoaki okazaki and hiroya takamura 2019. learning to select track and generate for in proceedings of the 57th annual data-to-text meeting of the association for computational linguistics jiwei li michel galley chris brockett jianfeng gao and bill dolan 2016a a diversity-promoting objective function for neural conversation models in proceedings of conference of the north american chapter of the association for computational linguistics human language technologies jiwei li will monroe alan ritter dan jurafsky michel galley and jianfeng gao 2016b deep reinforcement learning for dialogue generation in proceedings of conference on empirical methods in natural language processing pages 1192–1202 liunian li and xiaojun wan 2018. point precisely towards ensuring the precision of data in generated in proceedtexts using delayed copy mechanism ings of the 27th international conference on computational linguistics pages 1044–1055 tianyu liu fuli luo qiaolin xia shuming ma baobao chang and zhifang sui 2019. hierarchical encoder with auxiliary supervision for neural tableto-text generation learning better representation for tables aaai thang luong hieu pham and christopher d. manning 2015. effective approaches to attention-based in proceedings of the neural machine translation 2015 conference on empirical methods in natural language processing pages 1412–1421 association for computational linguistics kathleen mckeown 1985. text generation using discourse strategies and focus constraints to generate natural language text aakanksha naik abhilasha ravichander carolyn rose and eduard hovy 2019. exploring numeracy in word embeddings in proceedings of the 57th annual meeting of the association for computational linguistics feng nie jinpeng wang jin-ge yao rong pan and chin-yew lin 2018. operation-guided neural networks for high ﬁdelity data-to-text generation in proceedings of the conference on empirical methods in natural language processing pages 3879– 3889. kishore papineni salim roukos todd ward and weijing zhu 2002. bleu a method for automatic evaluation of machine translation in proceedings of the 40th annual meeting of the association for computational linguistics karl pearson 1901. liii on lines and planes of closest ﬁt to systems of points in space the london edinburgh and dublin philosophical magazine and journal of science 2 11 :559–572 karen kukich 1983. design of a knowledge-based rein proceedings of the 21st annual port generator meeting on association for computational linguistics pages 145–150 ratish puduppully li dong and mirella lapata 2019a data-to-text generation with content selection and planning proceedings of aaai conference on artiﬁcial intelligence 2913 ratish puduppully li dong and mirella lapata 2019b data-to-text generation with entity modeling in proceedings of the 57th annual meeting of the association for computational linguistics alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever 2019. language models are unsupervised multitask learners openai blog 1 8 . alessandro sordoni michel galley michael auli chris brockett yangfeng ji margaret mitchell jian-yun nie jianfeng gao and bill dolan 2015. a neural network approach to context-sensitive generation of conversational responses in proceedings of conference of the north american chapter of the association for computational linguistics human language technologies georgios spithourakis and sebastian riedel 2018. numeracy for language models evaluating and improving their ability to predict numbers in proceedings of the 56th annual meeting of the association for computational linguistics volume : long papers . richard sutton and andrew barto 1998. introduc tion to reinforcement learning volume 135. ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n. gomez lukasz kaiser and illia polosukhin 2017. attention is all you need in advances in neural information processing systems pages 5998–6008 pointer networks oriol vinyals meire fortunato and navdeep jaitly 2015. in c. cortes n. d. lawrence d. d. lee m. sugiyama and r. garnett editors advances in neural information processing systems 28 pages 2692–2700 curran associates inc. sam wiseman stuart m. shieber and alexander m. rush 2017. in data-to-document generation in proceedings of the conference on empirical methods in natural language processing 2914 a robust abstractive system for cross-lingual summarization jessica ouyang and boya song and kathleen mckeown deptartment of computer science columbia university new york ny 10027 ouyangj kathy @ cs.columbia.edu bs3065 columbia.edu abstract we present a robust neural abstractive summarization system for cross-lingual summarization we construct summarization corpora for documents automatically translated from three low-resource languages somali swahili and tagalog using machine translation and the new york times summarization corpus we train three language-speciﬁc abstractive summarizers and evaluate on documents originally written in the source languages as well as on a fourth unseen language arabic our systems achieve signiﬁcantly higher ﬂuency than a standard copy-attention summarizer on automatically translated input documents as well as comparable content selection 1 introduction cross-lingual summarization is a little-explored task combining the difﬁculties of automatic summarization with those of machine translation the goal is to summarize in one language a document available only in another language wan et al ( 2010 describe two approaches summarize then translate and translate then summarize they argue that summarize-then-translate is preferable to avoid both the computational expense of translating more sentences and sentence extraction errors caused by incorrect translations however summarize-then-translate can only be used when the source language is high-resource wan et al used english as the source for example ; if the source language is one of the thousands of low-resource languages in the world there are no summarization corpora available languageindependent techniques such as textrank mihalcea , might be used but there may be serious difﬁculties in their application such as morphologically rich languages that render token-based similarity measures useless in such a case translatethen-summarize is the only possible approach we address this scenario through the development of a neural abstractive summarization system that ﬂuently summarizes potentially disﬂuent automatically-translated documents by generating short simple phrases to replace awkward input phrases resulting from difﬁcult to translate source documents our novel combination of existing building block systems results in a summarization solution that can be easily applied to new lowresource languages we use machine translation on the new york times annotated corpus of document/summary pairs to create summarization corpora for documents automatically translated from three low-resource languages somali swahili and tagalog we use these corpora to train crosslingual summarizers for these source languages with english as the target we also evaluate our systems on a fourth source language arabic our experiments show that our abstractive summarizers produce more ﬂuent english summaries from automatically-translated documents and that this improvement generalizes across source languages our main contributions are as follows • we create summarization corpora for automatically translated somali swahili and tagalog documents noisy english input documents paired with clean english reference summaries • we present a method for producing crosslingual summarization systems for low resource languages where no summarization corpora currently exist providing a potential summarization solution for thousands of such languages • our novel approach of training on noisy input with clean references outperforms a standard copy-attention abstractive summarizer on realworld somali swahili and tagalog documents • our evaluation on arabic documents demonstrates that our robust abstractive summarizers minneapolis minnesota june june , 2019. ( ) 2019 association for computational linguistics proceedings of naacl-hlt 2019 pages 2025–2031 2025 in the editor why did president clinton continue to praise a program on welfare-to-work that failed in of those assigned in his comments he praised the consultation of the community of kansas city but was advised by gary j. stangler director of the department of social service of missouri which of the participants failed where are these people helping each other when the government cut them back to the pantry of food bad news mr. president the charity of the community will not help everyone who will come to us for help glenn classic valley park mo figure : a synthetic noisy english article from the tagalog nyt training set generalize to unseen languages 2 related work cross-lingual summarization or˘asan and chiorean 2008 extractively summarized romanian news articles and automatically translated the summaries into english their experiments showed that the poor quality of the translations turned reasonable romanian summaries into barely legible english ones the most extensively investigated source-target language pair is english-to-chinese wan et al ( 2010 used a predicted translation quality score as a feature in extracting sentences for their summaries wan 2011 translated the english sentences into chinese and represented sentences in the extraction stage by both the original english and the chinese translation yao et al ( 2015 scored aligned phrases from the original english documents and the chinese translations to perform sentence extraction and compression based on both salience and translation quality zhang et al ( 2016 parsed the original english documents into predicate-argument structures that were aligned with their chinese translations and generated the summary from these structures finally wan et al ( 2018 experimented with extracting and ranking multiple candidate summaries abstractive summarization rush et al ( 2015 presented the ﬁrst neural abstractive summarization model a convolutional neural network encoder and feed-forward network decoder with attention which learned to generate news headlines from the lead sentences of their articles chopra et al ( 2016 extended their work using a recurrent network for the decoder nallapati et al ( 2016 improved on the rnn encoder-decoder with attention model by adding linguistically-motivated part of speech and named entity type embeddings as well as a pointernetwork vinyals et al. 2015 to allow copying of rare or out-of-vocabulary words from the input document in this work we use see et al. s 2017 deﬁnition of the pointer-generator network which adds a coverage vector and coverage penalty to prevent repetition in generated words the new york times annotated corpus sandhaus 2008 was ﬁrst used for neural abstractive summarization by paulus et al ( 2018 , who used attention over the decoder s previous predictions to both prevent repetition and to allow for coherent longer summaries celikyilmaz et al ( 2018 also used the new york times corpus training multiple collaborating encoders to encode long documents one paragraph at a time 3 data we use the new york times hereafter nyt summarization corpus sandhaus 2008 , consisting of 650k articles and their humanwritten abstractive summaries we follow the train/test/validation split and preprocessing steps used by paulus et al ( 2018 , with one exception we do not anonymize named entities we ﬁrst translate 112k articles from the nyt corpus into each of our three low-resource languages somali swahili and tagalog using neural machine translation of the 112k articles 100k are taken from the training set 6k from validation and 6k from test we then translate the articles back into noisy english again using neural machine translation figure shows an example noisy english article we pair each noisy english article with the clean english reference summary corresponding to the clean english article that generated it thus our abstractive summarization model learns to take a bad english input document with translation errors and disﬂuencies and produce a good english summary for simplicity we refer to the corpus created by translating into somali and back as the somali nyt corpus and similarly with swahili and tagalog but all three corpora are in noisy english not somali swahili or tagalog 4 models 4.1 machine translation we use neural machine translation systems built on the marian framework junczys-dowmunt 2026 bleu model rouge-1 rouge-2 rouge-l table : neural machine translation performance model rouge-1 rouge-2 rouge-l language from english to english somali swahili tagalog 21.8 44.5 37.2 29.4 37.8 36.2 model rouge-1 rouge-2 rouge-l nyt-base paulus celikyilmaz 48.26 47.03 48.08 29.30 30.51 31.19 36.81 43.27 42.33 table : baseline summarizer performance et al. 2018 to translate the nyt corpus into somali swahili and tagalog and back to english the systems were developed at the university of edinburgh and were trained on a mix of clean human-curated parallel data about 23k sentences for somali and swahili and 51k for tagalog ; noisy web-crawled parallel data somali only about 354k sentences ; and synthetic backtranslated parallel data created from monolingual sources including news articles the common crawl and wikipedia 250-600k sentences . table shows the performance of the machine translation systems for each of the three languages on held-out test sets of 500 sentences taken from the clean human-curated parallel data 4.2 abstractive summarization for our abstractive summarizers hereafter abstractors , we implemented see et al. s 2017 pointer-generator network in pytorch paszke et al. 2017 . we pre-train for 12 epochs on the unmodiﬁed nyt corpus to obtain a baseline system table shows the performance of this baseline on the unmodiﬁed nyt test set our baseline underperforms the more complex systems of paulus et al ( 2018 and celikyilmaz et al ( 2018 , but we are more interested in the improvements our ﬂuency-focused approach makes over this baseline than in the baseline s performance compared to state-of-the-art systems we use each of the three noisy english corpora to train the baseline system for another epochs producing three languagespeciﬁc abstractors we also train a fourth mixedlanguage abstractor using 100k articles randomly selected from the somali swahili and tagalog training sets evenly split among the three ( a performance on somali nyt nyt-base abs-so abs-mix 32.94 37.72 38.07 nyt-base abs-sw abs-mix 35.28 39.24 39.96 nyt-base abs-tl abs-mix 37.17 40.96 40.87 10.36 15.39 15.76 12.96 17.01 17.56 14.67 18.72 18.91 22.51 26.56 26.82 25.64 29.88 30.24 27.27 31.06 31.14 b performance on swahili nyt model rouge-1 rouge-2 rouge-l c performance on tagalog nyt table : abs-so -sw and -tl are the somali swahili and tagalog systems respectively * indicates signiﬁcant improvement over nyt-base p 1.16 10−19 . model somali nyt swahili nyt tagalog nyt nyt-base abs-so abs-sw abs-tl abs-mix 4986 3357 3384 3501 3464 perplexity 4428 3429 3247 3476 3285 4707 3528 3312 3457 3402 table : language model perplexity of generated summaries on noisy somali swahili and tagalog nyt 5 evaluation 5.1 noisy nyt evaluation table shows the performance of our abstractors on the somali swahili and tagalog nyt test sets differences among the language-speciﬁc systems are not statistically signiﬁcant and the more general mixed model achieved the best scores1 however we found that abstractors trained solely on one language and tested on another significantly p 0.05 underperformed the mixed model which was trained on all three languages suggesting that training on some same-language data is still important we also trained a bigram language model on the entire set of nyt reference summaries and 1these results are shown in appendix a along with all combinations of the language-speciﬁc models on the three languages 2027 document mange kimambi i pray for the parliamentary seat for kinondoni constituency for ticket of ccm not special seats kinondoni without drugs is possible pray for the parliamentary seat for kinondoni constituency on the ticket of ccm yes it s not a special seats khuini kinondoni what will do for kinondoni tension is many get but we must remember no good that is available easily kinondoni without drugs is possible as a friend fan or patriotism urge you to grant your contribution to the situation and propert you can use western union or money to go to mange john kimambi account of crdb bank is on blog reduce my proﬁle in my blog understand why have to vie for kinondoni constituency you will understand more nyt-base mange kimambi who pray for parliamentary seat for kinondoni constituency for ticket of ccm in , is on blog and not special seats kinondoni without drugs abs-mix mange kimambi who pray for parliamentary seat for kinondoni constituency for ticket of ccm comments on his plans to vie for kinondoni without drugs figure : an automatically translated swahili weblog entry and its baseline and mixed abstractor summaries somali weblogs swahili weblogs tagalog weblogs model content fluency model content fluency model content fluency nyt-base abs-so abs-sw abs-tl abs-mix 1.66 1.92 1.94 1.86 2.08 1.62 1.90 1.88 1.82 2.04 nyt-base abs-so abs-sw abs-tl abs-mix 1.88 2.14 2.22 2.18 2.36 1.76 1.90 2.08 1.86 2.08 nyt-base abs-so abs-sw abs-tl abs-mix 1.72 1.76 1.94 1.80 2.08 1.76 1.88 1.92 2.08 2.16 table : average human-rated content and ﬂuency scores on somali swahili and tagalog weblog entries calculated the average perplexity of our abstractors output as a proxy for ﬂuency table ) we see that somali is the most difﬁcult overall but all three language-speciﬁc systems and the mixed model produce more ﬂuent english across source languages than does the base model 5.2 weblog evaluation we perform a human evaluation on 20 somali 20 swahili and 20 tagalog weblog entries that we automatically translate into english using the same neural machine translation systems we used to create our noisy nyt corpora unlike our nyt data which we translated from english into the low-resource languages these weblogs are realworld somali swahili and tagalog documents this evaluation demonstrates the performance of our system in a real use-case figure shows a swahili weblog entry and its summaries2 this example shows the advantage of our approach unlike a machine translation system which must translate every part of its input our abstractor is able to delete most of the long rambling and disﬂuent blog entry instead summing it up ﬂuently with the generated phrase comments on his plans and the repurposed phrase to vie for . we use ﬁve human judges all native english speakers and none of whom are the authors the 2all four abstractors produced very similar summaries judges were shown a translated document and a summary and asked to rate the content and ﬂuency of the summary on a scale of 1–3 table ) our human judges rated our abstractors higher in both ﬂuency and content and we see again that while the language-speciﬁc systems are more ﬂuent on their own languages than are the language-speciﬁc systems for the other languages the mixed model still performs the best we also see that while our improvement in content is more modest our improvement in ﬂuency the goal of this work is signiﬁcant the judges achieved substantial agreement fleiss s = 0.72 . 5.3 duc 2004 arabic evaluation finally we evaluate our system on a new language arabic we use the duc 2004 task test set which consists of real-world arabic news articles translated into english each paired with four human-written summaries table shows the performance of our abstractors on the arabic data demonstrating their ability to generalize and improve the ﬂuency of input documents automatically translated from a previously unseen language yielding a signiﬁcant improvement in rouge compared to the 28 duc 2004 systems our performance would have ranked 1st on summarizing the machine-translated documents despite our use of these lower-quality au 2028 document washington 10-23 afp was signed by benjamin netanyahu and yasser arafat on friday at the white house agreed on the israeli military withdrawal from the west bank in return for palestinian additional security guarantees nyt-base washington 10-23 signed by benjamin netanyahu and yasser arafat agrees on israeli military withdrawal from west bank in return for palestinian additional security guarantees abs-mix benjamin netanyahu and yasser arafat agree on israeli military withdrawal from west bank in return for palestinian security guarantees figure : an arabic article automatically translated into english and its baseline and mixed model summaries model rouge-1 rouge-2 rouge-l nyt-base abs-so abs-sw † abs-tl † abs-mix 26.56 28.64 28.08 29.43 28.79 5.86 6.66 6.39 7.02 6.74 15.76 19.62 18.36 19.89 19.79 table : duc 2004 with isi translations * indicates signiﬁcant improvement over nyt-base p 2.09×10−6 ; indicates signiﬁcant difference between systems p 0.05 . tomatically-translated documents we performed extremely well even in comparison with the duc 2004 systems on high-quality human-translated documents we would have ranked 1st 4th and 5th on rouge-1 -2 and -l respectively figure compares the baseline system and our abstractors on the arabic data3 6 discussion we ﬁnd that the nyt-base model tends to copy heavily from the beginning of its input documents since it was trained entirely on clean english news articles it is understandable that it tries to copy the lead sentence but in both examples it copies errors the confusing run-on sentence not special seats kinondoni without drugs is possible ( shown in yellow in figure ) and the phrase signed by ( shown in green in figure ) whose subject is missing in contrast our abstractors are able to correctly identify the important information in the input documents and produce ﬂuent summaries presenting this information in figure , abs-mix deletes the unnecessary washington 10-23 and produces the verb agree in the plural form agreeing with its plural subject more dramatically in figure , abs-mix identiﬁes kinondoni without drugs as mange kimambi s campaign platform and succinctly summarizes this using both the purely generated phrase comments on his plans and the repurposed but still ﬂuent and correct “ to vie for. 3all four abstractor summaries were identical the main limitation of our approach is that it assumes the existence of a machine translation system for the source language although our abstractors are able to handle errorful disﬂuent translations for extremely low-resource languages there may be no translations of any kind available in such a case another approach such as cross-lingual word embeddings is necessary 7 conclusion we have presented a robust abstractive summarization system for the task of cross-lingual summarization taking advantage of an abstractive system s ability to delete difﬁcult to translate phrases and generate new text to use instead our straightforward method allows us to produce summarization systems for low resource languages where no summarization corpora are currently available providing a potential summarization solution for thousands of such languages our experiments demonstrate that by using our novel approach of training on noisy english documents and clean english reference summaries the model learns to produce ﬂuent summaries from disﬂuent inputs further we have shown that while training a system for a speciﬁc source language gives strong performance the abstractive ﬂuency of these systems generalize to other source languages acknowledgements this research is based upon work supported in part by the national science foundation nsf , under grant no iis-1422863 and the ofﬁce of the director of national intelligence odni , intelligence advanced research projects activity iarpa , via contract fa8650-17-c-9117 the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies either expressed or implied of nsf odni iarpa or the u.s. government the u.s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein 2029 references asli celikyilmaz antoine bosselut xiaodong he and yejin choi 2018. deep communicating agents in proceedings of for abstractive summarization the 16th annual conference of the north american chapter of the association for computational linguistics human language technologies sumit chopra michael auli and alexander rush 2016. abstractive sentence summarization with attentive recurrent neural networks in proceedings of the 2016 conference of the north american chapter of the association for computational linguistics human language technologies marcin junczys-dowmunt roman grundkiewicz tomasz dwojak hieu hoang kenneth heaﬁeld tom neckermann frank seide ulrich germann alham fikri aji nikolay bogoychev andr´e f. t. martins and alexandra birch 2018. marian fast neural machine translation in c++ in proceedings of acl 2018 system demonstrations rada . language independent extractive sumin proceedings of the 43nd annual marization meeting of the association for computational linguistics ramesh nallapati bowen zhou cicero dos santos c¸ a glar gulc¸ehre and bing xiang 2016. abstractive text summarization using sequence-tosequence rnns and beyond in proceedings of the 20th signll conference on computational natural language learning constantin or˘asan and oana andreea chiorean 2008. romanian-english evaluation of a cross-lingual in proceedings of multi-document summariser the 6th international conference on language resources and evaluation adam paszke sam gross soumith chintala gregory chanan edward yang zachary devito zeming lin alban desmaison luca antiga and adam lerer 2017. automatic differentiation in pytorch in nips autodiff workshop romain paulus caiming xiong and richard socher 2018. a deep reinforced model for abstractive summarization in proceedings of the 6th international conference on learning representations alexander rush sumit chopra and jason weston 2015. a neural attention model for abstractive senin proceedings of the 2015 tence summarization conference on empirical methods in natural language processing evan sandhaus 2008. the new york times annotated corpus linguistic data consortium abigail see peter liu and christopher manning 2017. get to the point summarization with pointergenerator networks in proceedings of the 55th annual meeting of the association for computational linguistics oriol vinyals meire fortunato and navdeep jaitly 2015. pointer networks in advances in neural information processing systems xiaojun wan 2011. using bilingual information for in procross-language document summarization ceedings of the 49th annual meeting of the association for computational linguistics human language technologies xiaojun wan huiying li and jianguo xiao 2010. cross-language document summarization based on machine translation quality prediction in proceedings of the 48th annual meeting of the association for computational linguistics xiaojun wan fuli luo xue sun songfang huang and jin-ge yao 2018. cross-language document summarization via extraction and ranking of multiple summaries knowledge and information systems jin-ge yao xiaojun wan and jianguo xiao 2015. phrase-based compressive cross-language summarization in proceedings of the 2015 conference on empirical methods in natural language processing jiajun zhang yu zhou and chengqing zong 2016. abstractive cross-language summarization via translation model enhanced predicate argument structure fusing ieee/acm transactions on audio speech and language processing 24 10 . a appendix language-speciﬁc abstractor performance on noisy ny table expands table , showing the performance of each of our four abstractors on the somali swahili and tagalog nyt test sets as discussed in section 5.1 the differences among the three language-speciﬁc abstractors are not statistically signiﬁcant on any of the three languages however the differences between the mixed model and the two language-speciﬁc models not trained on a given test language are significant p 0.05 . that is the difference between abs-mix and abs-sw and the difference between abs-mix and abs-tl are signiﬁcant on the somali test set the difference between abs-mix and absso and the difference between abs-mix and abstl are signiﬁcant on the swahili test set and the difference between abs-mix and abs-so and the difference between abs-mix and abs-sw are signiﬁcant on the tagalog test set 2030 model rouge-1 rouge-2 rouge-l model rouge-1 rouge-2 rouge-l nyt-base abs-so abs-sw † abs-tl † abs-mix 32.94 37.72 37.26 36.89 38.07 10.36 15.39 14.94 14.41 15.76 22.51 26.56 25.92 25.53 26.82 nyt-base abs-so † abs-sw abs-tl † abs-mix 12.96 16.34 17.01 16.02 17.56 25.64 29.06 29.88 28.79 30.24 a performance on somali nyt ( ) performance on swahili nyt 35.28 38.42 39.24 38.24 39.96 27.26 29.16 29.43 31.06 31.14 model rouge-1 rouge-2 rouge-l nyt-base abs-so † abs-sw † abs-tl abs-mix 37.17 38.97 39.14 40.96 40.87 14.67 17.01 17.28 18.72 18.91 c performance on tagalog nyt table : abs-so -sw and -tl are the somali swahili and tagalog language-speciﬁc systems respectively * indicates signiﬁcant improvement over nyt-base p 1.16 10−19 ; indicates signiﬁcant difference between the mixed model and language-speciﬁc abstractors p 0.05 . 2031